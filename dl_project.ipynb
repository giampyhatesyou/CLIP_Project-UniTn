{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a6108",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project - a.y. 2024/2025\n",
    "\n",
    "This notebook represents our work for the 24/25 Deep Learning course project offered by the University of Trento. \n",
    "\n",
    "The task was few-shot adaptation of CLIP on the Flower102 dataset.\n",
    "\n",
    "**Authors**:\n",
    "- Andrea Giampietro - xxxxxx\n",
    "- Marco Gandolfi - 258017\n",
    "- Stefano Camposilvan - 257848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf4e7",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [**Introduction**](#1-introduction) \n",
    "\n",
    "2. [**Setup**](#2-setup)\n",
    "\n",
    "3. [**The Baseline: CLIP**](#3-the-baseline-clip)\n",
    "\n",
    "4. [**Our Approach: ProtoCoCoOp**](#4-our-approach-protococoop)\n",
    "\n",
    "5. [**Results and Discussion**](#6-results-and-discussion)\n",
    "\n",
    "6. [**Conclusions**](#5-conclusions)\n",
    "\n",
    "7. [**References**](#7-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36996",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. The Context\n",
    "\n",
    "Vision–Language Models (VLMs) are a class of models that integrate natural language processing and computer vision to perform a wide range of tasks, including image captioning, visual question answering, and text-to-image generation. When trained on large-scale datasets, these models achieve remarkable performance across diverse domains. </br>\n",
    "Among them, Contrastive Language–Image Pre-training (CLIP) in particular has demonstrated strong zero-shot capabilities, enabling it to recognize and classify images without explicit task-specific training.\n",
    "\n",
    "However, in many real-world scenarios, large labeled datasets are unavailable: data may be scarce, expensive to obtain, or highly specialized. Furthermore, fine-grained classification tasks, where categories differ only by subtle details, pose additional challenges. Thus, zero-shot performance may not be sufficient, and task-specific adaptation becomes necessary.\n",
    "\n",
    "In this context, Few-Shot Adaptation tries to address this challenge by improving generalization when only a limited number of labeled examples per class are available. The goal of such method is to leverage prior knowledge learned during pretraining and adapt the model to new tasks using minimal supervision, mimicking the human ability to learn new concepts from only a handful of examples. Importantly, the few-shot setting requires the model not only to specialize on the Base classes using limited supervision, but also to preserve its original zero-shot generalization on the Novel ones.\n",
    "\n",
    "\n",
    "### 1.1. Our Proposal\n",
    "\n",
    "In this project, we tackle this Few-shot Adapatation problem in order to try improving over CLIP's performance. \n",
    "\n",
    "To do so, we use the Oxford Flowers102 dataset as a benchmark for fine-grained visual recognition. Such dataset contains 102 flower species, many of which exhibit subtle inter-class difference, making classification more challenging. To simulate a realistic few-shot scenario, we adopt a base–novel split in which only a small number of labeled samples (specifically, 10 shots per class) are available for the Base categories during adaptation, while the remaining classes are treated as Novel and remain unseen during training.\n",
    "\n",
    "We then build upon CoCoOp, a prompt-learning method designed for few-shot adaptation, and propose a strategy to better balance the base–novel trade-off. Specifically, we combine:\n",
    "\n",
    "1. **Knowledge Distillation (KD)**\n",
    "\n",
    "We treat the original frozen CLIP model as a teacher and regularize our adapted model (the student) to remain close to CLIP’s zero-shot predictions. This is achieved through a distillation loss that aligns the student’s logits with those of the teacher. The goal is to prevent overfitting to Base classes and preserve generalization on Novel classes.\n",
    "\n",
    "2. **Prototype-Based Residual Fusion (Inference-Time)**\n",
    "\n",
    "For each Base class, we compute a visual prototype by averaging normalized image embeddings extracted from the few-shot training samples (including augmented views). These prototypes act as compact representations of Base-class visual structure.\n",
    "\n",
    "At inference time, we compute the similarity between the input image embedding and each Base-class prototype, and use this similarity to add a residual logit contribution exclusively to Base classes. This mechanism strengthens discrimination among Base categories while leaving Novel predictions unaffected.\n",
    "\n",
    "Our final approach therefore explicitly separates:\n",
    "\n",
    "- Training-time regularization, via Knowledge Distillation to preserve zero-shot behavior.\n",
    "\n",
    "- Inference-time enhancement, via prototype-based residual fusion to strengthen Base discrimination.\n",
    "\n",
    "By combining these two mechanisms, we aim to improve the harmonic mean between Base and Novel accuracy, achieving a more balanced base-to-novel generalization on the Flowers102 benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c3fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP already installed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from shutil import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Install CLIP if not already installed\n",
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d101d",
   "metadata": {},
   "source": [
    "### Paths and constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4faaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PATHS DEFINITION --\n",
    "# Directory for dataset\n",
    "data_path = \"data\"\n",
    "os.makedirs(data_path, exist_ok=True) \n",
    "\n",
    "# Directories for saving results\n",
    "models_path = \"results/models\"\n",
    "os.makedirs(models_path, exist_ok=True) \n",
    "logs_path = \"results/logs\"\n",
    "os.makedirs(logs_path, exist_ok=True) \n",
    "plots_path = \"results/plots\"\n",
    "os.makedirs(plots_path, exist_ok=True) \n",
    "\n",
    "# -- CONSTANTS DEFINITION --\n",
    "# Class names for Flowers102 dataset\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# -- REPRODUCIBILITY SETUP --\n",
    "# Function to set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Worker initialization function for DataLoader\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initialize random seed for each worker in DataLoader\n",
    "    Args:\n",
    "        worker_id (int): The ID of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b6d4",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec9e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA PREPARATION FUNCTIONS --\n",
    "# Load specific split of Flowers102 dataset, with given transformation\n",
    "def load_split(split, transform):\n",
    "    \"\"\"Load Flowers102 dataset split with given transformation.\n",
    "    Args:\n",
    "        split (str): One of \"train\", \"val\", or \"test\".\n",
    "        transform (callable): Transformation to apply to the images.\n",
    "    Returns:\n",
    "        torchvision.datasets.Flowers102: The requested dataset split.\n",
    "    \"\"\"\n",
    "    return torchvision.datasets.Flowers102(root=data_path, split=split, download=True, transform=transform)\n",
    "\n",
    "# Load Flowers102 dataset and return train, val, test sets\n",
    "def get_data(transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        transform (callable, optional): Transformation to apply to the images. Defaults to None.\n",
    "    Returns:\n",
    "        tuple: (train_set, val_set, test_set) as torchvision.datasets.Flowers102 instances.\n",
    "    \"\"\"\n",
    "    train = load_split(\"train\", transform)\n",
    "    val = load_split(\"val\", transform)\n",
    "    test = load_split(\"test\", transform)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Split dataset classes into base and novel classes\n",
    "def split_classes(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present in the dataset.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split classes from.\n",
    "    Returns:\n",
    "        tuple: (base_classes, novel_classes) as lists of class ids.\n",
    "    \"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "\n",
    "    # Split classes into base and novel (first half and second half)\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "# Split dataset into base and novel datasets\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Split dataset into base and novel datasets based on provided base classes.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split.\n",
    "        base_classes (list): List of class ids considered as base classes.\n",
    "    Returns:\n",
    "        tuple: (base_dataset, novel_dataset) as torch.utils.data.Subset instances.\n",
    "    \"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d7d0dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load CLIP model and preprocessing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/16\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load dataset and split into base and novel datasets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m get_data(transform\u001b[38;5;241m=\u001b[39mpreprocess)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip' is not defined"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Load dataset and split into base and novel datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# Get base and novel classes from the test set\n",
    "base_classes, novel_classes = split_classes(test_set)\n",
    "classes = base_classes + novel_classes\n",
    "\n",
    "# Get class names\n",
    "base_class_names = [CLASS_NAMES[i] for i in base_classes]\n",
    "print(f\"Base classes ({len(base_classes)}): {base_class_names}\")\n",
    "novel_class_names = [CLASS_NAMES[i] for i in novel_classes]\n",
    "print(f\"Novel classes ({len(novel_classes)}): {novel_class_names}\")\n",
    "print(f\"All classes: ({len(classes)}: { [CLASS_NAMES[i] for i in classes] }\")\n",
    "\n",
    "# Create base and novel datasets\n",
    "base_train_set, _ = split_data(train_set, base_classes)\n",
    "base_val_set, _ = split_data(val_set, base_classes)\n",
    "base_test_set, novel_test_set = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38062e",
   "metadata": {},
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**goal:** maximize HM between `base_acc_cocoop` and `novel_acc_cocoop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc73d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonic Mean Calculation\n",
    "def harmonic_mean(a, b):\n",
    "    \"\"\"Compute the harmonic mean of two accuracies.\"\"\"\n",
    "    # Guard against division by zero when both a and b are zero\n",
    "    return 2 * a * b / (a + b) if (a + b) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78449",
   "metadata": {},
   "source": [
    "## The Baseline: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, dataset, classes, batch_size, device, label=\"\"):\n",
    "    \"\"\"Evaluate CLIP model on given dataset and classes.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The CLIP model.\n",
    "        dataset (torch.utils.data.Dataset): The dataset to evaluate on.\n",
    "        classes (list): List of class ids to consider.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        device (str): Device to run the evaluation on.\n",
    "        label (str, optional): Label for progress bar. Defaults to none.\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the given dataset and classes.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap original class ids to contiguous ids starting from zero\n",
    "    class_ids = {cls: id for id, cls in enumerate(classes)}\n",
    "\n",
    "    # Apply and tokenize standard clip sentences\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in classes]).to(device)\n",
    "\n",
    "    # Encode text features and normalize\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # Compute accuracy of the model\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=f\"Evaluating on {label}\", leave=False):\n",
    "        target = torch.Tensor([class_ids[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Encode image features and normalize\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Predict class by finding the text feature with highest similarity\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions/len(dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nComputing CLIP zero-shot accuracy on base and novel classes...\")\n",
    "base_acc = test(model=model, dataset=base_test_set, classes=base_classes, batch_size=128, device=device, label=\"base classes\")\n",
    "novel_acc = test(model=model, dataset=novel_test_set, classes=novel_classes, batch_size=128, device=device, label=\"novel classes\")\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "print(\"\\nComputation done.\\n\")\n",
    "\n",
    "print(f\"Zero-shot accuracy on base classes: {base_acc*100:.2f}%\")\n",
    "print(f\"Zero-shot accuracy on novel classes: {novel_acc*100:.2f}%\")\n",
    "print(f\"Harmonic Mean: {hm*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39653",
   "metadata": {},
   "source": [
    "## Our Approach: Proto-guided CoCoOp with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e895bd",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32dbb4",
   "metadata": {},
   "source": [
    "### CoCoOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46a6a4",
   "metadata": {},
   "source": [
    "### Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a2aef",
   "metadata": {},
   "source": [
    "### Prototypes Generation\n",
    "\n",
    "We construct **class prototypes** from CLIP image embeddings of training samples.\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Use **frozen CLIP** (not the adapted model) to preserve zero-shot knowledge\n",
    "- Compute prototypes from **both normal and augmented samples** for better coverage\n",
    "- **L2-normalize** embeddings before averaging and after\n",
    "\n",
    "**At Inference:**\n",
    "- Compute prototype similarity: $\\text{sim}_{\\text{proto}}(x, c) = \\frac{f(x) \\cdot p_c}{\\|f(x)\\| \\|p_c\\|}$\n",
    "- Fuse with CoCoOp logits: $\\text{logits}_{\\text{final}} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{proto}}$\n",
    "- The fusion weight $\\alpha$ controls the trade-off between prompt-based and prototype-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transform for prototype construction\n",
    "aug_view_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                     (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Class to apply transform to an element of the dataset\n",
    "class TransformView(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.subset[idx]\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "# Build prototypes from augumented dataset\n",
    "@torch.no_grad()\n",
    "def build_prototypes(model, dataset, base_classes, device='cuda'):\n",
    "    \"\"\"\n",
    "    Build class prototypes from image embeddings extracted using frozen CLIP.\n",
    "\n",
    "    Args:\n",
    "        model: Frozen CLIP model used to extract image features.\n",
    "        dataset: Dataset containing images and labels.\n",
    "        base_classes: List of base class ids to build prototypes for.\n",
    "        device: Device to run computations on (default: 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (prototypes_dict, prototype_matrix):\n",
    "        - prototypes_dict: Dictionary mapping class_id -> prototype tensor (feature_dim,)\n",
    "        - prototype_matrix: Stacked tensor of shape (num_base_classes, feature_dim) for efficient inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "\n",
    "    for cls_id in base_classes:\n",
    "        if len(embeddings_per_class[cls_id]) == 0:\n",
    "            print(f\"Warning: no samples for class {cls_id}\")\n",
    "            continue\n",
    "\n",
    "        class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "        prototype = class_embeddings.mean(dim=0).to(device)\n",
    "        prototype = prototype / prototype.norm()\n",
    "\n",
    "        prototypes[cls_id] = prototype\n",
    "\n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix  # matrix of shape (num_base_classes, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d84f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 510 pool = 5610\n",
      "Extracting embeddings from 5610 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 88/88 [00:46<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 51 prototypes | Matrix shape: torch.Size([51, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw train dataset (PIL images)\n",
    "train_raw = load_split(\"train\", transform=None)\n",
    "\n",
    "# Build base subset indices on the same object (= avoid mismatched _labels across dataset instances)\n",
    "base_set = set(base_classes)\n",
    "base_idx = [i for i, y in enumerate(train_raw._labels) if y in base_set]  # uses Flowers102._labels\n",
    "base_train_raw = torch.utils.data.Subset(train_raw, base_idx)\n",
    "\n",
    "# Define transforms for original and augmented views\n",
    "orig_view = TransformView(base_train_raw, preprocess)\n",
    "\n",
    "num_samples = 10  # number of augmented views per original image\n",
    "views = [orig_view] + [TransformView(base_train_raw, aug_view_transform) for _ in range(num_samples)]\n",
    "\n",
    "# Create the prototype pool by concatenating all views\n",
    "proto_pool = torch.utils.data.ConcatDataset(views)\n",
    "\n",
    "print(\"N =\", len(orig_view), \"pool =\", len(proto_pool))\n",
    "\n",
    "# Build prototypes using frozen CLIP\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    model=model,\n",
    "    dataset=proto_pool,\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079cd1e",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder module adapts CLIP's text transformer for batched prompt embeddings.\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        # Reuse components from the loaded CLIP text encoder\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        \"\"\"\n",
    "        Encode batched prompt embeddings using CLIP's text transformer.\n",
    "        Args:\n",
    "            prompts: (batch_size, seq_len, dim) tensor of prompt embeddings\n",
    "            tokenized_prompts: (batch_size, seq_len) tensor of token ids\n",
    "        Returns:\n",
    "            (batch_size, proj_dim) tensor of encoded text features\n",
    "        \"\"\"\n",
    "        # prompts: (batch_tokens, seq_len, dim) positional embeddings already included below\n",
    "        # tokenized_prompts: token ids (used to pick the final token's embedding)\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)  # add positional embeddings\n",
    "        x = x.permute(1, 0, 2)  # transformer expects (seq_len, batch, dim)\n",
    "        x = self.transformer(x)  # run through CLIP transformer\n",
    "        x = x.permute(1, 0, 2)  # back to (batch, seq_len, dim)\n",
    "        x = self.ln_final(x).type(self.dtype)  # layer norm and cast\n",
    "        # select the embedding at the end-of-text token for each sequence, then project\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x  # (batch, proj_dim)\n",
    "\n",
    "\n",
    "# Prompt Learner generates per-class prompt embeddings, optionally conditioned on image features.\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the PromptLearner.\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of class names for the dataset.\n",
    "            n_ctx: Number of context tokens to learn.\n",
    "            ctx_init: Optional string to initialize context tokens.\n",
    "            device: Device to run the model on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]  # dimensionality of token embeddings\n",
    "        vis_dim = clip_model.visual.output_dim  # dimensionality of visual features\n",
    "        self.n_cls = len(classnames)\n",
    "        self.n_ctx = n_ctx\n",
    "        self.device = device\n",
    "\n",
    "        # Meta network: maps image features -> additive bias for context vectors.\n",
    "        hidden_dim = vis_dim // 16\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, hidden_dim)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(hidden_dim, ctx_dim))\n",
    "        ])).to(device)\n",
    "        \n",
    "        # Context initialization: either from provided text or random normal.\n",
    "        if ctx_init:  # if a string is provided, initialize context from its token embeddings\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).to(self.dtype)\n",
    "            # use tokens after the special start token (1:1+n_ctx)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # learnable context vectors initialized from N(0, 0.02)\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        # Make context vectors learnable parameters\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # Prepare tokenized prompts for all classes using the prefix and class names\n",
    "        ref_classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in ref_classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        \n",
    "        # Obtain static token embeddings for prefix and suffix parts (non-learnable buffers)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).to(self.dtype)\n",
    "            \n",
    "        # token_prefix: the special start token (e.g., [SOS]) for each class\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        # token_suffix: the remaining tokens after the learnable context tokens\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        # im_features: (batch, vis_dim)\n",
    "        batch_size = im_features.shape[0]\n",
    "        ctx = self.ctx.to(self.dtype).unsqueeze(0)  # (1, n_ctx, dim)\n",
    "        bias = self.meta_net(im_features).unsqueeze(1)  # (batch, 1, dim)\n",
    "        \n",
    "        # Add image-conditioned bias to the base context vectors\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, dim)\n",
    "        \n",
    "        # Expand prefix and suffix for batch and classes\n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (batch, n_cls, 1, dim)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (batch, n_cls, suffix_len, dim)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)  # (batch, n_cls, n_ctx, dim)\n",
    "        \n",
    "        # Concatenate tokens into full prompt embeddings per class per batch\n",
    "        return torch.cat([prefix, ctx_expanded, suffix], dim=2)  # (batch, n_cls, n_tokens, dim)\n",
    "\n",
    "\n",
    "# ProtoCoCoOp model: builds on CoCoOp-style prompt learning and supports prototype fusion at inference.\n",
    "class ProtoCoCoOp(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, base_ids, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "       \"\"\"\n",
    "        Initialize the ProtoCoCoOp model.\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of class names for the dataset.\n",
    "            base_ids: List of indices of base classes for prototype fusion.\n",
    "            n_ctx: Number of context tokens to learn.\n",
    "            ctx_init: Optional string to initialize context tokens.\n",
    "            device: Device to run the model on.\n",
    "       \"\"\"\n",
    "        super().__init__()\n",
    "        # CLIP logit scale and model references\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.clip_model = clip_model\n",
    "        self.dtype = self.clip_model.dtype\n",
    "        self.base_ids = torch.tensor(base_ids, device=device)  # indices of base classes for prototype fusion\n",
    "        self.device = device\n",
    "\n",
    "        # Encoders and prompt learner\n",
    "        self.image_encoder = self.clip_model.visual\n",
    "        self.text_encoder = TextEncoder(self.clip_model)\n",
    "        self.prompt_learner = PromptLearner(self.clip_model, classnames, n_ctx, ctx_init, device)\n",
    "\n",
    "        # Tokenized prompts for selecting projected text outputs\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "\n",
    "        # Prototype matrix (num_prototypes x dim) and fusion weight alpha set at inference\n",
    "        self.prototype_matrix = None\n",
    "        self.alpha = None            \n",
    "\n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.2):\n",
    "        # Store prototypes and fusion coefficient\n",
    "        self.prototype_matrix = prototype_matrix.to(self.device).type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, image, use_prototypes=False):\n",
    "        # image: (batch, 3, H, W)\n",
    "        image = image.to(self.device).type(self.dtype)\n",
    "        image_features = self.image_encoder(image)  # visual embedding\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "        # Generate per-class prompt embeddings conditioned on image features\n",
    "        prompts = self.prompt_learner(image_features)  # (batch, n_cls, n_tokens, dim)\n",
    "        B, C, T, D = prompts.shape\n",
    "        prompts = prompts.reshape(B * C, T, D).type(self.dtype)  # flatten for text encoder\n",
    "\n",
    "        # Repeat the stored tokenized prompt ids for each batch instance\n",
    "        tokenized = self.tokenized_prompts.to(prompts.device).repeat(B, 1)\n",
    "\n",
    "        # Encode text prompts and normalize\n",
    "        text_features = self.text_encoder(prompts, tokenized)  # (B*C, proj_dim)\n",
    "        text_features = text_features.reshape(B, C, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute CLIP-style logits: scaled cosine similarity\n",
    "        logits = self.logit_scale.exp() * (image_features.unsqueeze(1) @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # Optional prototype fusion during inference: add prototype logits to base-class logits\n",
    "        if use_prototypes and self.prototype_matrix is not None:\n",
    "            # proto_logits: (batch, num_prototypes) projected and scaled\n",
    "            proto_logits = self.logit_scale.exp() * (image_features @ self.prototype_matrix.T)\n",
    "\n",
    "            # Fuse prototypes into logits for base classes only\n",
    "            logits_base = logits[:, self.base_ids]\n",
    "            logits[:, self.base_ids] = logits_base + self.alpha * proto_logits\n",
    "\n",
    "        return logits  # (batch, n_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c59385",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7358d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, config, params, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        CoCoOp Trainer for training and evaluation.\n",
    "\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of all class names.\n",
    "            base_classes: List of base class ids.\n",
    "            config: Configuration dictionary for CoCoOp. Contains 'mode', 'n_ctx', 'ctx_init'.\n",
    "            params: Training parameters dictionary. Contains 'lr', 'momentum', 'weight_decay',\n",
    "                    'kd_alpha', 'temperature', 'num_epochs', 'tr_batch_size', 'ts_batch_size'.\n",
    "            device: Device to run the model on (default: \"cuda\").\n",
    "        \"\"\"\n",
    "        self.mode = config[\"mode\"].lower()\n",
    "        if self.mode == \"standard\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"kd\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = True\n",
    "        elif self.mode == \"proto\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"proto_kd\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = True\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}. Choose from 'standard', 'kd', 'proto', 'proto_kd'.\")\n",
    "        \n",
    "        print(f\"Initialized CoCoOpTrainer in '{self.mode}' mode | use_proto={self.use_proto} | use_kd={self.use_kd}\")\n",
    "\n",
    "        self.kd_alpha = params[\"kd_alpha\"]\n",
    "        self.temperature = params[\"temperature\"]\n",
    "        self.num_epochs = params[\"num_epochs\"]\n",
    "        self.tr_batch_size = params[\"tr_batch_size\"]\n",
    "        self.ts_batch_size = params[\"ts_batch_size\"]\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze CLIP model parameters (no fine-tuning of CLIP itself).\n",
    "        self.clip_model = clip_model.float().to(device).eval()\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Precompute normalized CLIP text features for all class prompts.\n",
    "        with torch.no_grad():\n",
    "            prompts = [f\"a photo of a {c}\" for c in classnames]\n",
    "            tokens = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "            text_features = self.clip_model.encode_text(tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.clip_text_features = text_features\n",
    "\n",
    "        # Initialize CoCoOp model (prompt learner + optional prototype components).\n",
    "        self.model = ProtoCoCoOp(\n",
    "            self.clip_model,\n",
    "            classnames,\n",
    "            base_ids=base_classes,\n",
    "            n_ctx=config[\"n_ctx\"],\n",
    "            ctx_init=config[\"ctx_init\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimize only the prompt learner parameters.\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(),\n",
    "            lr=params[\"lr\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # Cosine annealing LR scheduler over epochs.\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs)\n",
    "\n",
    "        # Base class ids tensor on device.\n",
    "        self.base_ids = torch.tensor(base_classes, device=device)\n",
    "\n",
    "        # Map global class indices -> compact base-class indices; -1 for non-base classes.\n",
    "        num_total_classes = len(classnames)\n",
    "        self.label_map = torch.full((num_total_classes,), -1, dtype=torch.long, device=device)\n",
    "        self.label_map[self.base_ids] = torch.arange(len(base_classes), device=device)\n",
    "\n",
    "    # Knowledge Distillation Loss computation (KL between teacher probs and student log-probs).\n",
    "    def compute_kd_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute the knowledge distillation loss between student and teacher logits.\n",
    "        Args:\n",
    "            student_logits: Logits from the student model.\n",
    "            teacher_logits: Logits from the teacher model.\n",
    "        Returns:\n",
    "            KL divergence loss value.\n",
    "        \"\"\"\n",
    "        T = self.temperature\n",
    "\n",
    "        student_log_probs = F.log_softmax(student_logits / T, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / T, dim=-1)\n",
    "\n",
    "        # Multiply by T^2 as in temperature-scaled KD formulation.\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\") * (T ** 2)\n",
    "    \n",
    "    # Training function\n",
    "    def train(self, dataset):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataset: Training dataset.\n",
    "        Returns:\n",
    "            Average training loss over the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # DataLoader for training.\n",
    "        train_loader = DataLoader(dataset, batch_size=self.tr_batch_size, shuffle=True, num_workers=1, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training [{self.mode}]\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (no prototype fusion during training here).\n",
    "            logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Compute CE loss restricted to base classes.\n",
    "            base_logits = logits[:, self.base_ids]\n",
    "            targets = self.label_map[labels]\n",
    "\n",
    "            loss_ce = F.cross_entropy(base_logits, targets)\n",
    "\n",
    "            # Optionally compute KD loss using frozen CLIP as teacher.\n",
    "            if self.use_kd:\n",
    "                with torch.no_grad():\n",
    "                    img_feat = self.model.clip_model.encode_image(images)\n",
    "                    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    teacher_logits = (self.model.clip_model.logit_scale.exp() * img_feat @ self.clip_text_features.T)\n",
    "\n",
    "                loss_kd = self.compute_kd_loss(logits, teacher_logits)\n",
    "\n",
    "                # Weighted combination of CE and KD losses.\n",
    "                loss = (1 - self.kd_alpha) * loss_ce + self.kd_alpha * loss_kd\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return total_loss/total_samples\n",
    "    \n",
    "    # Evaluation function\n",
    "    @torch.no_grad()\n",
    "    def test(self, dataset, class_ids, use_prototypes=False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the given dataset. \n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to evaluate on.\n",
    "            class_ids: List of class ids to consider during evaluation.\n",
    "            use_prototypes: Whether to apply prototype fusion at inference.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (accuracy, average loss) over the dataset.        \n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Build mapping from global class index -> compact evaluation index.\n",
    "        class_ids = torch.tensor(class_ids, device=self.device)\n",
    "        mapping = torch.full((len(self.clip_text_features),), -1, dtype=torch.long, device=self.device)\n",
    "        mapping[class_ids] = torch.arange(len(class_ids), device=self.device)\n",
    "\n",
    "        # DataLoader for testing.\n",
    "        test_loader = DataLoader(dataset, batch_size=self.ts_batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        correct_predictions = 0\n",
    "        predictions = 0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Forward pass with optional prototype fusion.\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "                \n",
    "            # Restrict logits to requested class subset.\n",
    "            logits = logits[:, class_ids]\n",
    "\n",
    "            targets = mapping[labels]\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            correct_predictions += (preds == targets).sum().item()\n",
    "            predictions += images.size(0)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        return (correct_predictions/predictions, total_loss/predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda22b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
    "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 10\n",
    "\n",
    "**What happens:**\n",
    "- The `PromptLearner` adapts its 4 context vectors to the Flowers102 dataset.\n",
    "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
    "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~2.5 - 3.5\n",
    "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
    "- Training time: ~2-4 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: choose training mode and prompt settings\n",
    "CURRENT_MODE = \"proto_kd\"  # options: \"standard\", \"proto\", \"kd\", \"proto_kd\"\n",
    "\n",
    "config = {\n",
    "    \"mode\": CURRENT_MODE,\n",
    "    \"n_ctx\": 8,      # number of learnable context tokens\n",
    "    \"ctx_init\": None # optional string to initialize context from text (None -> random init)\n",
    "}\n",
    "\n",
    "# Training hyperparameters and algorithm switches\n",
    "params = {\n",
    "    \"lr\": 0.002,            # SGD LR for prompt parameters\n",
    "    \"momentum\": 0.9,        # accelerate convergence and smooth updates\n",
    "    \"weight_decay\": 5e-4,   # small weight decay to regularize learned context vectors slightly\n",
    "    \"tr_batch_size\": 1,     # training batch size (small to fit prompt learner + meta-net)  \n",
    "    \"ts_batch_size\": 32,    # evaluation batch size\n",
    "    \"patience_init\": 5,     # top after N non-improving epochs\n",
    "    \"num_epochs\": 15,       # maximum epochs to allow sufficient prompt adaptation\n",
    "    \"proto_alpha\": 0.2,     # prototype fusion weight \n",
    "    \"kd_alpha\": 0.3,        # KD loss weight  \n",
    "    \"temperature\": 2.0      # KD temperature: >1 for stable distillation\n",
    "}\n",
    "\n",
    "# Initialize trainer with frozen CLIP and prompt learner\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,            # pretrained CLIP model object\n",
    "    classnames=CLASS_NAMES,      # list of class name strings\n",
    "    base_classes=base_classes,   # list of base-class integer ids\n",
    "    config=config,\n",
    "    params=params,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Container to record training progress and best model\n",
    "results = {\n",
    "    \"mode\": config[\"mode\"],\n",
    "    \"sampled_epochs\": [],   # epochs visited\n",
    "    \"val_accs\": [],         # validation accuracies per sampled epoch\n",
    "    \"best_val_acc\": 0.0,    # best validation accuracy seen so far\n",
    "    \"losses_train\": [],     # training losses per epoch\n",
    "    \"losses_val\": [],       # validation losses per epoch\n",
    "}\n",
    "\n",
    "# Initialize early stopping counter\n",
    "patience = params[\"patience_init\"]\n",
    "\n",
    "# Training loop with periodic evaluation and model checkpointing on improvement\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING LOOP (Patience: {params['patience_init']}) | Mode: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    results[\"sampled_epochs\"].append(epoch)\n",
    "\n",
    "    # Training step: updates only the prompt learner parameters\n",
    "    train_loss = trainer.train(base_train_set)\n",
    "    print(f\"\\nEpoch {epoch+1}/{trainer.num_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    results[\"losses_train\"].append(np.asarray(train_loss).mean())\n",
    "\n",
    "    # Evaluation step: measure performance on base validation set (no prototype fusion here)\n",
    "    val_acc, val_loss = trainer.test(base_val_set, base_classes, use_prototypes=False)\n",
    "    print(f\" Validation Acc: {val_acc*100:.2f}% | Val Loss: {np.asarray(val_loss).mean():.4f}\")\n",
    "\n",
    "    results[\"val_accs\"].append(val_acc)\n",
    "    results[\"losses_val\"].append(np.asarray(val_loss).mean())\n",
    "\n",
    "    # If validation improves, save checkpoint and reset patience\n",
    "    if val_acc > results[\"best_val_acc\"]:\n",
    "        results[\"best_val_acc\"] = val_acc\n",
    "        patience = params[\"patience_init\"]  # reset patience\n",
    "\n",
    "        save_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "        model_data = {\n",
    "            \"model_state_dict\": trainer.model.state_dict(),\n",
    "            \"optimizer_state_dict\": trainer.optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": config,\n",
    "            \"params\": params,\n",
    "            \"results\": results\n",
    "        }\n",
    "        torch.save(model_data, save_path)\n",
    "        print(f\"[BEST MODEL SAVED] Acc: {val_acc*100:.2f}%\")\n",
    "    else:\n",
    "        # No improvement: decrement patience and possibly stop early\n",
    "        patience -= 1\n",
    "        print(f\" [No Improvement | Patience left: {patience}]\")\n",
    "        if patience == 0:\n",
    "            print(f\"\\nEARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training complete. Best Val Acc: {results['best_val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a720cf7",
   "metadata": {},
   "source": [
    "### Training results logging and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and log utilities for training experiments\n",
    "\n",
    "def plot_results(results, plots_path):\n",
    "    \"\"\"\n",
    "    Save training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing training stats:\n",
    "                        - \"sampled_epochs\": list of epoch indices\n",
    "                        - \"losses_train\": list of training losses per epoch\n",
    "                        - \"losses_val\": list of validation losses per epoch\n",
    "        plots_path (str): Directory where the plot image will be saved.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    # Plot training and validation loss with markers for readability\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_train\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_val\"], label=\"Validation Loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss ({config['mode']})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to disk using experiment mode in filename\n",
    "    filename = f\"{config['mode']}_training_plot.png\"\n",
    "    filepath = os.path.join(plots_path, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_results(params, config, results, log_path):\n",
    "    \"\"\"\n",
    "    Append a CSV row summarizing experiment settings and best validation result.\n",
    "\n",
    "    Args:\n",
    "        params (dict): Training hyperparameters and settings.\n",
    "        config (dict): Prompt/trainer configuration.\n",
    "        results (dict): Collected training results including \"best_val_acc\".\n",
    "        log_path (str): Path to the CSV log file.\n",
    "    \"\"\"\n",
    "    # Fields recorded for each experiment run\n",
    "    log_fields = [\n",
    "        \"model_type\",\n",
    "        \"num_epochs\",\n",
    "        \"lr\",\n",
    "        \"tr_batch_size\",\n",
    "        \"ts_batch_size\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"kd_alpha\",\n",
    "        \"proto_alpha\",\n",
    "        \"temperature\",\n",
    "        \"n_ctx\",\n",
    "        \"base_accuracy\"\n",
    "    ]\n",
    "\n",
    "    # If CSV does not exist, create it and write header\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, mode=\"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Append a single row summarizing this run\n",
    "    with open(log_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "        writer.writerow({\n",
    "            \"model_type\": results.get(\"mode\", config.get(\"mode\")),\n",
    "            \"num_epochs\": len(results.get(\"sampled_epochs\", [])),\n",
    "            \"lr\": params.get(\"lr\"),\n",
    "            \"tr_batch_size\": params.get(\"tr_batch_size\"),\n",
    "            \"ts_batch_size\": params.get(\"ts_batch_size\"),\n",
    "            \"momentum\": params.get(\"momentum\"),\n",
    "            \"weight_decay\": params.get(\"weight_decay\"),\n",
    "            \"kd_alpha\": params.get(\"kd_alpha\"),\n",
    "            \"proto_alpha\": params.get(\"proto_alpha\"),\n",
    "            \"temperature\": params.get(\"temperature\"),\n",
    "            \"n_ctx\": config.get(\"n_ctx\"),\n",
    "            \"base_accuracy\": f\"{results.get('best_val_acc', 0.0)*100:.2f}\"\n",
    "        })\n",
    "\n",
    "\n",
    "# Generate and persist training plot and CSV log for this run\n",
    "plot_results(results, plots_path)\n",
    "\n",
    "log_filepath = os.path.join(logs_path, \"training_log.csv\")\n",
    "log_results(params, config, results, log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b97fbf",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We'll test the model with:\n",
    "1. **Test Base** - CoCoOp only vs CoCoOp + Prototypes\n",
    "2. **Test Novel** - CoCoOp only (no prototypes for novel classes)\n",
    "\n",
    "Computing Harmonic Mean between them to evaluate the trade-off.\n",
    "\n",
    "**Note:** Prototypes are only available for base classes (built from training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best trained checkpoint (if available) and evaluate final model.\n",
    "# The checkpoint file contains the saved model state, optimizer state, epoch and config/params used.\n",
    "best_model_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"\\nLoading best model from {best_model_path}...\")\n",
    "    # Load checkpoint dictionary (contains model_state_dict, optimizer_state_dict, config, params, results)\n",
    "    model_data = torch.load(best_model_path, weights_only=False)\n",
    "\n",
    "    # Restore training configuration and hyperparameters used for the saved model\n",
    "    config = model_data[\"config\"]\n",
    "    params = model_data[\"params\"] \n",
    "\n",
    "    # Re-create trainer with the exact config/params to ensure compatibility, then load weights\n",
    "    trainer = CoCoOpTrainer(\n",
    "        clip_model=model,\n",
    "        classnames=CLASS_NAMES,\n",
    "        base_classes=base_classes,\n",
    "        config=config,\n",
    "        params=params,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "\n",
    "    print(\"Best model loaded successfully.\")\n",
    "else:\n",
    "    # If no checkpoint is found, continue with the current in-memory trainer/model\n",
    "    print(\"Warning: Best model checkpoint not found! Using current model state.\")\n",
    "\n",
    "# If prototype fusion mode was used, attach the precomputed prototype matrix and fusion weight.\n",
    "# Prototypes were computed only for base classes and will be fused into base-class logits at inference.\n",
    "if trainer.use_proto:\n",
    "    print(\"Setting prototypes for inference...\")\n",
    "    trainer.model.set_prototypes(prototype_matrix, alpha=params[\"proto_alpha\"])\n",
    "\n",
    "# Evaluate on base and novel test splits. Prototype fusion applied only when enabled for the trainer.\n",
    "base_acc, _ = trainer.test(base_test_set, base_classes, use_prototypes=trainer.use_proto)\n",
    "novel_acc, _ = trainer.test(novel_test_set, novel_classes, use_prototypes=False)\n",
    "\n",
    "# Compute harmonic mean to assess trade-off between base and novel performance.\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "# Nicely formatted summary of final results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RESULTS for MODE: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccb54",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2b1c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79ce58",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- CLIP\n",
    "\n",
    "    - Radford et al., 2021 — Learning Transferable Visual Models From Natural Language Supervision\n",
    "\n",
    "- CoOp / CoCoOp\n",
    "\n",
    "    - Zhou et al., 2022 — Learning to Prompt for Vision-Language Models (CoOp)\n",
    "\n",
    "- Zhou et al., 2022 — Conditional Prompt Learning for Vision-Language Models (CoCoOp)\n",
    "\n",
    "    - Tip-Adapter\n",
    "\n",
    "- Zhang et al., 2022 — Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification\n",
    "\n",
    "    - Proto-based CLIP adaptation\n",
    "\n",
    "- Zhang et al., 2022 — Tip-Adapter-F (fine-tuned version)\n",
    "\n",
    "    - Some works refer to this direction as “cache-based adaptation” or “prototype adaptation”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
