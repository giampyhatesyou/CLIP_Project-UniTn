{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a6108",
   "metadata": {},
   "source": [
    "# ProtoCoCoOp: Few-shot adaptation of CLIP\n",
    "\n",
    "Deep Learning Course Project - a.y. 2024/2025\n",
    "\n",
    "Authors:\n",
    "- Andrea Giampietro - xxxxxx\n",
    "- Marco Gandolfi - 258017\n",
    "- Stefano Camposilvan - 257848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf4e7",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- TESTING OF FINAL MODEL\n",
    "    - wrt to baseline CLIP and base CoCoOp\n",
    "\n",
    "- ABLATION STUDY/PERFORMANCE COMPARISON\n",
    "    - train and eval baseline (CLIP)\n",
    "    - train and eval CoCoOp alone \n",
    "    - train and eval CoCoOp + proto\n",
    "    - train and eval CoCoOp + KD \n",
    "    - train and eval full model (CoCoOp + proto + KD)\n",
    "\n",
    "    note: ADD IMAGES, GRAPHS, etc...\n",
    "\n",
    "- PROPERLY COMMENT CODE\n",
    "\n",
    "- PROPERLY EXPLAIN THEORY/RATIONALE/IDEAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b28b00",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- Introduction\n",
    "- Setup\n",
    "- The Baseline: CLIP\n",
    "- Our Approach\n",
    "    - Overview\n",
    "    - CoCoOp\n",
    "    - Knowledge distillation\n",
    "    - Prototypes Generation\n",
    "    - Implementation\n",
    "- Results and Discussion\n",
    "- Conclusions\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236abd2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719db597",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36996",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c3fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP already installed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from shutil import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Install CLIP if not already installed\n",
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d101d",
   "metadata": {},
   "source": [
    "### Paths and constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4faaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PATHS DEFINITION --\n",
    "# Directory for dataset\n",
    "data_path = \"data\"\n",
    "os.makedirs(data_path, exist_ok=True) # for dataset storage\n",
    "\n",
    "# Directories for saving results\n",
    "models_path = \"results/models\"\n",
    "os.makedirs(models_path, exist_ok=True) # for saving models\n",
    "logs_path = \"results/logs\"\n",
    "os.makedirs(logs_path, exist_ok=True) # for saving logs\n",
    "plots_path = \"results/plots\"\n",
    "os.makedirs(plots_path, exist_ok=True) # for saving plots\n",
    "\n",
    "# -- CONSTANTS DEFINITION --\n",
    "# Class names for Flowers102 dataset\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# -- REPRODUCIBILITY SETUP --\n",
    "# Function to set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Worker initialization function for DataLoader\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b6d4",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec9e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA PREPARATION FUNCTIONS --\n",
    "# Load specific split of Flowers102 dataset, with given transformation\n",
    "def load_split(split, transform):\n",
    "    \"\"\"Load Flowers102 dataset split with given transformation.\n",
    "    Args:\n",
    "        split (str): One of \"train\", \"val\", or \"test\".\n",
    "        transform (callable): Transformation to apply to the images.\n",
    "    Returns:\n",
    "        torchvision.datasets.Flowers102: The requested dataset split.\n",
    "    \"\"\"\n",
    "    return torchvision.datasets.Flowers102(root=data_path, split=split, download=True, transform=transform)\n",
    "\n",
    "# Load Flowers102 dataset and return train, val, test sets\n",
    "def get_data(transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        transform (callable, optional): Transformation to apply to the images. Defaults to None.\n",
    "    Returns:\n",
    "        tuple: (train_set, val_set, test_set) as torchvision.datasets.Flowers102 instances.\n",
    "    \"\"\"\n",
    "    train = load_split(\"train\", transform)\n",
    "    val = load_split(\"val\", transform)\n",
    "    test = load_split(\"test\", transform)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Split dataset classes into base and novel classes\n",
    "def split_classes(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present in the dataset.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split classes from.\n",
    "    Returns:\n",
    "        tuple: (base_classes, novel_classes) as lists of class ids.\n",
    "    \"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "\n",
    "    # Split classes into base and novel (first half and second half)\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "# Split dataset into base and novel datasets\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Split dataset into base and novel datasets based on provided base classes.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split.\n",
    "        base_classes (list): List of class ids considered as base classes.\n",
    "    Returns:\n",
    "        tuple: (base_dataset, novel_dataset) as torch.utils.data.Subset instances.\n",
    "    \"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d7d0dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load CLIP model and preprocessing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/16\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load dataset and split into base and novel datasets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m get_data(transform\u001b[38;5;241m=\u001b[39mpreprocess)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip' is not defined"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Load dataset and split into base and novel datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# Get base and novel classes from the test set\n",
    "base_classes, novel_classes = split_classes(test_set)\n",
    "classes = base_classes + novel_classes\n",
    "\n",
    "# Get class names\n",
    "base_class_names = [CLASS_NAMES[i] for i in base_classes]\n",
    "print(f\"Base classes ({len(base_classes)}): {base_class_names}\")\n",
    "novel_class_names = [CLASS_NAMES[i] for i in novel_classes]\n",
    "print(f\"Novel classes ({len(novel_classes)}): {novel_class_names}\")\n",
    "print(f\"All classes: ({len(classes)}: { [CLASS_NAMES[i] for i in classes] }\")\n",
    "\n",
    "# Create base and novel datasets\n",
    "base_train_set, _ = split_data(train_set, base_classes)\n",
    "base_val_set, _ = split_data(val_set, base_classes)\n",
    "base_test_set, novel_test_set = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38062e",
   "metadata": {},
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**goal:** maximize HM between `base_acc_cocoop` and `novel_acc_cocoop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc73d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonic Mean Calculation\n",
    "def harmonic_mean(a, b):\n",
    "    return 2 * a * b / (a + b) if (a + b) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78449",
   "metadata": {},
   "source": [
    "## The Baseline: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, dataset, classes, batch_size, device, label=\"\"):\n",
    "    \"\"\"Evaluate CLIP model on given dataset and classes.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The CLIP model.\n",
    "        dataset (torch.utils.data.Dataset): The dataset to evaluate on.\n",
    "        classes (list): List of class ids to consider.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        device (str): Device to run the evaluation on.\n",
    "        label (str, optional): Label for progress bar. Defaults to none.\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the given dataset and classes.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap original class ids to contiguous ids starting from zero\n",
    "    class_ids = {cls: id for id, cls in enumerate(classes)}\n",
    "\n",
    "    # Apply and tokenize standard clip sentences\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in classes]).to(device)\n",
    "\n",
    "    # Encode text features and normalize\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # Compute accuracy of the model\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=f\"Evaluating on {label}\", leave=False):\n",
    "        target = torch.Tensor([class_ids[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Encode image features and normalize\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Predict class by finding the text feature with highest similarity\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions/len(dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nComputing CLIP zero-shot accuracy on base and novel classes...\")\n",
    "base_acc = test(model=model, dataset=base_test_set, classes=base_classes, batch_size=128, device=device, label=\"base classes\")\n",
    "novel_acc = test(model=model, dataset=novel_test_set, classes=novel_classes, batch_size=128, device=device, label=\"novel classes\")\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "print(\"\\nComputation done.\\n\")\n",
    "\n",
    "print(f\"Zero-shot accuracy on base classes: {base_acc*100:.2f}%\")\n",
    "print(f\"Zero-shot accuracy on novel classes: {novel_acc*100:.2f}%\")\n",
    "print(f\"Harmonic Mean: {hm*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39653",
   "metadata": {},
   "source": [
    "## Our Approach: Proto-guided CoCoOp with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e895bd",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32dbb4",
   "metadata": {},
   "source": [
    "### CoCoOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46a6a4",
   "metadata": {},
   "source": [
    "### Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a2aef",
   "metadata": {},
   "source": [
    "### Prototypes Generation\n",
    "\n",
    "We construct **class prototypes** from CLIP image embeddings of training samples.\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Use **frozen CLIP** (not the adapted model) to preserve zero-shot knowledge\n",
    "- Compute prototypes from **both normal and augmented samples** for better coverage\n",
    "- **L2-normalize** embeddings before averaging and after\n",
    "\n",
    "**At Inference:**\n",
    "- Compute prototype similarity: $\\text{sim}_{\\text{proto}}(x, c) = \\frac{f(x) \\cdot p_c}{\\|f(x)\\| \\|p_c\\|}$\n",
    "- Fuse with CoCoOp logits: $\\text{logits}_{\\text{final}} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{proto}}$\n",
    "- The fusion weight $\\alpha$ controls the trade-off between prompt-based and prototype-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transform for prototype construction\n",
    "aug_view_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                     (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Class to apply transform to an element of the dataset\n",
    "class TransformView(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.subset[idx]\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "# Build prototypes from augumented dataset\n",
    "@torch.no_grad()\n",
    "def build_prototypes(model, dataset, base_classes, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "\n",
    "    for cls_id in base_classes:\n",
    "        if len(embeddings_per_class[cls_id]) == 0:\n",
    "            print(f\"Warning: no samples for class {cls_id}\")\n",
    "            continue\n",
    "\n",
    "        class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "        prototype = class_embeddings.mean(dim=0).to(device)\n",
    "        prototype = prototype / prototype.norm()\n",
    "\n",
    "        prototypes[cls_id] = prototype\n",
    "\n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix  # matrix of shape (num_base_classes, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d84f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 510 pool = 5610\n",
      "Extracting embeddings from 5610 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 88/88 [00:46<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 51 prototypes | Matrix shape: torch.Size([51, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw train dataset (PIL images)\n",
    "train_raw = load_split(\"train\", transform=None)\n",
    "\n",
    "# Build base subset indices on the same object (= avoid mismatched _labels across dataset instances)\n",
    "base_set = set(base_classes)\n",
    "base_idx = [i for i, y in enumerate(train_raw._labels) if y in base_set]  # uses Flowers102._labels\n",
    "base_train_raw = torch.utils.data.Subset(train_raw, base_idx)\n",
    "\n",
    "# Define transforms for original and augmented views\n",
    "orig_view = TransformView(base_train_raw, preprocess)\n",
    "\n",
    "num_samples = 10  # number of augmented views per original image\n",
    "views = [orig_view] + [TransformView(base_train_raw, aug_view_transform) for _ in range(num_samples)]\n",
    "\n",
    "# Create the prototype pool by concatenating all views\n",
    "proto_pool = torch.utils.data.ConcatDataset(views)\n",
    "\n",
    "print(\"N =\", len(orig_view), \"pool =\", len(proto_pool))\n",
    "\n",
    "# Build prototypes using frozen CLIP\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    model=model,\n",
    "    dataset=proto_pool,\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079cd1e",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder module definition\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "# Prompt Learner module definition\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0] # Dimension of context vectors\n",
    "        vis_dim = clip_model.visual.output_dim # Dimension of visual features\n",
    "        self.n_cls = len(classnames)\n",
    "        self.n_ctx = n_ctx\n",
    "        self.device = device\n",
    "\n",
    "        # Meta network to generate context bias from visual features\n",
    "        hidden_dim = vis_dim // 16\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, hidden_dim)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(hidden_dim, ctx_dim))\n",
    "        ])).to(device)\n",
    "        \n",
    "        # Context Initialization\n",
    "        if ctx_init: # If context initialization is provided (i.e. a string)\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).to(self.dtype)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        # Context vectors as learnable parameters\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        ref_classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in ref_classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).to(self.dtype)\n",
    "            \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :]) # Register prefix tokens as buffer (non-learnable)\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :]) # Register suffix tokens as buffer (non-learnable)\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        batch_size = im_features.shape[0]\n",
    "        ctx = self.ctx.to(self.dtype).unsqueeze(0)\n",
    "        bias = self.meta_net(im_features).unsqueeze(1)\n",
    "        \n",
    "        ctx_shifted = ctx + bias  # Shift context by adding bias from meta network\n",
    "        \n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)\n",
    "        \n",
    "        return torch.cat([prefix, ctx_expanded, suffix], dim=2) # (batch, n_cls, n_tokens, dim)\n",
    "\n",
    "# ProtoCoCoOp model definition, extending CoCoOp with optional prototype residuals at inference time\n",
    "class ProtoCoCoOp(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, base_ids, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.clip_model = clip_model\n",
    "        self.dtype = self.clip_model.dtype\n",
    "        self.base_ids = torch.tensor(base_ids, device=device)\n",
    "        self.device = device\n",
    "\n",
    "        self.image_encoder = self.clip_model .visual\n",
    "        self.text_encoder = TextEncoder(self.clip_model)\n",
    "        self.prompt_learner = PromptLearner(self.clip_model, classnames, n_ctx, ctx_init, device)\n",
    "\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "\n",
    "        self.prototype_matrix = None\n",
    "        self.alpha = None            \n",
    "\n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.2):\n",
    "        self.prototype_matrix = prototype_matrix.to(self.device).type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, image, use_prototypes=False):\n",
    "        image = image.to(self.device).type(self.dtype)\n",
    "        image_features = self.image_encoder(image)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        B, C, T, D = prompts.shape\n",
    "        prompts = prompts.reshape(B * C, T, D).type(self.dtype)\n",
    "\n",
    "        tokenized = self.tokenized_prompts.to(prompts.device).repeat(B, 1)\n",
    "\n",
    "        text_features = self.text_encoder(prompts, tokenized)\n",
    "        text_features = text_features.reshape(B, C, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = self.logit_scale.exp() * (image_features.unsqueeze(1) @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # Prototype fusion (inference only)\n",
    "        if use_prototypes and self.prototype_matrix is not None:\n",
    "            # Compute prototype logits\n",
    "            proto_logits = self.logit_scale.exp() * (image_features @ self.prototype_matrix.T)\n",
    "\n",
    "            # Fuse only base classes\n",
    "            logits_base = logits[:, self.base_ids]\n",
    "            logits[:, self.base_ids] = logits_base + self.alpha * proto_logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c59385",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7358d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, config, params, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        CoCoOp Trainer class for training and evaluation.\n",
    "\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of all class names.\n",
    "            base_classes: List of base class ids.\n",
    "            config: Configuration dictionary for CoCoOp.\n",
    "                    Contains 'mode', 'n_ctx', 'ctx_init'.\n",
    "            params: Training parameters dictionary.\n",
    "                    contains 'lr', 'momentum', 'weight_decay', 'kd_alpha', 'temperature', 'num_epochs'.\n",
    "            device: Device to run the model on (default: \"cuda\").\n",
    "        \"\"\"\n",
    "        self.mode = config[\"mode\"].lower()\n",
    "        if self.mode == \"standard\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"kd\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = True\n",
    "        elif self.mode == \"proto\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"proto_kd\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = True\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}. Choose from 'standard', 'kd', 'proto', 'proto_kd'.\")\n",
    "        \n",
    "        print(f\"Initialized CoCoOpTrainer in '{self.mode}' mode | use_proto={self.use_proto} | use_kd={self.use_kd}\")\n",
    "\n",
    "        self.kd_alpha = params[\"kd_alpha\"]\n",
    "        self.temperature = params[\"temperature\"]\n",
    "        self.num_epochs = params[\"num_epochs\"]\n",
    "        self.tr_batch_size = params[\"tr_batch_size\"]\n",
    "        self.ts_batch_size = params[\"ts_batch_size\"]\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze CLIP model\n",
    "        self.clip_model = clip_model.float().to(device).eval()\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Precompute CLIP model's text features\n",
    "        with torch.no_grad():\n",
    "            prompts = [f\"a photo of a {c}\" for c in classnames]\n",
    "            tokens = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "            text_features = self.clip_model.encode_text(tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.clip_text_features = text_features\n",
    "\n",
    "        # Model initialization\n",
    "        self.model = ProtoCoCoOp(\n",
    "            self.clip_model,\n",
    "            classnames,\n",
    "            base_ids=base_classes,\n",
    "            n_ctx=config[\"n_ctx\"],\n",
    "            ctx_init=config[\"ctx_init\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer definition\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(),\n",
    "            lr=params[\"lr\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler definition\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs)\n",
    "\n",
    "        # Base class ids tensor\n",
    "        self.base_ids = torch.tensor(base_classes, device=device)\n",
    "\n",
    "        # Safe label mapping\n",
    "        num_total_classes = len(classnames)\n",
    "        self.label_map = torch.full((num_total_classes,), -1, dtype=torch.long, device=device)\n",
    "        self.label_map[self.base_ids] = torch.arange(len(base_classes), device=device)\n",
    "\n",
    "    # Knowledge Distillation Loss computation\n",
    "    def compute_kd_loss(self, student_logits, teacher_logits):\n",
    "        T = self.temperature\n",
    "\n",
    "        student_log_probs = F.log_softmax(student_logits / T, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / T, dim=-1)\n",
    "\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\") * (T ** 2)\n",
    "    \n",
    "    # Training function\n",
    "    def train(self, dataset):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataset: Training dataset.\n",
    "        Returns:\n",
    "            Average training loss over the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Initialize Dataloader\n",
    "        train_loader = DataLoader(dataset, batch_size=self.tr_batch_size, shuffle=True, num_workers=1, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training [{self.mode}]\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model without prototype fusion\n",
    "            logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Compute Cross-Entropy Loss on base classes\n",
    "            base_logits = logits[:, self.base_ids]\n",
    "            targets = self.label_map[labels]\n",
    "\n",
    "            loss_ce = F.cross_entropy(base_logits, targets)\n",
    "\n",
    "            # Compute Knowledge Distillation Loss if enabled\n",
    "            if self.use_kd:\n",
    "                with torch.no_grad():\n",
    "                    img_feat = self.model.clip_model.encode_image(images)\n",
    "                    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    teacher_logits = (self.model.clip_model.logit_scale.exp() * img_feat @ self.clip_text_features.T)\n",
    "\n",
    "                loss_kd = self.compute_kd_loss(logits, teacher_logits)\n",
    "\n",
    "                loss = (1 - self.kd_alpha) * loss_ce + self.kd_alpha * loss_kd\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return total_loss/total_samples\n",
    "    \n",
    "    # Evaluation function\n",
    "    @torch.no_grad()\n",
    "    def test(self, dataset, class_ids, use_prototypes=False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the given dataset. \n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to evaluate on.\n",
    "            class_ids: List of class ids to consider during evaluation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (accuracy, average loss) over the dataset.        \n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Build mapping\n",
    "        class_ids = torch.tensor(class_ids, device=self.device)\n",
    "        mapping = torch.full((len(self.clip_text_features),), -1, dtype=torch.long, device=self.device)\n",
    "        mapping[class_ids] = torch.arange(len(class_ids), device=self.device)\n",
    "\n",
    "        # Initialize Dataloader\n",
    "        test_loader = DataLoader(dataset, batch_size=self.ts_batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        correct_predictions = 0\n",
    "        predictions = 0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Evaluation loop\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Forward pass through the model with optional prototype fusion\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "                \n",
    "            logits = logits[:, class_ids]\n",
    "\n",
    "            targets = mapping[labels]\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            correct_predictions += (preds == targets).sum().item()\n",
    "            predictions += images.size(0)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        return (correct_predictions/predictions, total_loss/predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda22b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
    "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 10\n",
    "\n",
    "**What happens:**\n",
    "- The `PromptLearner` adapts its 4 context vectors to the Flowers102 dataset.\n",
    "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
    "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~2.5 - 3.5\n",
    "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
    "- Training time: ~2-4 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CURRENT_MODE = \"standard\" # Select one of: \"standard\", \"proto\", \"kd\", \"proto_kd\"\n",
    "\n",
    "config = {\n",
    "    \"mode\": CURRENT_MODE,\n",
    "    \"n_ctx\": 8,\n",
    "    \"ctx_init\": None\n",
    "}\n",
    "\n",
    "# Define training hyperparameters\n",
    "params = {\n",
    "    \"lr\": 0.002,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"tr_batch_size\": 1,    # Training batch size\n",
    "    \"ts_batch_size\": 32,   # Testing batch size\n",
    "    \"patience_init\": 5,\n",
    "    \"num_epochs\": 15,\n",
    "    \"proto_alpha\": 0.2,    # Weight for prototype logits in ProtoCoCoOp\n",
    "    \"kd_alpha\": 0.3,       # Weight for KD Loss\n",
    "    \"temperature\": 2.0     # Softmax temperature for KD\n",
    "}\n",
    "\n",
    "# Initialization\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,            # pretrained CLIP model\n",
    "    classnames=CLASS_NAMES,      # all class names\n",
    "    base_classes=base_classes,   # base class ids\n",
    "    config=config,\n",
    "    params=params,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Results storage\n",
    "results = {\n",
    "    \"mode\": config[\"mode\"],\n",
    "    \"sampled_epochs\": [],\n",
    "    \"val_accs\": [],\n",
    "    \"best_val_acc\": 0.0,\n",
    "    \"losses_train\": [],\n",
    "    \"losses_val\": [],\n",
    "}\n",
    "\n",
    "# Initialize early stopping patience\n",
    "patience = params[\"patience_init\"]\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING LOOP (Patience: {params['patience_init']}) | Mode: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    results[\"sampled_epochs\"].append(epoch)\n",
    "\n",
    "    # Training Step\n",
    "    train_loss = trainer.train(base_train_set)\n",
    "    print(f\"\\nEpoch {epoch+1}/{trainer.num_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    results[\"losses_train\"].append(np.asarray(train_loss).mean())\n",
    "\n",
    "    # Evaluation Step\n",
    "    val_acc, val_loss = trainer.test(base_val_set, base_classes, use_prototypes=False)\n",
    "    print(f\" Validation Acc: {val_acc*100:.2f}% | Val Loss: {np.asarray(val_loss).mean():.4f}\")\n",
    "   \n",
    "    results[\"val_accs\"].append(val_acc)\n",
    "    results[\"losses_val\"].append(np.asarray(val_loss).mean())\n",
    "\n",
    "    # Early Stopping and checkpointing\n",
    "    if val_acc > results[\"best_val_acc\"]:\n",
    "        results[\"best_val_acc\"] = val_acc\n",
    "        patience = params[\"patience_init\"] # Reset patience\n",
    "        \n",
    "        # Save model data\n",
    "        save_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "        model_data = {\n",
    "            \"model_state_dict\": trainer.model.state_dict(),\n",
    "            \"optimizer_state_dict\": trainer.optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": config,\n",
    "            \"params\": params,\n",
    "            \"results\": results\n",
    "        }\n",
    "        torch.save(model_data, save_path)\n",
    "        print(f\"[BEST MODEL SAVED] Acc: {val_acc*100:.2f}%\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        print(f\" [No Improvement | Patience left: {patience}]\")\n",
    "        if patience == 0:\n",
    "            print(f\"\\nEARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training complete. Best Val Acc: {results['best_val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a720cf7",
   "metadata": {},
   "source": [
    "### Training results logging and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots training results\n",
    "def plot_results(results, plots_path):\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_train\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_val\"], label=\"Validation Loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Dev Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    filename = f\"{config['mode']}_training_plot.png\"\n",
    "    filepath = os.path.join(plots_path, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "# Logs training results\n",
    "def log_results(params, config, results, log_path):\n",
    "    # Log and save training results\n",
    "    log_fields = [\n",
    "        \"model_type\",\n",
    "        \"num_epochs\",\n",
    "        \"lr\",\n",
    "        \"tr_batch_size\",\n",
    "        \"ts_batch_size\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"kd_alpha\",\n",
    "        \"proto_alpha\",\n",
    "        \"temperature\",\n",
    "        \"n_ctx\",\n",
    "        \"base_accuracy\"\n",
    "    ]\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, mode=\"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "            writer.writeheader()\n",
    "    with open(log_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "        writer.writerow({\n",
    "            \"model_type\": results[\"mode\"],\n",
    "            \"num_epochs\": len(results[\"sampled_epochs\"]),\n",
    "            \"lr\": params[\"lr\"],\n",
    "            \"tr_batch_size\": params[\"tr_batch_size\"],\n",
    "            \"ts_batch_size\": params[\"ts_batch_size\"],\n",
    "            \"momentum\": params[\"momentum\"],\n",
    "            \"weight_decay\": params[\"weight_decay\"],\n",
    "            \"kd_alpha\": params.get(\"kd_alpha\"),\n",
    "            \"proto_alpha\": params.get(\"proto_alpha\"),\n",
    "            \"temperature\": params.get(\"temperature\"),\n",
    "            \"n_ctx\": config[\"n_ctx\"],\n",
    "            \"base_accuracy\": f\"{results['best_val_acc']*100:.2f}\"\n",
    "        })\n",
    "\n",
    "# Plot and log results\n",
    "plot_results(results, plots_path)\n",
    "\n",
    "log_filepath = os.path.join(logs_path, \"training_log.csv\")\n",
    "log_results(params, config, results, log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b97fbf",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We'll test the model with:\n",
    "1. **Test Base** - CoCoOp only vs CoCoOp + Prototypes\n",
    "2. **Test Novel** - CoCoOp only (no prototypes for novel classes)\n",
    "\n",
    "Computing Harmonic Mean between them to evaluate the trade-off.\n",
    "\n",
    "**Note:** Prototypes are only available for base classes (built from training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "best_model_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"\\nLoading best model from {best_model_path}...\")\n",
    "    # Load model data\n",
    "    model_data = torch.load(best_model_path, weights_only=False)\n",
    "\n",
    "    # Re-initialize trainer and load best model state\n",
    "    config = model_data[\"config\"]\n",
    "    params = model_data[\"params\"] \n",
    "\n",
    "    trainer = CoCoOpTrainer(\n",
    "        clip_model=model,\n",
    "        classnames=CLASS_NAMES,\n",
    "        base_classes=base_classes,\n",
    "        config=config,\n",
    "        params=params,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "\n",
    "    print(\"Best model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Warning: Best model checkpoint not found! Using current model state.\")\n",
    "\n",
    "if trainer.use_proto:\n",
    "    print(\"Setting prototypes for inference...\")\n",
    "    trainer.model.set_prototypes(prototype_matrix, alpha=params[\"proto_alpha\"])\n",
    "\n",
    "base_acc, _ = trainer.test(base_test_set, base_classes, use_prototypes=trainer.use_proto)\n",
    "novel_acc, _ = trainer.test(novel_test_set, novel_classes, use_prototypes=False)\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RESULTS for MODE: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccb54",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2b1c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79ce58",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- CLIP\n",
    "\n",
    "    - Radford et al., 2021 — Learning Transferable Visual Models From Natural Language Supervision\n",
    "\n",
    "- CoOp / CoCoOp\n",
    "\n",
    "    - Zhou et al., 2022 — Learning to Prompt for Vision-Language Models (CoOp)\n",
    "\n",
    "- Zhou et al., 2022 — Conditional Prompt Learning for Vision-Language Models (CoCoOp)\n",
    "\n",
    "    - Tip-Adapter\n",
    "\n",
    "- Zhang et al., 2022 — Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification\n",
    "\n",
    "    - Proto-based CLIP adaptation\n",
    "\n",
    "- Zhang et al., 2022 — Tip-Adapter-F (fine-tuned version)\n",
    "\n",
    "    - Some works refer to this direction as “cache-based adaptation” or “prototype adaptation”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
