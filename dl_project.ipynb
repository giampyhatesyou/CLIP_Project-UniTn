{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a6108",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b28b00",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- Introduction\n",
    "- The Baseline: CLIP\n",
    "- Our Approach\n",
    "    - Overview\n",
    "    - CoCoOp\n",
    "    - Prototypes Generation\n",
    "    - Knowledge distillation\n",
    "    - Implementation\n",
    "- Results and Discussion\n",
    "- Conclusions\n",
    "- Authors\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236abd2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45decb3d",
   "metadata": {},
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**goal:** maximize HM between `base_acc_cocoop` and `novel_acc_cocoop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    # Guard against zero to avoid division-by-zero errors\n",
    "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
    "        return 0.0\n",
    "    numerator = 2.0\n",
    "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719db597",
   "metadata": {},
   "source": [
    "## The Baseline: CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36996",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4faaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist (for saving models)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Function to set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Worker init function\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b6d4",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA PREPARATION FUNCTIONS --\n",
    "# Load specific split of Flowers102 dataset, with given transformation\n",
    "def load_split(split, transform):\n",
    "    \"\"\"Load Flowers102 dataset split with given transformation.\"\"\"\n",
    "    return torchvision.datasets.Flowers102(root=\"./data\", split=split, download=True, transform=transform)\n",
    "\n",
    "# Load Flowers102 dataset and return train, val, test sets\n",
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
    "    train = load_split(\"train\", transform)\n",
    "    val = load_split(\"val\", transform)\n",
    "    test = load_split(\"test\", transform)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Split dataset classes into base and novel classes\n",
    "def split_classes(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present in the dataset.\"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "# Split dataset into base and novel datasets\n",
    "def split_data(dataset, base_classes):\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Define class names for Flowers102 dataset\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Load dataset and split into base and novel datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# Get base and novel classes from the test set\n",
    "base_classes, novel_classes = split_classes(test_set)\n",
    "classes = base_classes + novel_classes\n",
    "\n",
    "# Get class names\n",
    "base_class_names = [CLASS_NAMES[i] for i in base_classes]\n",
    "novel_class_names = [CLASS_NAMES[i] for i in novel_classes]\n",
    "class_names = [CLASS_NAMES[i] for i in classes]\n",
    "\n",
    "# Create base and novel datasets\n",
    "base_train_set, _ = split_data(train_set, base_classes)\n",
    "base_val_set, _ = split_data(val_set, base_classes)\n",
    "base_test_set, novel_test_set = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78449",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, dataset, classes, batch_size, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Map original class ids to contiguous ids starting from zero\n",
    "    class_map = {cat: idx for idx, cat in enumerate(classes)}\n",
    "\n",
    "    # Apply and tokenize standard clip sentences\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in classes]).to(device)\n",
    "\n",
    "    # Encode text features and normalize\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Compute accuracy of the model on the dataset\n",
    "    tp = 0\n",
    "    for image, target in tqdm(dataloader):\n",
    "        target = torch.Tensor([class_map[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity between image and text features and keep the argmax for every image\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "\n",
    "        tp += (predicted_class == target).sum().item()\n",
    "\n",
    "    accuracy = tp/len(dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"Computing Zero-shot accuracy on both base and novel classes...\")\n",
    "zero_shot_base_accuracy = eval(model=model, dataset=base_test_set, classes=base_classes, batch_size=128, device=device)\n",
    "zero_shot_novel_accuracy = eval(model=model, dataset=novel_test_set, classes=novel_classes, batch_size=128, device=device)\n",
    "print(\"Computation done.\\n\")\n",
    "\n",
    "print(f\"Zero-shot accuracy on base classes: {zero_shot_base_accuracy*100:.2f}%\")\n",
    "print(f\"Zero-shot accuract on novel classes: {zero_shot_novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39653",
   "metadata": {},
   "source": [
    "## Our Approach: Proto-guided CoCoOp with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a2aef",
   "metadata": {},
   "source": [
    "### Prototypes Generation\n",
    "\n",
    "We construct **class prototypes** from CLIP image embeddings of training samples.\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Use **frozen CLIP** (not the adapted model) to preserve zero-shot knowledge\n",
    "- Compute prototypes from **both normal and augmented samples** for better coverage\n",
    "- **L2-normalize** embeddings before averaging and after\n",
    "\n",
    "**At Inference:**\n",
    "- Compute prototype similarity: $\\text{sim}_{\\text{proto}}(x, c) = \\frac{f(x) \\cdot p_c}{\\|f(x)\\| \\|p_c\\|}$\n",
    "- Fuse with CoCoOp logits: $\\text{logits}_{\\text{final}} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{proto}}$\n",
    "- The fusion weight $\\alpha$ controls the trade-off between prompt-based and prototype-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transform for prototype construction\n",
    "aug_view_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                     (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Applies a transform to the PIL image and returns (tensor, label)\n",
    "class TransformView(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.subset[idx]          # img is PIL.Image.Image\n",
    "        img = self.transform(img)          # must become a torch.Tensor\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "# Build prototypes from augumented dataset\n",
    "@torch.no_grad()\n",
    "def build_prototypes(model, dataset, base_classes, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "    present_classes = []\n",
    "\n",
    "    for cls_id in base_classes:\n",
    "        if len(embeddings_per_class[cls_id]) == 0:\n",
    "            print(f\"Warning: no samples for class {cls_id}\")\n",
    "            continue\n",
    "\n",
    "        class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "        prototype = class_embeddings.mean(dim=0).to(device)\n",
    "        prototype = prototype / prototype.norm()\n",
    "\n",
    "        prototypes[cls_id] = prototype\n",
    "        present_classes.append(cls_id)\n",
    "\n",
    "    prototype_matrix = torch.stack(\n",
    "        [prototypes[c] for c in present_classes]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"✓ Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw train dataset (PIL images)\n",
    "train_raw = load_split(\"train\", transform=None)\n",
    "\n",
    "# Build base subset indices on the same object (= avoid mismatched _labels across dataset instances)\n",
    "base_set = set(base_classes)\n",
    "base_idx = [i for i, y in enumerate(train_raw._labels) if y in base_set]  # uses Flowers102._labels\n",
    "base_train_raw = torch.utils.data.Subset(train_raw, base_idx)\n",
    "\n",
    "# Define transforms for original and augmented views\n",
    "orig_view = TransformView(base_train_raw, preprocess)\n",
    "\n",
    "num_samples = 10  # number of augmented views per original image\n",
    "views = [orig_view] + [TransformView(base_train_raw, aug_view_transform) for _ in range(num_samples)]\n",
    "\n",
    "# Create prototype pool by concatenating all views\n",
    "proto_pool = torch.utils.data.ConcatDataset(views)\n",
    "\n",
    "print(\"N =\", len(orig_view), \"pool =\", len(proto_pool))  # should be N*(1+num_samples)\n",
    "\n",
    "# Build prototypes using frozen CLIP\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    model=model,\n",
    "    dataset=proto_pool,\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079cd1e",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        # Extract [EOS] features\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        \n",
    "        # Context Initialization\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(torch.float16)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float16)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        \n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [f\"{prompt_prefix} {name}.\" for name in classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(torch.float16)\n",
    "            \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
    "        self.n_cls, self.n_ctx = n_cls, n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        batch_size = im_features.shape[0]\n",
    "        bias = self.meta_net(im_features).unsqueeze(1)\n",
    "        ctx_shifted = self.ctx.unsqueeze(0) + bias\n",
    "        \n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)\n",
    "        \n",
    "        return torch.cat([prefix, ctx_expanded, suffix], dim=2)\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, device)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.prototype_matrix = None \n",
    "        self.alpha = 0.5 \n",
    "\n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.5):\n",
    "        self.prototype_matrix = prototype_matrix.type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, image, use_prototypes=False):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        b, c, n, d = prompts.shape\n",
    "        prompts_flat = prompts.reshape(b * c, n, d).type(self.dtype)\n",
    "        tokenized_expanded = self.tokenized_prompts.repeat(b, 1)\n",
    "\n",
    "        text_features = self.text_encoder(prompts_flat, tokenized_expanded)\n",
    "        text_features = text_features.reshape(b, c, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = logit_scale * (image_features.unsqueeze(1) @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        if use_prototypes and self.prototype_matrix is not None:\n",
    "            proto_logits = logit_scale * (image_features @ self.prototype_matrix.T)\n",
    "            return self.alpha * logits + (1 - self.alpha) * proto_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c59385",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, device='cuda', lr=0.002, num_epochs=10):\n",
    "        self.device = device\n",
    "        self.classnames = classnames\n",
    "        self.base_classes = base_classes\n",
    "        self.base_indices = torch.arange(len(self.base_classes), device=self.device)\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Teacher CLIP model (frozen)\n",
    "        self.teacher = clip_model.float().to(device).eval()\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Pre-compute teacher text features for base classes\n",
    "        with torch.no_grad():\n",
    "            tokens = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classnames]).to(self.device)\n",
    "            text_features = self.teacher.encode_text(tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        self.teacher_text_features = text_features\n",
    "\n",
    "        # Student model\n",
    "        self.model = CustomCLIP(clip_model, classnames, device=device).to(device)\n",
    "\n",
    "        # Label mapping: global dataset label -> local base-class index (for CE)\n",
    "        max_label_id = max(base_classes) + 1\n",
    "        self.label_map = torch.full((max_label_id,), -1, dtype=torch.long, device=device)\n",
    "        base_ids_tensor = torch.tensor(base_classes, device=device)\n",
    "        self.label_map[base_ids_tensor] = torch.arange(len(base_classes), device=device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(),\n",
    "            lr=lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=num_epochs)\n",
    "        \n",
    "\n",
    "    def compute_kd_loss(self, student_logits, teacher_logits, temperature=2.0):\n",
    "        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "    def train_epoch_with_kd(self, dataloader, kd_alpha=0.5, temperature=2.0, accumulation_steps=4):\n",
    "        self.model.train()\n",
    "        total_loss, n_batches = 0.0, 0\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training with KD (Grad Accum)\")):\n",
    "            images = images.to(self.device).float()\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Map labels to 0..N-1 for base classes CE\n",
    "            labels_mapped = self.label_map[labels]\n",
    "\n",
    "            # Student forward (logits over all classes)\n",
    "            student_logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Teacher forward (zero-shot logits over all classes)\n",
    "            with torch.no_grad():\n",
    "                img_features = self.teacher.encode_image(images)\n",
    "                img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "                teacher_logits = self.teacher.logit_scale.exp() * (img_features @ self.teacher_text_features.T)\n",
    "\n",
    "            # Compute CE only on base classes\n",
    "            student_base_logits = student_logits[:, self.base_indices]\n",
    "            loss_ce = F.cross_entropy(student_base_logits, labels_mapped)\n",
    "\n",
    "            # Compute KD over all classes (student vs teacher logits)\n",
    "            loss_kd = self.compute_kd_loss(student_logits, teacher_logits, temperature)\n",
    "\n",
    "            # Weighted hybrid loss\n",
    "            loss = (1 - kd_alpha) * loss_ce + kd_alpha * loss_kd\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            n_batches += 1\n",
    "\n",
    "        # Step for remaining gradients\n",
    "        if len(dataloader) % accumulation_steps != 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.scheduler.step()\n",
    "        return total_loss / max(1, n_batches)\n",
    "    \n",
    "    def train_epoch(self, dataloader, accumulation_steps=4):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training (Grad Accum)\")):\n",
    "            images = images.to(self.device).float()\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Fast mapping using pre-computed GPU tensor\n",
    "            labels_mapped = self.label_map[labels]\n",
    "            \n",
    "            # Forward pass (logits over all classes)\n",
    "            logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Compute CE only on base classes\n",
    "            base_logits = logits[:, self.base_indices]\n",
    "            loss = F.cross_entropy(base_logits, labels_mapped)\n",
    "            \n",
    "            # --- GRADIENT ACCUMULATION LOGIC ---\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Process remaining gradients if dataloader size is not divisible by accumulation_steps\n",
    "        if len(dataloader) % accumulation_steps != 0:\n",
    "             self.optimizer.step()\n",
    "             self.optimizer.zero_grad()\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        return total_loss / max(1, n_batches)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataset, categories, batch_size=64, use_prototypes=False):\n",
    "        self.model.eval()\n",
    "        local_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images = images.to(self.device).float()\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            targets = torch.tensor([local_cat2idx[l.item()] for l in labels], device=self.device)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda22b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
    "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 10\n",
    "\n",
    "**What happens:**\n",
    "- The `PromptLearner` adapts its 16 context vectors to the Flowers102 dataset.\n",
    "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
    "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~2.5 - 3.5\n",
    "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
    "- Training time: ~2-4 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make sure checkpoint folder exists\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,\n",
    "    classnames=base_class_names,   # Only base classes for training\n",
    "    base_classes=base_classes,     \n",
    "    device=device,\n",
    "    lr=0.002,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "patience = 5\n",
    "mode = \"standard\"  # \"standard\" = CE only, \"kd\" = knowledge distillation\n",
    "accumulation_steps = 4\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING CoCoOp (Patience: {patience})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    # Base-class training dataloader\n",
    "    train_loader = DataLoader(base_train_set, batch_size=4, shuffle=True)\n",
    "\n",
    "    # --- TRAINING STEP ---\n",
    "    if mode == \"standard\":\n",
    "        avg_loss = trainer.train_epoch(train_loader, accumulation_steps=accumulation_steps)\n",
    "    else:\n",
    "        avg_loss = trainer.train_epoch_with_kd(\n",
    "            train_loader,\n",
    "            kd_alpha=0.5,\n",
    "            temperature=2.0,\n",
    "            accumulation_steps=accumulation_steps\n",
    "        )\n",
    "\n",
    "    # --- EVALUATION STEP ---\n",
    "    # 1. Base-class validation (few-shot) with prototypes\n",
    "    val_acc_base = trainer.eval(base_val_set, base_classes, batch_size=64, use_prototypes=True)\n",
    "    \n",
    "    # 2. Optional: Novel-class validation (zero-shot)\n",
    "    val_acc_novel = trainer.eval(novel_test_set, novel_classes, batch_size=64, use_prototypes=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{trainer.num_epochs} - \"\n",
    "          f\"Loss: {avg_loss:.4f} | Base Acc: {val_acc_base*100:.2f}% | \"\n",
    "          f\"Novel Acc: {val_acc_novel*100:.2f}%\", end=\"\")\n",
    "\n",
    "    # --- EARLY STOPPING & CHECKPOINT ---\n",
    "    if val_acc_base > best_val_acc:\n",
    "        best_val_acc = val_acc_base\n",
    "        best_epoch = epoch + 1\n",
    "        counter = 0\n",
    "        torch.save(trainer.model.prompt_learner.state_dict(), \"checkpoints/best_model.pth\")\n",
    "        print(\"  [BEST MODEL SAVED]\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"  [No Improvement {counter}/{patience}]\")\n",
    "        if counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training complete. Best Base Acc: {best_val_acc*100:.2f}% at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b97fbf",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We'll test the model with:\n",
    "1. **Test Base** - CoCoOp only vs CoCoOp + Prototypes\n",
    "2. **Test Novel** - CoCoOp only (no prototypes for novel classes)\n",
    "\n",
    "Computing Harmonic Mean between them to evaluate the trade-off.\n",
    "\n",
    "**Note:** Prototypes are only available for base classes (built from training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD BEST MODEL FOR FINAL EVALUATION ---\n",
    "trainer.model.prompt_learner.load_state_dict(torch.load(\"checkpoints/best_model.pth\"))\n",
    "\n",
    "# --- INJECT PROTOTYPE MATRIX FOR HYBRID INFERENCE (BASE CLASSES ONLY) ---\n",
    "trainer.model.set_prototypes(prototype_matrix, alpha=0.5)\n",
    "print(\"Prototype Matrix injected for hybrid inference.\")\n",
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "base_acc = trainer.eval(base_test_set, base_classes, batch_size=64, use_prototypes=True)\n",
    "novel_acc = trainer.eval(novel_test_set, novel_classes, batch_size=64, use_prototypes=False)\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccb54",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2b1c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79ce58",
   "metadata": {},
   "source": [
    "## Refrences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
