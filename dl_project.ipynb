{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a6108",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project - a.y. 2024/2025\n",
    "\n",
    "This notebook represents our work for the 24/25 Deep Learning course project offered by the University of Trento. \n",
    "\n",
    "The task was few-shot adaptation of CLIP on the Flower102 dataset.\n",
    "\n",
    "**Authors**:\n",
    "- Andrea Giampietro - 258237\n",
    "- Marco Gandolfi - 258017\n",
    "- Stefano Camposilvan - 257848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf4e7",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [**Introduction**](#1-introduction) \n",
    "\n",
    "2. [**Setup**](#2-setup)\n",
    "\n",
    "3. [**The Baseline: CLIP**](#3-the-baseline-clip)\n",
    "\n",
    "4. [**Our Approach: ProtoCoCoOp**](#4-our-approach-protococoop)\n",
    "\n",
    "5. [**Results and Discussion**](#6-results-and-discussion)\n",
    "\n",
    "6. [**Conclusions**](#5-conclusions)\n",
    "\n",
    "7. [**References**](#7-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36996",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. The Context\n",
    "\n",
    "Vision–Language Models (VLMs) are a class of models that integrate natural language processing and computer vision to perform a wide range of tasks, including image captioning, visual question answering, and text-to-image generation. When trained on large-scale datasets, these models achieve remarkable performance across diverse domains. </br>\n",
    "Among them, Contrastive Language–Image Pre-training (CLIP) in particular has demonstrated strong zero-shot capabilities, enabling it to recognize and classify images without explicit task-specific training.\n",
    "\n",
    "However, in many real-world scenarios, large labeled datasets are unavailable: data may be scarce, expensive to obtain, or highly specialized. Furthermore, fine-grained classification tasks, where categories differ only by subtle details, pose additional challenges. Thus, zero-shot performance may not be sufficient, and task-specific adaptation becomes necessary.\n",
    "\n",
    "In this context, Few-Shot Adaptation tries to address this challenge by improving generalization when only a limited number of labeled examples per class are available. The goal of such method is to leverage prior knowledge learned during pretraining and adapt the model to new tasks using minimal supervision, mimicking the human ability to learn new concepts from only a handful of examples. Importantly, the few-shot setting requires the model not only to specialize on the Base classes using limited supervision, but also to preserve its original zero-shot generalization on the Novel ones.\n",
    "\n",
    "\n",
    "### 1.1. Our Proposal\n",
    "\n",
    "In this project, we tackle this Few-shot Adapatation problem in order to try improving over CLIP's performance. \n",
    "\n",
    "To do so, we use the Oxford Flowers102 dataset as a benchmark for fine-grained visual recognition. Such dataset contains 102 flower species, many of which exhibit subtle inter-class difference, making classification more challenging. To simulate a realistic few-shot scenario, we adopt a base–novel split in which only a small number of labeled samples (specifically, 10 shots per class) are available for the Base categories during adaptation, while the remaining classes are treated as Novel and remain unseen during training.\n",
    "\n",
    "We then build upon CoCoOp, a prompt-learning method designed for few-shot adaptation, and propose a strategy to better balance the base–novel trade-off. Specifically, we combine:\n",
    "\n",
    "1. **Knowledge Distillation (KD)**\n",
    "\n",
    "We treat the original frozen CLIP model as a teacher and regularize our adapted model (the student) to remain close to CLIP’s zero-shot predictions. This is achieved through a distillation loss that aligns the student’s logits with those of the teacher. The goal is to prevent overfitting to Base classes and preserve generalization on Novel classes.\n",
    "\n",
    "2. **Prototype-Based Residual Fusion (Inference-Time)**\n",
    "\n",
    "For each Base class, we compute a visual prototype by averaging normalized image embeddings extracted from the few-shot training samples (including augmented views). These prototypes act as compact representations of Base-class visual structure.\n",
    "\n",
    "At inference time, we compute the similarity between the input image embedding and each Base-class prototype, and use this similarity to add a residual logit contribution exclusively to Base classes. This mechanism strengthens discrimination among Base categories while leaving Novel predictions unaffected.\n",
    "\n",
    "Our final approach therefore explicitly separates:\n",
    "\n",
    "- Training-time regularization, via Knowledge Distillation to preserve zero-shot behavior.\n",
    "\n",
    "- Inference-time enhancement, via prototype-based residual fusion to strengthen Base discrimination.\n",
    "\n",
    "By combining these two mechanisms, we aim to improve the harmonic mean between Base and Novel accuracy, achieving a more balanced base-to-novel generalization on the Flowers102 benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fae21d",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "This section serves as the foundation of our project and ensures reproducibility and correct execution of all experiments. </br>\n",
    "The corresponding code cells include environment preparation, dataset handling, and evaluation protocol definition.\n",
    "\n",
    "More specifically, this section covers:\n",
    "\n",
    "- Installation of required packages and dependencies;\n",
    "\n",
    "- Definition of directories for data storage, model checkpoints, logs, and plots;\n",
    "\n",
    "- Initialization of constants and global parameters used throughout training and evaluation;\n",
    "\n",
    "- Dataset loading, splitting, and preprocessing;\n",
    "\n",
    "- Definition of evaluation metrics.\n",
    "\n",
    "Additional information are contained in each specific cell's comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548d47d",
   "metadata": {},
   "source": [
    "### 2.1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c3fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP already installed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from shutil import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Install CLIP if not already installed\n",
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d101d",
   "metadata": {},
   "source": [
    "### 2.2. Paths and Constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4faaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PATHS DEFINITION --\n",
    "# Directory for dataset\n",
    "data_path = \"data\"\n",
    "os.makedirs(data_path, exist_ok=True) \n",
    "\n",
    "# Directories for saving results\n",
    "models_path = \"results/models\"\n",
    "os.makedirs(models_path, exist_ok=True) \n",
    "logs_path = \"results/logs\"\n",
    "os.makedirs(logs_path, exist_ok=True) \n",
    "plots_path = \"results/plots\"\n",
    "os.makedirs(plots_path, exist_ok=True) \n",
    "\n",
    "# -- CONSTANTS DEFINITION --\n",
    "# Class names for Flowers102 dataset\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6005fd7",
   "metadata": {},
   "source": [
    "### 2.3. Reproducibility Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35208368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- REPRODUCIBILITY SETUP --\n",
    "# Function to set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Worker initialization function for DataLoader\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initialize random seed for each worker in DataLoader\n",
    "    Args:\n",
    "        worker_id (int): The ID of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b6d4",
   "metadata": {},
   "source": [
    "### 2.4. Data preparation\n",
    "\n",
    "Oxford Flowers102 dataset downloading and preparation.\n",
    "\n",
    "The preparation consist in dividing the 102 classes of the dataset into two blocks: base and novel categories. </br>\n",
    "The base classes are the first 51, while the novel classes are the remaining 51. </br>\n",
    "Only 10 labeled training samples per Base class are available. </br>\n",
    "\n",
    "Additionally to this, train, evaluation and test splits are also created.\n",
    "\n",
    "This setup simulates the few-shot adaptation protocol commonly adopted in the literature:\n",
    "\n",
    "- The model is adapted using only the given samples from the known classes (**Base**), while **Novel categories** remain unseen during training.\n",
    "- Evaluation is performed separately on Base and Novel categories to measure base-to-novel generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA PREPARATION FUNCTIONS --\n",
    "# Load specific split of Flowers102 dataset, with given transformation\n",
    "def load_split(split, transform):\n",
    "    \"\"\"Load Flowers102 dataset split with given transformation.\n",
    "    Args:\n",
    "        split (str): One of \"train\", \"val\", or \"test\".\n",
    "        transform (callable): Transformation to apply to the images.\n",
    "    Returns:\n",
    "        torchvision.datasets.Flowers102: The requested dataset split.\n",
    "    \"\"\"\n",
    "    return torchvision.datasets.Flowers102(root=data_path, split=split, download=True, transform=transform)\n",
    "\n",
    "# Load Flowers102 dataset and return train, val, test sets\n",
    "def get_data(transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        transform (callable, optional): Transformation to apply to the images. Defaults to None.\n",
    "    Returns:\n",
    "        tuple: (train_set, val_set, test_set) as torchvision.datasets.Flowers102 instances.\n",
    "    \"\"\"\n",
    "    train = load_split(\"train\", transform)\n",
    "    val = load_split(\"val\", transform)\n",
    "    test = load_split(\"test\", transform)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Split dataset classes into base and novel classes\n",
    "def split_classes(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present in the dataset.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split classes from.\n",
    "    Returns:\n",
    "        tuple: (base_classes, novel_classes) as lists of class ids.\n",
    "    \"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "\n",
    "    # Split classes into base and novel (first half and second half)\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "# Split dataset into base and novel datasets\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Split dataset into base and novel datasets based on provided base classes.\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.Flowers102): The dataset to split.\n",
    "        base_classes (list): List of class ids considered as base classes.\n",
    "    Returns:\n",
    "        tuple: (base_dataset, novel_dataset) as torch.utils.data.Subset instances.\n",
    "    \"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset\n",
    "\n",
    "\n",
    "# -- DATA PREPARATION --\n",
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Load dataset and split into base and novel datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# Get base and novel classes from the test set\n",
    "base_classes, novel_classes = split_classes(test_set)\n",
    "classes = base_classes + novel_classes\n",
    "\n",
    "# Get class names\n",
    "base_class_names = [CLASS_NAMES[i] for i in base_classes]\n",
    "print(f\"Base classes ({len(base_classes)}): {base_class_names}\")\n",
    "novel_class_names = [CLASS_NAMES[i] for i in novel_classes]\n",
    "print(f\"Novel classes ({len(novel_classes)}): {novel_class_names}\")\n",
    "print(f\"All classes: ({len(classes)}: { [CLASS_NAMES[i] for i in classes] }\")\n",
    "\n",
    "# Create base and novel datasets\n",
    "base_train_set, _ = split_data(train_set, base_classes)\n",
    "base_val_set, _ = split_data(val_set, base_classes)\n",
    "base_test_set, novel_test_set = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38062e",
   "metadata": {},
   "source": [
    "### 2.4. Evaluation metric: Harmonic Mean (HM)\n",
    "\n",
    "To evaluate model's performance correctly, it is important to consider both the accuracy on the Base and the accuracy on the Novel categories. </br>\n",
    "In order to correctly consider this trade-off, the **Harmonic Mean (HM)** , a standard metric in few-shot adaptation literature, is used.\n",
    "\n",
    "The harmonic mean between Base accuracy and Novel accuracy is defined as:\n",
    "\n",
    "$$\n",
    "    HM = \\frac{2}{\\frac{1}{\\text{BaseAcc}} + \\frac{1}{\\text{NovelAcc}}}\n",
    "$$\n",
    "\n",
    "Where $BaseAcc$ is the accuracy on the Base classes, $NovelAcc$ the accuracy on the Novel ones.\n",
    "\n",
    "The motivation behind this choice is that harmonic mean strongly penalizes imbalanced performance, as it decreases significantly when one of the two accuracies is low. </br> By considering HM as the principal evaluation metric, it is possible to lean towards balanced performance rather than optimizing one split at the expense of the other, which is particularly important in base-to-novel generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc73d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonic Mean Calculation\n",
    "def harmonic_mean(a, b):\n",
    "    \"\"\"Compute the harmonic mean of two accuracies.\"\"\"\n",
    "    # Guard against division by zero when both a and b are zero\n",
    "    return 2 * a * b / (a + b) if (a + b) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78449",
   "metadata": {},
   "source": [
    "## 3. The Baseline: CLIP\n",
    "\n",
    "### 3.1 Contrastive Language–Image Pre-training\n",
    "\n",
    "Contrastive Language–Image Pre-training (CLIP) [Radford et al., 2021] is a Vision–Language Model trained on large-scale image–text pairs collected from the web. </br>\n",
    "Instead of learning to classify images into a fixed set of predefined categories, CLIP is trained using a contrastive objective that aligns images and their corresponding textual descriptions in a shared embedding space.\n",
    "\n",
    "The model consists of two main components:\n",
    "\n",
    "- An **image encoder**, which extracts visual features from an image.\n",
    "- A **text encoder**, which extracts semantic features from a textual description.\n",
    "\n",
    "Both encoders project their inputs into the same high-dimensional space. During training, CLIP learns to bring matching image–text pairs closer together in this space while pushing mismatched pairs apart.  \n",
    "\n",
    "As a result, the model does not memorize class labels. Instead, it learns a semantic alignment between visual concepts and language, enabling it to reason about images through textual descriptions.\n",
    "\n",
    "\n",
    "### 3.2 Zero-Shot Classification with CLIP\n",
    "\n",
    "One of CLIP’s most powerful properties is its ability to perform **zero-shot classification**, meaning it can classify images without being explicitly trained on the target dataset.\n",
    "\n",
    "Given a set of class names $C = \\{c_1, \\dots, c_K\\}$, each class is converted into a textual prompt, for example:\n",
    "\n",
    "> \"a photo of a {class_name}\"\n",
    "\n",
    "These prompts are encoded by the text encoder to obtain a representation for each category.  \n",
    "At inference time, an input image is encoded by the image encoder, and classification is performed by computing the similarity between the image embedding and each class embedding.\n",
    "\n",
    "The predicted label corresponds to the class whose textual representation is most similar to the image representation.\n",
    "\n",
    "Importantly, this procedure requires **no additional training** on the downstream dataset. As long as a category can be described in natural language, CLIP can attempt to recognize it.  \n",
    "\n",
    "This flexibility makes CLIP a powerful baseline for few-shot adaptation, as it already provides strong generalization to unseen classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, dataset, classes, batch_size, device, label=\"\"):\n",
    "    \"\"\"Evaluate CLIP model on given dataset and classes.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The CLIP model.\n",
    "        dataset (torch.utils.data.Dataset): The dataset to evaluate on.\n",
    "        classes (list): List of class ids to consider.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        device (str): Device to run the evaluation on.\n",
    "        label (str, optional): Label for progress bar. Defaults to none.\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the given dataset and classes.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap original class ids to contiguous ids starting from zero\n",
    "    class_ids = {cls: id for id, cls in enumerate(classes)}\n",
    "\n",
    "    # Apply and tokenize standard clip sentences\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in classes]).to(device)\n",
    "\n",
    "    # Encode text features and normalize\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # Compute accuracy of the model\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=f\"Evaluating on {label}\", leave=False):\n",
    "        target = torch.Tensor([class_ids[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Encode image features and normalize\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Predict class by finding the text feature with highest similarity\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions/len(dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nComputing CLIP zero-shot accuracy on base and novel classes...\")\n",
    "base_acc = test(model=model, dataset=base_test_set, classes=base_classes, batch_size=128, device=device, label=\"base classes\")\n",
    "novel_acc = test(model=model, dataset=novel_test_set, classes=novel_classes, batch_size=128, device=device, label=\"novel classes\")\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "print(\"\\nComputation done.\\n\")\n",
    "\n",
    "print(f\"Zero-shot accuracy on base classes: {base_acc*100:.2f}%\")\n",
    "print(f\"Zero-shot accuracy on novel classes: {novel_acc*100:.2f}%\")\n",
    "print(f\"Harmonic Mean: {hm*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39653",
   "metadata": {},
   "source": [
    "### CoCoOp: Conditional Context Optimization\n",
    "\n",
    "Our approach builds upon **CoCoOp** (Conditional Context Optimization) [Zhou et al., 2022], which addresses a critical limitation of its predecessor, CoOp.\n",
    "\n",
    "To avoid searching for the prompt that maximizes CLIP zero-shot performance (i.e.,\n",
    "finding a better prompt than a photo of $\\text{class\\_name}$), **CoOp** [Zhou et al., 2022] automatizes the process by\n",
    "learning a set of context vectors (i.e., prompts) using few annotated samples. Formally, let $P ∈ RM×d$ be the set of learnable tokens. Then the input to the text encoder for class class_name becomes $c = [P1, ..., PM, \\text{class\\_name}]$. By forwarding images and corresponding text prompts through the pre-trained VLM, CoOp tunes P by computing the cross-entropy loss:\n",
    "$$\n",
    "\\mathcal{L}_{CE}(x_i, y_i)\n",
    "= - \\log\n",
    "\\frac{\n",
    "\\exp\\left(\\langle f_\\theta^v(x_i), f_\\theta^t(y_i)\\rangle\\right)\n",
    "}{\n",
    "\\sum_{c \\in C}\n",
    "\\exp\\left(\\langle f_\\theta^v(x_i), f_\\theta^t(c)\\rangle\\right)\n",
    "}\n",
    "$$\n",
    "where both $y_i$ and $c$ are built by pretending the learnable context to the category name. Yet, CoOp cannot generalize to Novel classes of the same dataset, caused by overfitting of Base classes during adaptation.\n",
    "\n",
    "To address this problem, **CoCoOp** improves over its predecessor by combining the context vectors with an image-conditioned token, which shifts the focus away from a specific set of classes to a specific input instance, reducing overfitting. To generate the image-conditioned token, CoCoOp trains a lightweight MLP called **Meta-Net** alongside learnable prompts. Formally, let $h_φ$ be the Meta-Net, then each conditional token is obtained as $P_m(x_i) = P_m+h_φ(x_i)$,\n",
    "where $P_m$ is the m-th learnable context vector in the sequence and $P_m(x_i)$ is the m-th conditional token. Both the Meta-Net and the context vectors are trained end-to-end using the cross-entropy loss. Combining context vectors with an input-conditioned token, has the joint benefit of adapting the VLM to Base classes while keeping high accuracies for Novel ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76352af9",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "\n",
    "A key challenge in prompt learning is that while adapting prompts to base classes improves performance on those classes, it can **degrade generalization to novel classes**. The learned prompts may drift away from CLIP's original semantic alignment, losing the zero-shot transfer capabilities that make CLIP powerful.\n",
    "\n",
    "We employ **knowledge distillation (KD)** to regularize the training process, using the frozen CLIP model as a *teacher* and the CoCoOp model as a *student*. The goal is to encourage the student's predictions to remain close to the teacher's zero-shot predictions, preserving CLIP's general knowledge while still adapting to the task.\n",
    "\n",
    "During training, for each input image $x$:\n",
    "\n",
    "1. **Student logits:** The CoCoOp model produces logits $z^{(s)}$ using learned instance-conditional prompts.\n",
    "\n",
    "2. **Teacher logits:** The frozen CLIP model computes logits $z^{(t)}$ using standard hand-crafted prompts (`\"a photo of a [CLASS]\"`).\n",
    "\n",
    "3. **Soft probability matching:** We convert logits to soft probability distributions using temperature scaling $T$:\n",
    "   $$p^{(s)}_i = \\frac{\\exp(z^{(s)}_i / T)}{\\sum_j \\exp(z^{(s)}_j / T)}, \\quad p^{(t)}_i = \\frac{\\exp(z^{(t)}_i / T)}{\\sum_j \\exp(z^{(t)}_j / T)}$$\n",
    "\n",
    "4. **KL divergence loss:** The distillation loss minimizes the KL divergence between student and teacher distributions:\n",
    "   $$\\mathcal{L}_{\\text{KD}} = T^2 \\cdot \\text{KL}(p^{(t)} \\| p^{(s)})$$\n",
    "\n",
    "   The $T^2$ factor compensates for the reduced gradient magnitude when using temperature scaling.\n",
    "\n",
    "5. **Combined loss:** The final training objective balances task-specific learning with knowledge preservation:\n",
    "   $$\\mathcal{L} = (1 - \\alpha) \\cdot \\mathcal{L}_{\\text{CE}} + \\alpha \\cdot \\mathcal{L}_{\\text{KD}}$$\n",
    "\n",
    "   where $\\alpha$ controls the trade-off between cross-entropy on base classes and distillation from CLIP.\n",
    "\n",
    "By regularizing toward CLIP's predictions over *all* classes (not just base classes), we expect the model to retain better generalization to unseen categories. The soft targets from the teacher should provide richer supervision than hard labels alone, acting as a regularizer that reduces overfitting. As a result, the trade-off between base and novel accuracy is expected to improve, leading to higher harmonic mean scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa753183",
   "metadata": {},
   "source": [
    "### Prototype Generation and Fusion\n",
    "\n",
    "While CoCoOp provides instance-conditional prompts, we further enhance base class performance by incorporating **class prototypes**—representative embeddings that capture the visual characteristics of each class.\n",
    "\n",
    "#### Prototype Construction\n",
    "\n",
    "For each base class $c$, we construct a prototype $\\mathbf{p}_c$ by aggregating CLIP image embeddings from training samples:\n",
    "\n",
    "1. **Extract embeddings:** For each training image $x_i$ of class $c$, compute the CLIP image embedding $f(x_i)$ using the **frozen** CLIP encoder.\n",
    "\n",
    "2. **Data augmentation:** To build more robust prototypes, we extract embeddings from both original images and multiple augmented views (random crops, flips, rotations, color jitter). This increases the effective sample size and captures intra-class variation.\n",
    "\n",
    "3. **L2 normalization:** All embeddings are L2-normalized before aggregation:\n",
    "   $$\\hat{f}(x_i) = \\frac{f(x_i)}{\\|f(x_i)\\|}$$\n",
    "\n",
    "4. **Mean aggregation:** The prototype is the normalized mean of all class embeddings:\n",
    "   $$\\mathbf{p}_c = \\frac{1}{|\\mathcal{X}_c|} \\sum_{x_i \\in \\mathcal{X}_c} \\hat{f}(x_i), \\quad \\mathbf{p}_c \\leftarrow \\frac{\\mathbf{p}_c}{\\|\\mathbf{p}_c\\|}$$\n",
    "\n",
    "**Key design choice:** We use the frozen CLIP encoder (not the adapted CoCoOp model) to preserve CLIP's zero-shot semantic structure in the prototype space.\n",
    "\n",
    "#### Inference-Time Fusion\n",
    "\n",
    "At inference, we combine CoCoOp's prompt-based predictions with prototype-based similarity scores:\n",
    "\n",
    "1. **CoCoOp logits:** Compute logits $z_{\\text{CoCoOp}}$ using instance-conditional prompts (as described in the CoCoOp section).\n",
    "\n",
    "2. **Prototype logits:** Compute similarity between the test image embedding and each class prototype:\n",
    "   $$z_{\\text{proto}}(c) = \\tau \\cdot \\cos(f(x), \\mathbf{p}_c)$$\n",
    "   where $\\tau$ is CLIP's learned temperature parameter.\n",
    "\n",
    "3. **Additive fusion:** The final logits for base classes are:\n",
    "   $$z_{\\text{final}}(c) = z_{\\text{CoCoOp}}(c) + \\alpha \\cdot z_{\\text{proto}}(c)$$\n",
    "   where $\\alpha$ controls the contribution of prototype information.\n",
    "\n",
    "**Note:** Prototype fusion is applied **only to base classes** since prototypes are constructed from training data. Novel classes rely solely on CoCoOp's generalization.\n",
    "\n",
    "#### Expected Benefits\n",
    "\n",
    "- **Improved base class accuracy:** Prototypes provide an additional source of class-specific information, complementing the prompt-based predictions.\n",
    "- **Robustness to few-shot settings:** By aggregating multiple augmented views, prototypes capture richer class representations even with limited training samples.\n",
    "- **Preserved novel class performance:** Since prototypes don't affect novel class predictions, the generalization benefits of CoCoOp are retained.\n",
    "- **Better harmonic mean:** The combination of improved base accuracy and maintained novel accuracy leads to higher overall HM scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de819d",
   "metadata": {},
   "source": [
    "The code below implements prototype construction with the following components:\n",
    "\n",
    "- **`aug_view_transform`**: A data augmentation pipeline (resize, random crop, flip, rotation, color jitter) used to generate diverse views of each training image, improving prototype robustness.\n",
    "\n",
    "- **`TransformView`**: A lightweight dataset wrapper that applies a given transform to images from a subset, enabling us to create multiple augmented views of the same underlying data.\n",
    "\n",
    "- **`build_prototypes`**: Extracts CLIP image embeddings from all samples (original + augmented), groups them by class, computes the L2-normalized mean for each class, and returns both a dictionary of prototypes and a stacked matrix for efficient batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transform for prototype construction\n",
    "aug_view_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                     (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Class to apply transform to an element of the dataset\n",
    "class TransformView(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.subset[idx]\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "# Build prototypes from augumented dataset\n",
    "@torch.no_grad()\n",
    "def build_prototypes(model, dataset, base_classes, device='cuda'):\n",
    "    \"\"\"\n",
    "    Build class prototypes from image embeddings extracted using frozen CLIP.\n",
    "\n",
    "    Args:\n",
    "        model: Frozen CLIP model used to extract image features.\n",
    "        dataset: Dataset containing images and labels.\n",
    "        base_classes: List of base class ids to build prototypes for.\n",
    "        device: Device to run computations on (default: 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (prototypes_dict, prototype_matrix):\n",
    "        - prototypes_dict: Dictionary mapping class_id -> prototype tensor (feature_dim,)\n",
    "        - prototype_matrix: Stacked tensor of shape (num_base_classes, feature_dim) for efficient inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "\n",
    "    for cls_id in base_classes:\n",
    "        if len(embeddings_per_class[cls_id]) == 0:\n",
    "            print(f\"Warning: no samples for class {cls_id}\")\n",
    "            continue\n",
    "\n",
    "        class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "        prototype = class_embeddings.mean(dim=0).to(device)\n",
    "        prototype = prototype / prototype.norm()\n",
    "\n",
    "        prototypes[cls_id] = prototype\n",
    "\n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix  # matrix of shape (num_base_classes, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d84f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 510 pool = 5610\n",
      "Extracting embeddings from 5610 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 88/88 [00:46<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 51 prototypes | Matrix shape: torch.Size([51, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw train dataset (PIL images)\n",
    "train_raw = load_split(\"train\", transform=None)\n",
    "\n",
    "# Build base subset indices on the same object (= avoid mismatched _labels across dataset instances)\n",
    "base_set = set(base_classes)\n",
    "base_idx = [i for i, y in enumerate(train_raw._labels) if y in base_set]  # uses Flowers102._labels\n",
    "base_train_raw = torch.utils.data.Subset(train_raw, base_idx)\n",
    "\n",
    "# Define transforms for original and augmented views\n",
    "orig_view = TransformView(base_train_raw, preprocess)\n",
    "\n",
    "num_samples = 10  # number of augmented views per original image\n",
    "views = [orig_view] + [TransformView(base_train_raw, aug_view_transform) for _ in range(num_samples)]\n",
    "\n",
    "# Create the prototype pool by concatenating all views\n",
    "proto_pool = torch.utils.data.ConcatDataset(views)\n",
    "\n",
    "print(\"N =\", len(orig_view), \"pool =\", len(proto_pool))\n",
    "\n",
    "# Build prototypes using frozen CLIP\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    model=model,\n",
    "    dataset=proto_pool,\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079cd1e",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3a924",
   "metadata": {},
   "source": [
    "The code below defines three key modules that work together to implement our ProtoCoCoOp model:\n",
    "\n",
    "**`TextEncoder`**: A wrapper around CLIP's text transformer that processes *continuous prompt embeddings* rather than discrete tokens. Given a prompt tensor of shape `(Batch × Num_Classes, Sequence_Length, Dim)`, it adds positional embeddings, permutes to sequence-first format for the transformer, passes through CLIP's frozen transformer layers, applies layer normalization, extracts the embedding at the EOS token position, and projects it to the joint image-text embedding space via the text projection matrix. This module is necessary because CLIP's built-in `encode_text()` expects tokenized integers, not learnable embedding vectors.\n",
    "\n",
    "**`PromptLearner`**: Implements the CoCoOp prompt learning mechanism with the following components:\n",
    "- **Context Vectors**: $M$ learnable vectors of dimension $d$ (e.g., 16 × 512), initialized with Gaussian noise $\\mathcal{N}(0, 0.02)$, providing the base context for all prompts.\n",
    "- **Meta-Net**: A lightweight MLP with architecture `Linear(512→32) → ReLU → Linear(32→512)` that takes image features as input and produces an instance-specific bias vector. Unlike the paper's notation $\\pi$, we implement this as an additive shift to the context vectors.\n",
    "- **Fixed Token Embeddings**: Pre-computed embeddings for the SOS prefix token and the class name + EOS suffix, which remain frozen during training.\n",
    "\n",
    "During the forward pass, the `PromptLearner` computes the instance-conditional context as $\\mathbf{v}(x) = \\mathbf{v} + h_\\theta(f(x))$, then constructs the full prompt sequence by concatenating `[Prefix] + [Shifted Context] + [Suffix]` for each class. To avoid looping over images, the implementation broadcasts tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)` and performs all operations in parallel.\n",
    "\n",
    "**`ProtoCoCoOp`**: The main model that orchestrates the full forward pipeline. Given an input batch of images, it first encodes them using CLIP's frozen ViT image encoder and L2-normalizes the resulting features. These image features are passed to `PromptLearner`, which generates instance-conditional prompt embeddings for all classes simultaneously. The prompts are reshaped to `(Batch × Num_Classes, Sequence_Length, Dim)` and fed through `TextEncoder` to obtain text features, which are also L2-normalized. The model then computes cosine similarity logits between image and text features, scaled by CLIP's learned temperature parameter $\\tau$. At inference time, when prototype fusion is enabled, the model additionally computes similarity between the image features and pre-computed class prototypes, and additively fuses these prototype logits with the CoCoOp logits for base classes only (controlled by the $\\alpha$ hyperparameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder module adapts CLIP's text transformer for batched prompt embeddings.\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        # Reuse components from the loaded CLIP text encoder\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        \"\"\"\n",
    "        Encode batched prompt embeddings using CLIP's text transformer.\n",
    "        Args:\n",
    "            prompts: (batch_size, seq_len, dim) tensor of prompt embeddings\n",
    "            tokenized_prompts: (batch_size, seq_len) tensor of token ids\n",
    "        Returns:\n",
    "            (batch_size, proj_dim) tensor of encoded text features\n",
    "        \"\"\"\n",
    "        # prompts: (batch_tokens, seq_len, dim) positional embeddings already included below\n",
    "        # tokenized_prompts: token ids (used to pick the final token's embedding)\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)  # add positional embeddings\n",
    "        x = x.permute(1, 0, 2)  # transformer expects (seq_len, batch, dim)\n",
    "        x = self.transformer(x)  # run through CLIP transformer\n",
    "        x = x.permute(1, 0, 2)  # back to (batch, seq_len, dim)\n",
    "        x = self.ln_final(x).type(self.dtype)  # layer norm and cast\n",
    "        # select the embedding at the end-of-text token for each sequence, then project\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x  # (batch, proj_dim)\n",
    "\n",
    "\n",
    "# Prompt Learner generates per-class prompt embeddings, optionally conditioned on image features.\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the PromptLearner.\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of class names for the dataset.\n",
    "            n_ctx: Number of context tokens to learn.\n",
    "            ctx_init: Optional string to initialize context tokens.\n",
    "            device: Device to run the model on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]  # dimensionality of token embeddings\n",
    "        vis_dim = clip_model.visual.output_dim  # dimensionality of visual features\n",
    "        self.n_cls = len(classnames)\n",
    "        self.n_ctx = n_ctx\n",
    "        self.device = device\n",
    "\n",
    "        # Meta network: maps image features -> additive bias for context vectors.\n",
    "        hidden_dim = vis_dim // 16\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, hidden_dim)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(hidden_dim, ctx_dim))\n",
    "        ])).to(device)\n",
    "        \n",
    "        # Context initialization: either from provided text or random normal.\n",
    "        if ctx_init:  # if a string is provided, initialize context from its token embeddings\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).to(self.dtype)\n",
    "            # use tokens after the special start token (1:1+n_ctx)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # learnable context vectors initialized from N(0, 0.02)\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        # Make context vectors learnable parameters\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # Prepare tokenized prompts for all classes using the prefix and class names\n",
    "        ref_classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in ref_classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        \n",
    "        # Obtain static token embeddings for prefix and suffix parts (non-learnable buffers)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).to(self.dtype)\n",
    "            \n",
    "        # token_prefix: the special start token (e.g., [SOS]) for each class\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        # token_suffix: the remaining tokens after the learnable context tokens\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        # im_features: (batch, vis_dim)\n",
    "        batch_size = im_features.shape[0]\n",
    "        ctx = self.ctx.to(self.dtype).unsqueeze(0)  # (1, n_ctx, dim)\n",
    "        bias = self.meta_net(im_features).unsqueeze(1)  # (batch, 1, dim)\n",
    "        \n",
    "        # Add image-conditioned bias to the base context vectors\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, dim)\n",
    "        \n",
    "        # Expand prefix and suffix for batch and classes\n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (batch, n_cls, 1, dim)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (batch, n_cls, suffix_len, dim)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)  # (batch, n_cls, n_ctx, dim)\n",
    "        \n",
    "        # Concatenate tokens into full prompt embeddings per class per batch\n",
    "        return torch.cat([prefix, ctx_expanded, suffix], dim=2)  # (batch, n_cls, n_tokens, dim)\n",
    "\n",
    "\n",
    "# ProtoCoCoOp model: builds on CoCoOp-style prompt learning and supports prototype fusion at inference.\n",
    "class ProtoCoCoOp(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, base_ids, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the ProtoCoCoOp model.\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of class names for the dataset.\n",
    "            base_ids: List of indices of base classes for prototype fusion.\n",
    "            n_ctx: Number of context tokens to learn.\n",
    "            ctx_init: Optional string to initialize context tokens.\n",
    "            device: Device to run the model on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # CLIP logit scale and model references\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.clip_model = clip_model\n",
    "        self.dtype = self.clip_model.dtype\n",
    "        self.base_ids = torch.tensor(base_ids, device=device)  # indices of base classes for prototype fusion\n",
    "        self.device = device\n",
    "\n",
    "        # Encoders and prompt learner\n",
    "        self.image_encoder = self.clip_model.visual\n",
    "        self.text_encoder = TextEncoder(self.clip_model)\n",
    "        self.prompt_learner = PromptLearner(self.clip_model, classnames, n_ctx, ctx_init, device)\n",
    "\n",
    "        # Tokenized prompts for selecting projected text outputs\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "\n",
    "        # Prototype matrix (num_prototypes x dim) and fusion weight alpha set at inference\n",
    "        self.prototype_matrix = None\n",
    "        self.alpha = None            \n",
    "\n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.2):\n",
    "        # Store prototypes and fusion coefficient\n",
    "        self.prototype_matrix = prototype_matrix.to(self.device).type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, image, use_prototypes=False):\n",
    "        # image: (batch, 3, H, W)\n",
    "        image = image.to(self.device).type(self.dtype)\n",
    "        image_features = self.image_encoder(image)  # visual embedding\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "        # Generate per-class prompt embeddings conditioned on image features\n",
    "        prompts = self.prompt_learner(image_features)  # (batch, n_cls, n_tokens, dim)\n",
    "        B, C, T, D = prompts.shape\n",
    "        prompts = prompts.reshape(B * C, T, D).type(self.dtype)  # flatten for text encoder\n",
    "\n",
    "        # Repeat the stored tokenized prompt ids for each batch instance\n",
    "        tokenized = self.tokenized_prompts.to(prompts.device).repeat(B, 1)\n",
    "\n",
    "        # Encode text prompts and normalize\n",
    "        text_features = self.text_encoder(prompts, tokenized)  # (B*C, proj_dim)\n",
    "        text_features = text_features.reshape(B, C, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute CLIP-style logits: scaled cosine similarity\n",
    "        logits = self.logit_scale.exp() * (image_features.unsqueeze(1) @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # Optional prototype fusion during inference: add prototype logits to base-class logits\n",
    "        if use_prototypes and self.prototype_matrix is not None:\n",
    "            # proto_logits: (batch, num_prototypes) projected and scaled\n",
    "            proto_logits = self.logit_scale.exp() * (image_features @ self.prototype_matrix.T)\n",
    "\n",
    "            # Fuse prototypes into logits for base classes only\n",
    "            logits_base = logits[:, self.base_ids]\n",
    "            logits[:, self.base_ids] = logits_base + self.alpha * proto_logits\n",
    "\n",
    "        return logits  # (batch, n_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c59385",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf76bc",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "The code below defines `CoCoOpTrainer`, a high-level class that manages the complete training and evaluation workflow for our ProtoCoCoOp model.\n",
    "\n",
    "**Initialization**: The trainer accepts a pretrained CLIP model, class names, base class IDs, and configuration dictionaries for both the model architecture and training hyperparameters. It freezes all CLIP parameters to preserve the pretrained representations, initializes a `ProtoCoCoOp` model with the specified context length and optional initialization string, and configures an SGD optimizer that updates only the `PromptLearner` parameters (context vectors and Meta-Net weights). A cosine annealing learning rate scheduler is used to smoothly decay the learning rate over training. The trainer also precomputes CLIP's zero-shot text features for all classes, which serve as the teacher signal when knowledge distillation is enabled. Based on the selected mode (`standard`, `kd`, `proto`, or `proto_kd`), the trainer determines which components to activate during training and inference.\n",
    "\n",
    "**Training**: The `train()` method performs one epoch of optimization. For each batch, it computes the forward pass through `ProtoCoCoOp` to obtain student logits, then calculates the cross-entropy loss on base classes only. When knowledge distillation is enabled, it additionally computes teacher logits using frozen CLIP with hand-crafted prompts, applies temperature-scaled softmax to both distributions, and minimizes the KL divergence between them. The final loss is a weighted combination of cross-entropy and distillation losses, controlled by the $\\alpha$ hyperparameter. Gradients flow only through the `PromptLearner`, leaving CLIP's encoders unchanged.\n",
    "\n",
    "**Evaluation**: The `test()` method evaluates the model on any dataset (base or novel classes). It computes accuracy by comparing predicted class indices against ground truth labels. When prototype fusion is enabled and prototypes have been set, the model additively combines CoCoOp logits with prototype similarity scores for base classes, while novel classes rely solely on the learned prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7358d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, config, params, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        CoCoOp Trainer for training and evaluation.\n",
    "\n",
    "        Args:\n",
    "            clip_model: Pretrained CLIP model.\n",
    "            classnames: List of all class names.\n",
    "            base_classes: List of base class ids.\n",
    "            config: Configuration dictionary for CoCoOp. Contains 'mode', 'n_ctx', 'ctx_init'.\n",
    "            params: Training parameters dictionary. Contains 'lr', 'momentum', 'weight_decay',\n",
    "                    'kd_alpha', 'temperature', 'num_epochs', 'tr_batch_size', 'ts_batch_size'.\n",
    "            device: Device to run the model on (default: \"cuda\").\n",
    "        \"\"\"\n",
    "        self.mode = config[\"mode\"].lower()\n",
    "        if self.mode == \"standard\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"kd\":\n",
    "            self.use_proto = False\n",
    "            self.use_kd = True\n",
    "        elif self.mode == \"proto\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = False\n",
    "        elif self.mode == \"proto_kd\":\n",
    "            self.use_proto = True\n",
    "            self.use_kd = True\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}. Choose from 'standard', 'kd', 'proto', 'proto_kd'.\")\n",
    "        \n",
    "        print(f\"Initialized CoCoOpTrainer in '{self.mode}' mode | use_proto={self.use_proto} | use_kd={self.use_kd}\")\n",
    "\n",
    "        self.kd_alpha = params[\"kd_alpha\"]\n",
    "        self.temperature = params[\"temperature\"]\n",
    "        self.num_epochs = params[\"num_epochs\"]\n",
    "        self.tr_batch_size = params[\"tr_batch_size\"]\n",
    "        self.ts_batch_size = params[\"ts_batch_size\"]\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze CLIP model parameters (no fine-tuning of CLIP itself).\n",
    "        self.clip_model = clip_model.float().to(device).eval()\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Precompute normalized CLIP text features for all class prompts.\n",
    "        with torch.no_grad():\n",
    "            prompts = [f\"a photo of a {c}\" for c in classnames]\n",
    "            tokens = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "            text_features = self.clip_model.encode_text(tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.clip_text_features = text_features\n",
    "\n",
    "        # Initialize CoCoOp model (prompt learner + optional prototype components).\n",
    "        self.model = ProtoCoCoOp(\n",
    "            self.clip_model,\n",
    "            classnames,\n",
    "            base_ids=base_classes,\n",
    "            n_ctx=config[\"n_ctx\"],\n",
    "            ctx_init=config[\"ctx_init\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimize only the prompt learner parameters.\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(),\n",
    "            lr=params[\"lr\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # Cosine annealing LR scheduler over epochs.\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs)\n",
    "\n",
    "        # Base class ids tensor on device.\n",
    "        self.base_ids = torch.tensor(base_classes, device=device)\n",
    "\n",
    "        # Map global class indices -> compact base-class indices; -1 for non-base classes.\n",
    "        num_total_classes = len(classnames)\n",
    "        self.label_map = torch.full((num_total_classes,), -1, dtype=torch.long, device=device)\n",
    "        self.label_map[self.base_ids] = torch.arange(len(base_classes), device=device)\n",
    "\n",
    "    # Knowledge Distillation Loss computation (KL between teacher probs and student log-probs).\n",
    "    def compute_kd_loss(self, student_logits, teacher_logits):\n",
    "        \"\"\"\n",
    "        Compute the knowledge distillation loss between student and teacher logits.\n",
    "        Args:\n",
    "            student_logits: Logits from the student model.\n",
    "            teacher_logits: Logits from the teacher model.\n",
    "        Returns:\n",
    "            KL divergence loss value.\n",
    "        \"\"\"\n",
    "        T = self.temperature\n",
    "\n",
    "        student_log_probs = F.log_softmax(student_logits / T, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / T, dim=-1)\n",
    "\n",
    "        # Multiply by T^2 as in temperature-scaled KD formulation.\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\") * (T ** 2)\n",
    "    \n",
    "    # Training function\n",
    "    def train(self, dataset):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataset: Training dataset.\n",
    "        Returns:\n",
    "            Average training loss over the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # DataLoader for training.\n",
    "        train_loader = DataLoader(dataset, batch_size=self.tr_batch_size, shuffle=True, num_workers=1, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training [{self.mode}]\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (no prototype fusion during training here).\n",
    "            logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Compute CE loss restricted to base classes.\n",
    "            base_logits = logits[:, self.base_ids]\n",
    "            targets = self.label_map[labels]\n",
    "\n",
    "            loss_ce = F.cross_entropy(base_logits, targets)\n",
    "\n",
    "            # Optionally compute KD loss using frozen CLIP as teacher.\n",
    "            if self.use_kd:\n",
    "                with torch.no_grad():\n",
    "                    img_feat = self.model.clip_model.encode_image(images)\n",
    "                    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    teacher_logits = (self.model.clip_model.logit_scale.exp() * img_feat @ self.clip_text_features.T)\n",
    "\n",
    "                loss_kd = self.compute_kd_loss(logits, teacher_logits)\n",
    "\n",
    "                # Weighted combination of CE and KD losses.\n",
    "                loss = (1 - self.kd_alpha) * loss_ce + self.kd_alpha * loss_kd\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return total_loss/total_samples\n",
    "    \n",
    "    # Evaluation function\n",
    "    @torch.no_grad()\n",
    "    def test(self, dataset, class_ids, use_prototypes=False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the given dataset. \n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to evaluate on.\n",
    "            class_ids: List of class ids to consider during evaluation.\n",
    "            use_prototypes: Whether to apply prototype fusion at inference.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (accuracy, average loss) over the dataset.        \n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Build mapping from global class index -> compact evaluation index.\n",
    "        class_ids = torch.tensor(class_ids, device=self.device)\n",
    "        mapping = torch.full((len(self.clip_text_features),), -1, dtype=torch.long, device=self.device)\n",
    "        mapping[class_ids] = torch.arange(len(class_ids), device=self.device)\n",
    "\n",
    "        # DataLoader for testing.\n",
    "        test_loader = DataLoader(dataset, batch_size=self.ts_batch_size, shuffle=False, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        correct_predictions = 0\n",
    "        predictions = 0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Forward pass with optional prototype fusion.\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "                \n",
    "            # Restrict logits to requested class subset.\n",
    "            logits = logits[:, class_ids]\n",
    "\n",
    "            targets = mapping[labels]\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            correct_predictions += (preds == targets).sum().item()\n",
    "            predictions += images.size(0)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        return (correct_predictions/predictions, total_loss/predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda22b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The following cell implements the complete training pipeline for our ProtoCoCoOp model. It initializes the trainer with the specified configuration, sets up the training loop with early stopping based on validation accuracy, and saves the best model checkpoint during training. The training process runs for up to 15 epochs on base classes only, with early stopping triggered if no improvement is observed for 5 consecutive epochs.\n",
    "\n",
    "We will train the PromptLearner for **15 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 8 (Balanced capacity for prompt learning)\n",
    "- **Batch size:** 1 (Training batch size for memory efficiency)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 15\n",
    "- **Early stopping patience:** 5 epochs\n",
    "\n",
    "**What happens:**\n",
    "The training pipeline initializes a CoCoOpTrainer instance with the pre-trained CLIP model and configures it according to the selected mode (standard, proto, kd, or proto_kd). During each epoch, the PromptLearner adapts its 8 context vectors to the Flowers102 dataset while the MetaNetwork learns to inject image-specific bias efficiently. The system uses GPU-based label lookup tables to speed up target mapping and implements early stopping to prevent overfitting by monitoring validation accuracy improvements.\n",
    "\n",
    "**Expected output:**\n",
    "The training process begins with an initial loss typically ranging from 2.5 to 3.5, which progressively decreases to a final loss between 0.5 and 1.0 as the model learns better prompt representations. Training time is expected to be approximately 2-4 minutes on GPU, with the best model checkpoint automatically saved when validation accuracy improves. The early stopping mechanism ensures efficient training by halting the process when no further improvements are observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: choose training mode and prompt settings\n",
    "CURRENT_MODE = \"proto_kd\"  # options: \"standard\", \"proto\", \"kd\", \"proto_kd\"\n",
    "\n",
    "config = {\n",
    "    \"mode\": CURRENT_MODE,\n",
    "    \"n_ctx\": 8,      # number of learnable context tokens\n",
    "    \"ctx_init\": None # optional string to initialize context from text (None -> random init)\n",
    "}\n",
    "\n",
    "# Training hyperparameters and algorithm switches\n",
    "params = {\n",
    "    \"lr\": 0.002,            # SGD LR for prompt parameters\n",
    "    \"momentum\": 0.9,        # accelerate convergence and smooth updates\n",
    "    \"weight_decay\": 5e-4,   # small weight decay to regularize learned context vectors slightly\n",
    "    \"tr_batch_size\": 1,     # training batch size (small to fit prompt learner + meta-net)  \n",
    "    \"ts_batch_size\": 32,    # evaluation batch size\n",
    "    \"patience_init\": 5,     # top after N non-improving epochs\n",
    "    \"num_epochs\": 15,       # maximum epochs to allow sufficient prompt adaptation\n",
    "    \"proto_alpha\": 0.2,     # prototype fusion weight \n",
    "    \"kd_alpha\": 0.3,        # KD loss weight  \n",
    "    \"temperature\": 2.0      # KD temperature: >1 for stable distillation\n",
    "}\n",
    "\n",
    "# Initialize trainer with frozen CLIP and prompt learner\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,            # pretrained CLIP model object\n",
    "    classnames=CLASS_NAMES,      # list of class name strings\n",
    "    base_classes=base_classes,   # list of base-class integer ids\n",
    "    config=config,\n",
    "    params=params,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Container to record training progress and best model\n",
    "results = {\n",
    "    \"mode\": config[\"mode\"],\n",
    "    \"sampled_epochs\": [],   # epochs visited\n",
    "    \"val_accs\": [],         # validation accuracies per sampled epoch\n",
    "    \"best_val_acc\": 0.0,    # best validation accuracy seen so far\n",
    "    \"losses_train\": [],     # training losses per epoch\n",
    "    \"losses_val\": [],       # validation losses per epoch\n",
    "}\n",
    "\n",
    "# Initialize early stopping counter\n",
    "patience = params[\"patience_init\"]\n",
    "\n",
    "# Training loop with periodic evaluation and model checkpointing on improvement\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING LOOP (Patience: {params['patience_init']}) | Mode: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    results[\"sampled_epochs\"].append(epoch)\n",
    "\n",
    "    # Training step: updates only the prompt learner parameters\n",
    "    train_loss = trainer.train(base_train_set)\n",
    "    print(f\"\\nEpoch {epoch+1}/{trainer.num_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    results[\"losses_train\"].append(np.asarray(train_loss).mean())\n",
    "\n",
    "    # Evaluation step: measure performance on base validation set (no prototype fusion here)\n",
    "    val_acc, val_loss = trainer.test(base_val_set, base_classes, use_prototypes=False)\n",
    "    print(f\" Validation Acc: {val_acc*100:.2f}% | Val Loss: {np.asarray(val_loss).mean():.4f}\")\n",
    "\n",
    "    results[\"val_accs\"].append(val_acc)\n",
    "    results[\"losses_val\"].append(np.asarray(val_loss).mean())\n",
    "\n",
    "    # If validation improves, save checkpoint and reset patience\n",
    "    if val_acc > results[\"best_val_acc\"]:\n",
    "        results[\"best_val_acc\"] = val_acc\n",
    "        patience = params[\"patience_init\"]  # reset patience\n",
    "\n",
    "        save_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "        model_data = {\n",
    "            \"model_state_dict\": trainer.model.state_dict(),\n",
    "            \"optimizer_state_dict\": trainer.optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": config,\n",
    "            \"params\": params,\n",
    "            \"results\": results\n",
    "        }\n",
    "        torch.save(model_data, save_path)\n",
    "        print(f\"[BEST MODEL SAVED] Acc: {val_acc*100:.2f}%\")\n",
    "    else:\n",
    "        # No improvement: decrement patience and possibly stop early\n",
    "        patience -= 1\n",
    "        print(f\" [No Improvement | Patience left: {patience}]\")\n",
    "        if patience == 0:\n",
    "            print(f\"\\nEARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training complete. Best Val Acc: {results['best_val_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a720cf7",
   "metadata": {},
   "source": [
    "### Training results logging and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and log utilities for training experiments\n",
    "\n",
    "def plot_results(results, plots_path):\n",
    "    \"\"\"\n",
    "    Save training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing training stats:\n",
    "                        - \"sampled_epochs\": list of epoch indices\n",
    "                        - \"losses_train\": list of training losses per epoch\n",
    "                        - \"losses_val\": list of validation losses per epoch\n",
    "        plots_path (str): Directory where the plot image will be saved.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    # Plot training and validation loss with markers for readability\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_train\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"losses_val\"], label=\"Validation Loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss ({config['mode']})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to disk using experiment mode in filename\n",
    "    filename = f\"{config['mode']}_training_plot.png\"\n",
    "    filepath = os.path.join(plots_path, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_results(params, config, results, log_path):\n",
    "    \"\"\"\n",
    "    Append a CSV row summarizing experiment settings and best validation result.\n",
    "\n",
    "    Args:\n",
    "        params (dict): Training hyperparameters and settings.\n",
    "        config (dict): Prompt/trainer configuration.\n",
    "        results (dict): Collected training results including \"best_val_acc\".\n",
    "        log_path (str): Path to the CSV log file.\n",
    "    \"\"\"\n",
    "    # Fields recorded for each experiment run\n",
    "    log_fields = [\n",
    "        \"model_type\",\n",
    "        \"num_epochs\",\n",
    "        \"lr\",\n",
    "        \"tr_batch_size\",\n",
    "        \"ts_batch_size\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"kd_alpha\",\n",
    "        \"proto_alpha\",\n",
    "        \"temperature\",\n",
    "        \"n_ctx\",\n",
    "        \"base_accuracy\"\n",
    "    ]\n",
    "\n",
    "    # If CSV does not exist, create it and write header\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, mode=\"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Append a single row summarizing this run\n",
    "    with open(log_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "        writer.writerow({\n",
    "            \"model_type\": results.get(\"mode\", config.get(\"mode\")),\n",
    "            \"num_epochs\": len(results.get(\"sampled_epochs\", [])),\n",
    "            \"lr\": params.get(\"lr\"),\n",
    "            \"tr_batch_size\": params.get(\"tr_batch_size\"),\n",
    "            \"ts_batch_size\": params.get(\"ts_batch_size\"),\n",
    "            \"momentum\": params.get(\"momentum\"),\n",
    "            \"weight_decay\": params.get(\"weight_decay\"),\n",
    "            \"kd_alpha\": params.get(\"kd_alpha\"),\n",
    "            \"proto_alpha\": params.get(\"proto_alpha\"),\n",
    "            \"temperature\": params.get(\"temperature\"),\n",
    "            \"n_ctx\": config.get(\"n_ctx\"),\n",
    "            \"base_accuracy\": f\"{results.get('best_val_acc', 0.0)*100:.2f}\"\n",
    "        })\n",
    "\n",
    "\n",
    "# Generate and persist training plot and CSV log for this run\n",
    "plot_results(results, plots_path)\n",
    "\n",
    "log_filepath = os.path.join(logs_path, \"training_log.csv\")\n",
    "log_results(params, config, results, log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b97fbf",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "The following cell performs the final evaluation of our trained ProtoCoCoOp model. It loads the best model checkpoint saved during training, reinitializes the trainer with the optimal configuration, and conducts comprehensive testing on both base and novel classes to assess the model's generalization capabilities.\n",
    "\n",
    "We'll test the model with two distinct evaluation scenarios: testing on base classes (where we can optionally use prototype fusion if the mode supports it) and testing on novel classes (where prototypes are unavailable since these classes weren't seen during training). The evaluation computes the harmonic mean between base and novel accuracies to assess the trade-off between maintaining performance on seen classes while generalizing to unseen ones.\n",
    "\n",
    "It's important to note that prototypes are only available for base classes since they are built from the training data. Novel classes, by definition, have no training examples and therefore cannot benefit from prototype fusion during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best trained checkpoint (if available) and evaluate final model.\n",
    "# The checkpoint file contains the saved model state, optimizer state, epoch and config/params used.\n",
    "best_model_path = os.path.join(models_path, f\"best_model_{config['mode']}.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"\\nLoading best model from {best_model_path}...\")\n",
    "    # Load checkpoint dictionary (contains model_state_dict, optimizer_state_dict, config, params, results)\n",
    "    model_data = torch.load(best_model_path, weights_only=False)\n",
    "\n",
    "    # Restore training configuration and hyperparameters used for the saved model\n",
    "    config = model_data[\"config\"]\n",
    "    params = model_data[\"params\"] \n",
    "\n",
    "    # Re-create trainer with the exact config/params to ensure compatibility, then load weights\n",
    "    trainer = CoCoOpTrainer(\n",
    "        clip_model=model,\n",
    "        classnames=CLASS_NAMES,\n",
    "        base_classes=base_classes,\n",
    "        config=config,\n",
    "        params=params,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "\n",
    "    print(\"Best model loaded successfully.\")\n",
    "else:\n",
    "    # If no checkpoint is found, continue with the current in-memory trainer/model\n",
    "    print(\"Warning: Best model checkpoint not found! Using current model state.\")\n",
    "\n",
    "# If prototype fusion mode was used, attach the precomputed prototype matrix and fusion weight.\n",
    "# Prototypes were computed only for base classes and will be fused into base-class logits at inference.\n",
    "if trainer.use_proto:\n",
    "    print(\"Setting prototypes for inference...\")\n",
    "    trainer.model.set_prototypes(prototype_matrix, alpha=params[\"proto_alpha\"])\n",
    "\n",
    "# Evaluate on base and novel test splits. Prototype fusion applied only when enabled for the trainer.\n",
    "base_acc, _ = trainer.test(base_test_set, base_classes, use_prototypes=trainer.use_proto)\n",
    "novel_acc, _ = trainer.test(novel_test_set, novel_classes, use_prototypes=False)\n",
    "\n",
    "# Compute harmonic mean to assess trade-off between base and novel performance.\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "# Nicely formatted summary of final results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RESULTS for MODE: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccb54",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2b1c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79ce58",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Radford et al., 2021 — Learning Transferable Visual Models From Natural Language Supervision\n",
    "\n",
    "- Zhou et al., 2022 — Learning to Prompt for Vision-Language Models (CoOp)\n",
    "\n",
    "- Zhou et al., 2022 — Conditional Prompt Learning for Vision-Language Models (CoCoOp)\n",
    "\n",
    "- Zhang et al., 2022 — Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification\n",
    "\n",
    "- Hinton et al, 2015 - Distilling the Knowledge in a Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
