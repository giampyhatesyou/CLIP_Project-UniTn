{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88a6108",
   "metadata": {},
   "source": [
    "# ProtoCoCoOp: Few-shot adaptation of CLIP\n",
    "\n",
    "Deep Learning Course Project - a.y. 2024/2025\n",
    "\n",
    "Authors:\n",
    "- Andrea Giampietro - xxxxxx\n",
    "- Marco Gandolfi - 258017\n",
    "- Stefano Camposilvan - 257848"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf4e7",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- TESTING OF FINAL MODEL\n",
    "    - wrt to baseline CLIP and base CoCoOp\n",
    "\n",
    "- ABLATION STUDY/PERFORMANCE COMPARISON\n",
    "    - train and eval baseline (CLIP)\n",
    "    - train and eval CoCoOp alone \n",
    "    - train and eval CoCoOp + proto\n",
    "    - train and eval CoCoOp + KD \n",
    "    - train and eval full model (CoCoOp + proto + KD)\n",
    "\n",
    "    note: ADD IMAGES, GRAPHS, etc...\n",
    "\n",
    "- PROPERLY COMMENT CODE\n",
    "\n",
    "- PROPERLY EXPLAIN THEORY/RATIONALE/IDEAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b28b00",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- Introduction\n",
    "- Setup\n",
    "- The Baseline: CLIP\n",
    "- Our Approach\n",
    "    - Overview\n",
    "    - CoCoOp\n",
    "    - Knowledge distillation\n",
    "    - Prototypes Generation\n",
    "    - Implementation\n",
    "- Results and Discussion\n",
    "- Conclusions\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236abd2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719db597",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f36996",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c3fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP already installed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from shutil import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d101d",
   "metadata": {},
   "source": [
    "### Paths and constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4faaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths definition\n",
    "models_path = \"results/models\"\n",
    "os.makedirs(models_path, exist_ok=True) # for saving models\n",
    "logs_path = \"results/logs\"\n",
    "os.makedirs(logs_path, exist_ok=True) # for saving logs\n",
    "plots_path = \"results/plots\"\n",
    "os.makedirs(plots_path, exist_ok=True) # for saving plots\n",
    "data_path = \"data\"\n",
    "os.makedirs(data_path, exist_ok=True) # for dataset storage\n",
    "\n",
    "# Class names for Flowers102 dataset\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Seed constant for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Function to set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Worker init function\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b6d4",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec9e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA PREPARATION FUNCTIONS --\n",
    "# Load specific split of Flowers102 dataset, with given transformation\n",
    "def load_split(split, transform):\n",
    "    \"\"\"Load Flowers102 dataset split with given transformation.\"\"\"\n",
    "    return torchvision.datasets.Flowers102(root=data_path, split=split, download=True, transform=transform)\n",
    "\n",
    "# Load Flowers102 dataset and return train, val, test sets\n",
    "def get_data(transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
    "    train = load_split(\"train\", transform)\n",
    "    val = load_split(\"val\", transform)\n",
    "    test = load_split(\"test\", transform)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Split dataset classes into base and novel classes\n",
    "def split_classes(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present in the dataset.\"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "# Split dataset into base and novel datasets\n",
    "def split_data(dataset, base_classes):\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d7d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Load dataset and split into base and novel datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# Get base and novel classes from the test set\n",
    "base_classes, novel_classes = split_classes(test_set)\n",
    "classes = base_classes + novel_classes\n",
    "\n",
    "# Get class names\n",
    "base_class_names = [CLASS_NAMES[i] for i in base_classes]\n",
    "novel_class_names = [CLASS_NAMES[i] for i in novel_classes]\n",
    "class_names = [CLASS_NAMES[i] for i in classes]\n",
    "\n",
    "# Create base and novel datasets\n",
    "base_train_set, _ = split_data(train_set, base_classes)\n",
    "base_val_set, novel_val_set = split_data(val_set, base_classes)\n",
    "base_test_set, novel_test_set = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38062e",
   "metadata": {},
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**goal:** maximize HM between `base_acc_cocoop` and `novel_acc_cocoop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc73d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    # Guard against zero to avoid division-by-zero errors\n",
    "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
    "        return 0.0\n",
    "    numerator = 2.0\n",
    "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78449",
   "metadata": {},
   "source": [
    "## The Baseline: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508b0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Zero-shot accuracy on both base and novel classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:02<00:05,  2.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing Zero-shot accuracy on both base and novel classes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m zero_shot_base_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_test_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m zero_shot_novel_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel, dataset\u001b[38;5;241m=\u001b[39mnovel_test_set, classes\u001b[38;5;241m=\u001b[39mnovel_classes, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputation done.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, dataset, classes, batch_size, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Cosine similarity between image and text features and keep the argmax for every image\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m (image_features \u001b[38;5;241m@\u001b[39m text_features\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     tp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, dataset, classes, batch_size, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Map original class ids to contiguous ids starting from zero\n",
    "    class_map = {cat: idx for idx, cat in enumerate(classes)}\n",
    "\n",
    "    # Apply and tokenize standard clip sentences\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in classes]).to(device)\n",
    "\n",
    "    # Encode text features and normalize\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Compute accuracy of the model on the dataset\n",
    "    tp = 0\n",
    "    for image, target in tqdm(dataloader):\n",
    "        target = torch.Tensor([class_map[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity between image and text features and keep the argmax for every image\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "\n",
    "        tp += (predicted_class == target).sum().item()\n",
    "\n",
    "    accuracy = tp/len(dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"Computing Zero-shot accuracy on both base and novel classes...\")\n",
    "zero_shot_base_accuracy = eval(model=model, dataset=base_test_set, classes=base_classes, batch_size=128, device=device)\n",
    "zero_shot_novel_accuracy = eval(model=model, dataset=novel_test_set, classes=novel_classes, batch_size=128, device=device)\n",
    "print(\"Computation done.\\n\")\n",
    "\n",
    "print(f\"Zero-shot accuracy on base classes: {zero_shot_base_accuracy*100:.2f}%\")\n",
    "print(f\"Zero-shot accuracy on novel classes: {zero_shot_novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39653",
   "metadata": {},
   "source": [
    "## Our Approach: Proto-guided CoCoOp with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e895bd",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32dbb4",
   "metadata": {},
   "source": [
    "### CoCoOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46a6a4",
   "metadata": {},
   "source": [
    "### Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a2aef",
   "metadata": {},
   "source": [
    "### Prototypes Generation\n",
    "\n",
    "We construct **class prototypes** from CLIP image embeddings of training samples.\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Use **frozen CLIP** (not the adapted model) to preserve zero-shot knowledge\n",
    "- Compute prototypes from **both normal and augmented samples** for better coverage\n",
    "- **L2-normalize** embeddings before averaging and after\n",
    "\n",
    "**At Inference:**\n",
    "- Compute prototype similarity: $\\text{sim}_{\\text{proto}}(x, c) = \\frac{f(x) \\cdot p_c}{\\|f(x)\\| \\|p_c\\|}$\n",
    "- Fuse with CoCoOp logits: $\\text{logits}_{\\text{final}} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{proto}}$\n",
    "- The fusion weight $\\alpha$ controls the trade-off between prompt-based and prototype-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transform for prototype construction\n",
    "aug_view_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                     (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Class to apply transform to an element of the dataset\n",
    "class TransformView(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.subset[idx]          # img is PIL.Image.Image\n",
    "        img = self.transform(img)          # must become a torch.Tensor\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "# Build prototypes from augumented dataset\n",
    "@torch.no_grad()\n",
    "def build_prototypes(model, dataset, base_classes, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "\n",
    "    for cls_id in base_classes:\n",
    "        if len(embeddings_per_class[cls_id]) == 0:\n",
    "            print(f\"Warning: no samples for class {cls_id}\")\n",
    "            continue\n",
    "\n",
    "        class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "        prototype = class_embeddings.mean(dim=0).to(device)\n",
    "        prototype = prototype / prototype.norm()\n",
    "\n",
    "        prototypes[cls_id] = prototype\n",
    "\n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix  # matrix of shape (num_base_classes, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d84f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 510 pool = 5610\n",
      "Extracting embeddings from 5610 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 88/88 [00:37<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built 51 prototypes | Matrix shape: torch.Size([51, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw train dataset (PIL images)\n",
    "train_raw = load_split(\"train\", transform=None)\n",
    "\n",
    "# Build base subset indices on the same object (= avoid mismatched _labels across dataset instances)\n",
    "base_set = set(base_classes)\n",
    "base_idx = [i for i, y in enumerate(train_raw._labels) if y in base_set]  # uses Flowers102._labels\n",
    "base_train_raw = torch.utils.data.Subset(train_raw, base_idx)\n",
    "\n",
    "# Define transforms for original and augmented views\n",
    "orig_view = TransformView(base_train_raw, preprocess)\n",
    "\n",
    "num_samples = 10  # number of augmented views per original image\n",
    "views = [orig_view] + [TransformView(base_train_raw, aug_view_transform) for _ in range(num_samples)]\n",
    "\n",
    "# Create the prototype pool by concatenating all views\n",
    "proto_pool = torch.utils.data.ConcatDataset(views)\n",
    "\n",
    "print(\"N =\", len(orig_view), \"pool =\", len(proto_pool))\n",
    "\n",
    "# Build prototypes using frozen CLIP\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    model=model,\n",
    "    dataset=proto_pool,\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079cd1e",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c593a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder module definition\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        # Extract [EOS] features\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "# Prompt Learner module definition\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0] # Dimension of context vectors\n",
    "        vis_dim = clip_model.visual.output_dim # Dimension of visual features\n",
    "        \n",
    "        # Context Initialization\n",
    "        if ctx_init: # If context initialization is provided (i.e. a string)\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(torch.float16)\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float16)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        self.meta_net = nn.Sequential(OrderedDict([  # Meta network to generate context bias\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        \n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [f\"{prompt_prefix} {name}.\" for name in classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(torch.float16)\n",
    "            \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
    "        self.n_cls, self.n_ctx = n_cls, n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        batch_size = im_features.shape[0]\n",
    "        bias = self.meta_net(im_features).unsqueeze(1)\n",
    "        ctx_shifted = self.ctx.unsqueeze(0) + bias\n",
    "        \n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)\n",
    "        \n",
    "        return torch.cat([prefix, ctx_expanded, suffix], dim=2)\n",
    "\n",
    "# ProtoCoCoOp model definition, extending CoCoOp with prototype comparison at inference time\n",
    "class protoCoCoOp(nn.Module):\n",
    "    def __init__(self, model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(model, classnames, n_ctx, ctx_init, device) # Prompt learner\n",
    "        self.text_encoder = TextEncoder(model) # Text encoder\n",
    "        self.image_encoder = model.visual # Image encoderì\n",
    "        \n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.logit_scale = model.logit_scale\n",
    "        self.dtype = model.dtype\n",
    "        self.prototype_matrix = None \n",
    "        self.alpha = 0.5\n",
    "\n",
    "        self.base_local_indices = None  # To be set externally\n",
    "\n",
    "    # Set prototypes for prototype comparison\n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.5):\n",
    "        self.prototype_matrix = prototype_matrix.type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Forward pass with optional prototype comparison (activated at inference time)\n",
    "    def forward(self, image, use_prototypes=False):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        b, c, n, d = prompts.shape\n",
    "        prompts_flat = prompts.reshape(b * c, n, d).type(self.dtype)\n",
    "        tokenized_expanded = self.tokenized_prompts.repeat(b, 1)\n",
    "\n",
    "        text_features = self.text_encoder(prompts_flat, tokenized_expanded)\n",
    "        text_features = text_features.reshape(b, c, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = logit_scale * (image_features.unsqueeze(1) @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        if use_prototypes and self.prototype_matrix is not None: # Prototype fusion at inference time\n",
    "            proto_logits = logit_scale * (image_features @ self.prototype_matrix.T) # Prototype logits\n",
    "\n",
    "            # Get base class indices\n",
    "            base_idx = self.base_local_indices \n",
    "\n",
    "            # Extract base logits from CoCoOp\n",
    "            base_logits = logits[:, base_idx]\n",
    "\n",
    "            # Fuse only base classes\n",
    "            fused_base = self.alpha * proto_logits + (1 - self.alpha) * base_logits\n",
    "\n",
    "            # Replace base class logits with fused logits\n",
    "            logits[:, base_idx] = fused_base\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c59385",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoCoOp Trainer class definition, to handle training and evaluation of the protoCoCoOp model\n",
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, config, params, device='cuda'):\n",
    "        self.device = device\n",
    "        self.classnames = classnames\n",
    "        self.base_classes = base_classes\n",
    "        self.base_indices = torch.tensor(base_classes, device=self.device)\n",
    "\n",
    "        self.kd_alpha = params[\"kd_alpha\"]\n",
    "        self.temperature = params[\"temperature\"]\n",
    "        self.accumulation_steps = params[\"accumulation_steps\"]\n",
    "        self.num_epochs = params[\"num_epochs\"]\n",
    "\n",
    "        self.mode = config[\"mode\"]\n",
    "\n",
    "        # Teacher CLIP model (frozen)\n",
    "        self.teacher = clip_model.float().to(device).eval()\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Pre-compute teacher text features for classes\n",
    "        with torch.no_grad():\n",
    "            tokens = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classnames]).to(self.device)\n",
    "            text_features = self.teacher.encode_text(tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        self.teacher_text_features = text_features\n",
    "\n",
    "        # Student model\n",
    "        self.model = protoCoCoOp(self.teacher, classnames, config[\"n_ctx\"], config[\"ctx_init\"], device=device).to(device)\n",
    "        self.model.base_local_indices = self.base_indices  # Set base class local indices\n",
    "\n",
    "        # Label mapping: global dataset label -> local base-class index (for CE)\n",
    "        max_label_id = max(base_classes) + 1 # max label id in base classes dataset\n",
    "        self.label_map = torch.full((max_label_id,), -1, dtype=torch.long, device=device)\n",
    "        base_ids_tensor = torch.tensor(base_classes, device=device)\n",
    "        self.label_map[base_ids_tensor] = torch.arange(len(base_classes), device=device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(),\n",
    "            lr=params[\"lr\"], momentum=params[\"momentum\"], weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs)\n",
    "        \n",
    "    # Knowledge Distillation loss computation\n",
    "    def compute_kd_loss(self, student_logits, teacher_logits, temperature=2.0):\n",
    "        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss, n_batches = 0.0, 0\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        desc = f\"Training mode: {self.mode}\"\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=desc)):\n",
    "            images = images.to(self.device).type(self.model.dtype)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Map labels to 0..N-1 for base classes CE\n",
    "            labels_mapped = self.label_map[labels]\n",
    "\n",
    "            # Student forward (logits over all classes)\n",
    "            logits = self.model(images, use_prototypes=False)\n",
    "\n",
    "            # Cross-Entropy loss on base classes\n",
    "            base_logits = logits[:, self.base_indices]\n",
    "            loss_ce = F.cross_entropy(base_logits, labels_mapped)\n",
    "\n",
    "            # Knowledge Distillation loss\n",
    "            if self.mode == \"kd\":\n",
    "                with torch.no_grad():\n",
    "                    img_features = self.teacher.encode_image(images)\n",
    "                    img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "                    teacher_logits = self.teacher.logit_scale.exp() * (img_features @ self.teacher_text_features.T)\n",
    "\n",
    "                # Compute KD over all classes (student vs teacher logits)\n",
    "                loss_kd = self.compute_kd_loss(logits, teacher_logits, self.temperature)\n",
    "\n",
    "                # Weighted hybrid loss\n",
    "                loss = (1 - self.kd_alpha) * loss_ce + self.kd_alpha * loss_kd\n",
    "            else:\n",
    "                loss = loss_ce\n",
    "\n",
    "            # Gradient accumulation\n",
    "            loss = loss / self.accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.accumulation_steps\n",
    "            n_batches += 1\n",
    "            \n",
    "        # Process remaining gradients if dataloader size is not divisible by accumulation_steps\n",
    "        if len(dataloader) % self.accumulation_steps != 0:\n",
    "             self.optimizer.step()\n",
    "             self.optimizer.zero_grad()\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        return total_loss / max(1, n_batches)\n",
    "    \n",
    "    # Evaluate model on given dataset and classes\n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataloader, classes, use_prototypes=False):\n",
    "        self.model.eval()\n",
    "        local_cat2idx = {cat: idx for idx, cat in enumerate(classes)}\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images = images.to(self.device).float()\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            targets = torch.tensor([local_cat2idx[l.item()] for l in labels], device=self.device)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda22b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
    "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 10\n",
    "\n",
    "**What happens:**\n",
    "- The `PromptLearner` adapts its 16 context vectors to the Flowers102 dataset.\n",
    "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
    "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~2.5 - 3.5\n",
    "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
    "- Training time: ~2-4 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING LOOP (Patience: 5) | Mode: STANDARD\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampled_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# --- TRAINING STEP ---  \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39masarray(train_loss)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 57\u001b[0m, in \u001b[0;36mCoCoOpTrainer.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     56\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode}\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     58\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     59\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/tqdm/std.py:1098\u001b[0m, in \u001b[0;36mtqdm.__init__\u001b[0;34m(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, write_bytes, lock_args, nrows, colour, delay, gui, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_printer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp)\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1098\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# Init the time counter\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/tqdm/std.py:1151\u001b[0m, in \u001b[0;36mtqdm.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_meter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl25/lib/python3.10/site-packages/tqdm/std.py:585\u001b[0m, in \u001b[0;36mtqdm.format_meter\u001b[0;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# old prefix setup work around\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     bool_prefix_colon_already \u001b[38;5;241m=\u001b[39m (prefix[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 585\u001b[0m     l_bar \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;28;01mif\u001b[39;00m bool_prefix_colon_already \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     l_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "# Define model configuration\n",
    "config = {\n",
    "    \"mode\": \"std_cocoop\", # \"std_cocoop\", \"proto\", \"kd\", \"proto+kd\"\n",
    "    \"n_ctx\": 16,\n",
    "    \"ctx_init\": None\n",
    "}\n",
    "\n",
    "# Define training hyperparameters\n",
    "params = {\n",
    "    \"lr\": 0.002,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"batch_size\": 4,\n",
    "    \"patience_init\": 5,\n",
    "    \"num_epochs\": 15,\n",
    "    \"kd_alpha\": 0.5,\n",
    "    \"temperature\": 2.0,\n",
    "    \"accumulation_steps\": 4\n",
    "}\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,\n",
    "    classnames=class_names, # all class names\n",
    "    base_classes=base_classes, # base classes ids\n",
    "    config=config,\n",
    "    params=params,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Base-class training dataloader\n",
    "train_loader = DataLoader(base_train_set, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(base_val_set, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Results dictionary to store training data\n",
    "results = {\n",
    "        \"mode\": config[\"mode\"],\n",
    "        \"sampled_epochs\": [],\n",
    "        \"val_accs\": [],\n",
    "        \"best_val_acc\": 0.0,\n",
    "        \"losses_train\": [],\n",
    "        \"losses_val\": [],\n",
    "    }\n",
    "\n",
    "# Patience-based early stopping variable\n",
    "patience = params[\"patience_init\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING LOOP (Patience: {params['patience_init']}) | Mode: {config['mode'].upper()}\")\n",
    "print(\"=\"*70)\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    results[\"sampled_epochs\"].append(epoch)\n",
    "\n",
    "    # --- TRAINING STEP ---  \n",
    "    train_loss = trainer.train(train_loader)\n",
    "    results[\"losses_train\"].append(np.asarray(train_loss).mean())\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{trainer.num_epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # --- EVALUATION STEP ---\n",
    "    # Base-class validation (few-shot)\n",
    "    print(\"\\nNO PROTOTYPES EVAL:\")\n",
    "    val_acc_base, val_loss = trainer.eval(val_loader, base_classes, use_prototypes=False)\n",
    "    print(\"\\nPROTOTYPES EVAL:\")\n",
    "    val_acc_base, val_loss = trainer.eval(val_loader, base_classes, use_prototypes=True)\n",
    "    results[\"val_accs\"].append(val_acc_base)\n",
    "    results[\"losses_val\"].append(np.asarray(val_loss).mean())\n",
    "\n",
    "    print(f\"Validation Base Acc: {val_acc_base*100:.2f}% - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # --- EARLY STOPPING & CHECKPOINT ---\n",
    "    if val_acc_base > results[\"best_val_acc\"]:\n",
    "        results[\"best_val_acc\"] = val_acc_base\n",
    "        patience = 0\n",
    "        torch.save(trainer.model.prompt_learner.state_dict(), os.path.join(models_path, f\"best_model_{config['mode']}.pth\"))\n",
    "        print(\"[BEST MODEL SAVED]\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        print(f\"  [No Improvement | Patience left: {patience}]\")\n",
    "        if patience == 0:\n",
    "            print(f\"\\nEARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training complete. Best Base Acc: {results['best_val_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a720cf7",
   "metadata": {},
   "source": [
    "### Training results logging and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots training results\n",
    "def plot_results(results, plots_path):\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"train_losses\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(results[\"sampled_epochs\"], results[\"val_losses\"], label=\"Validation Loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Dev Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "    filename = f\"{config['mode']}_training_plot.png\"\n",
    "    filepath = os.path.join(plots_path, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "# Logs training results\n",
    "def log_results(params, results, log_path):\n",
    "    # Log and save training results\n",
    "    log_fields = [\n",
    "        \"experiment_id\",\n",
    "        \"model_type\",\n",
    "        \"num_epochs\",\n",
    "        \"lr\",\n",
    "        \"batch_size\",\n",
    "        \"momentum\",\n",
    "        \"weight_decay\",\n",
    "        \"kd_alpha\",\n",
    "        \"temperature\",\n",
    "        \"base_accuracy\"\n",
    "    ]\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, mode=\"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "            writer.writeheader()\n",
    "    with open(log_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
    "        writer.writerow({\n",
    "            \"model_type\": results[\"mode\"],\n",
    "            \"num_epochs\": len(results[\"sampled_epochs\"]),\n",
    "            \"lr\": params[\"lr\"],\n",
    "            \"batch_size\": params[\"batch_size\"],\n",
    "            \"momentum\": params[\"momentum\"],\n",
    "            \"weight_decay\": params[\"weight_decay\"],\n",
    "            \"kd_alpha\": params.get(\"kd_alpha\"),\n",
    "            \"temperature\": params.get(\"temperature\"),\n",
    "            \"base_accuracy\": f\"{results['best_val_acc']*100:.2f}\"\n",
    "        })\n",
    "\n",
    "# Plot and log results\n",
    "plot_results(results, plots_path)\n",
    "\n",
    "log_filepath = os.path.join(logs_path, \"training_log.csv\")\n",
    "log_results(params, results, log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b97fbf",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We'll test the model with:\n",
    "1. **Test Base** - CoCoOp only vs CoCoOp + Prototypes\n",
    "2. **Test Novel** - CoCoOp only (no prototypes for novel classes)\n",
    "\n",
    "Computing Harmonic Mean between them to evaluate the trade-off.\n",
    "\n",
    "**Note:** Prototypes are only available for base classes (built from training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD BEST MODEL FOR FINAL EVALUATION ---\n",
    "if os.path.exists(os.path.join(models_path, f\"best_model_{config[\"mode\"]}.pth\")):\n",
    "    trainer.model.prompt_learner.load_state_dict(torch.load(os.path.join(models_path, f\"best_model_{config[\"mode\"]}.pth\")))\n",
    "\n",
    "# --- INJECT PROTOTYPE MATRIX FOR HYBRID INFERENCE (BASE CLASSES ONLY) ---\n",
    "trainer.model.set_prototypes(prototype_matrix, alpha=0.5)\n",
    "print(\"Prototype Matrix injected for hybrid inference.\")\n",
    "\n",
    "# Create test dataloaders\n",
    "base_test_loader = DataLoader(base_test_set, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "novel_test_loader = DataLoader(novel_test_set, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "base_acc = trainer.eval(base_test_loader, base_classes, use_prototypes=True)\n",
    "novel_acc = trainer.eval(novel_test_loader, novel_classes, use_prototypes=False)\n",
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS for MODE:\", config[\"mode\"].upper())\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccb54",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2b1c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79ce58",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
