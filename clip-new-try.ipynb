{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff453b1a",
   "metadata": {},
   "source": [
    "# CLIP with Flowers!?!?!??!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73175206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#!pip install openai-clip\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e45cb",
   "metadata": {},
   "source": [
    "## Dataset Functions\n",
    "\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adfe795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    all_classes = set(dataset._labels)\n",
    "    num_classes = len(all_classes)\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "    \n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "    \n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bda8d2",
   "metadata": {},
   "source": [
    "## Class Names and Dataset Loading\n",
    "\n",
    "We load the names of 102 flower classes from Flowers102.\n",
    "\n",
    "This is **critical** for CLIP:\n",
    "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
    "- Each prompt is encoded by CLIP's text encoder\n",
    "- Image features are compared against these text templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f443bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Uncomment to see class names\n",
    "# print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "# print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a6d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model: ViT-B/16\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: ViT-B/16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e05089",
   "metadata": {},
   "source": [
    "## Load Flowers102 and Split Base/Novel\n",
    "\n",
    "We load the 3 splits (train, val, test) and divide into base/novel.\n",
    "\n",
    "**Statistics:**\n",
    "- Train Base: 10 images √ó 51 classes = 510 images\n",
    "- Val Base: 10 images √ó 51 classes = 510 images\n",
    "- Test Base: ~10 images √ó 51 classes (from test split)\n",
    "- Test Novel: Remaining (~10 per class)\n",
    "\n",
    "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdd5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Base: 510 samples\n",
      "Val Base: 510 samples\n",
      "Test Base: 2473 samples\n",
      "Test Novel: 3676 samples\n"
     ]
    }
   ],
   "source": [
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "print(f\"Train Base: {len(train_base)} samples\")\n",
    "print(f\"Val Base: {len(val_base)} samples\")\n",
    "print(f\"Test Base: {len(test_base)} samples\")\n",
    "print(f\"Test Novel: {len(test_novel)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa3eba",
   "metadata": {},
   "source": [
    "## Zero-Shot CLIP Evaluation\n",
    "\n",
    "We evaluate the original CLIP **without any training**:\n",
    "\n",
    "1. For each class, create a fixed prompt: \"a photo of a {class_name}, a type of flower\"\n",
    "\n",
    "2. Encode the prompt with text encoder ‚Üí text_features (512-dim)\n",
    "\n",
    "3. Encode the image with vision encoder ‚Üí image_features (512-dim)\n",
    "\n",
    "4. Cosine similarity between image and text features ‚Üí logits\n",
    "\n",
    "5. Prediction = class with highest similarity\n",
    "\n",
    "**Expected results:**\n",
    "- Base Accuracy: ~71% (CLIP is not specialized on this dataset)\n",
    "- Novel Accuracy: ~78% (CLIP generalizes better on new classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aa1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîµ ZERO-SHOT BASELINE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [33:30<00:00, 51.56s/it]\n",
      "Zero-shot evaluation on Novel Classes:  17%|‚ñà‚ñã        | 10/58 [08:31<40:49, 51.04s/it]"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    \"\"\"Zero-shot evaluation using fixed CLIP templates\"\"\"\n",
    "    model.eval()\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "    \n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "    \n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Compute zero-shot baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîµ ZERO-SHOT BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=64, device=device, label=\"Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=64, device=device, label=\"Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print(f\"\\nüîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071f907",
   "metadata": {},
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**Goal:** HM > 75% (improvement over zero-shot ~74.6%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Harmonic Mean: 74.60%\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "hm_zeroshot = harmonic_mean(base_accuracy, novel_accuracy)\n",
    "print(f\"üîç Harmonic Mean: {hm_zeroshot*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4a384",
   "metadata": {},
   "source": [
    "## MetaNetwork: Conditional Token Generator\n",
    "\n",
    "**Problem:** Fixed prompts don't adapt to each image.\n",
    "\n",
    "**Solution:** A small neural network that transforms image features into a conditional token.\n",
    "\n",
    "**Parameters:** ~256K (negligible vs. fine-tuning)\n",
    "\n",
    "**Effect:** Each image gets a different prompt ‚Üí instance-level adaptation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MetaNetwork √® una piccola rete neurale (MLP con 2 layer)\n",
    "che trasforma le image_features (512-dim) in un token\n",
    "condizionale (512-dim) usato in CoCoOp.\n",
    "\n",
    "Questo token varia per ogni immagine, permettendo prompt\n",
    "personalizzati per ogni input.\n",
    "\"\"\"\n",
    "\n",
    "class MetaNetwork(nn.Module):\n",
    "    def __init__(self, ctx_dim=512, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ctx_dim: dimensione degli embeddings (512 per ViT-B/16)\n",
    "            hidden_dim: dimensione dello strato nascosto\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(ctx_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(hidden_dim, ctx_dim)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_features: tensor (B, ctx_dim) dalle immagini encodate\n",
    "        \n",
    "        Returns:\n",
    "            conditional_token: tensor (B, ctx_dim)\n",
    "        \"\"\"\n",
    "        # Assicura il tipo corretto (importante per mixed precision)\n",
    "        image_features = image_features.to(self.linear1.weight.dtype)\n",
    "        \n",
    "        out = self.linear1(image_features)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916487b1",
   "metadata": {},
   "source": [
    "## CoCoOpPromptLearner: Dynamic Prompts\n",
    "\n",
    "\n",
    "**Components:**\n",
    "1. **V1...VM:** 16 context vectors (learned via SGD)\n",
    "   - Shape: (16, 512) tensors\n",
    "   - Initialized randomly from N(0, 0.02¬≤)\n",
    "   - Optimized during training\n",
    "\n",
    "2. **œÄ(x):** Conditional token (generated per image)\n",
    "   - Shape: (B, 512) from MetaNetwork output\n",
    "   - Different for each image\n",
    "\n",
    "3. **[CLASS]:** Class name embedding\n",
    "   - Shape: (seq_len, 512) from CLIP's token embedding\n",
    "   - Same for all images of the same class\n",
    "\n",
    "**Forward Pass:**\n",
    "- Input: image_features (B, 512)\n",
    "- Output: prompts (B, num_classes, seq_len_total, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4):  # ‚Üê 4 context tokens\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.classnames = classnames\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = int(clip_model.ln_final.weight.shape[0])\n",
    "        self.clip_context_length = clip_model.context_length\n",
    "        print(f\"[CoCoOp] ctx_dim={ctx_dim}, max_len={self.clip_context_length}\")\n",
    "        \n",
    "        # 4 context vectors\n",
    "        ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "        nn.init.normal_(ctx_vectors, std=0.02)\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        self.meta_net = MetaNetwork(ctx_dim)\n",
    "        \n",
    "        device = next(clip_model.parameters()).device\n",
    "        classnames_tokens = clip.tokenize(classnames).to(device)\n",
    "        with torch.no_grad():\n",
    "            self.register_buffer(\"class_token_embeddings\", \n",
    "                               clip_model.token_embedding(classnames_tokens))\n",
    "            # Save token ids for each class sequence so we can find the EOT position later\n",
    "            self.register_buffer(\"class_token_ids\", classnames_tokens)\n",
    "        \n",
    "    def forward(self, image_features):\n",
    "        batch_size = image_features.shape[0]\n",
    "        num_classes, class_len, ctx_dim = self.class_token_embeddings.shape\n",
    "        \n",
    "        # Conditional token per immagine\n",
    "        cond = self.meta_net(image_features).unsqueeze(1).unsqueeze(2)  # B,1,1,D\n",
    "        cond = cond.repeat(1, num_classes, 1, 1)                         # B,N,1,D\n",
    "        \n",
    "        # Context\n",
    "        ctx = self.ctx.unsqueeze(0).unsqueeze(0).repeat(batch_size, num_classes, 1, 1)  # B,N,4,D\n",
    "        \n",
    "        # Class\n",
    "        cls_emb = self.class_token_embeddings.unsqueeze(0).repeat(batch_size, 1, 1, 1)  # B,N,L,D\n",
    "        \n",
    "        # Concat: [ctx] + [œÄ(x)] + [class] \n",
    "        prompts = torch.cat([ctx, cond, cls_emb], dim=2)[:,:,:self.clip_context_length,:]\n",
    "        return prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b162a",
   "metadata": {},
   "source": [
    "## CoCoOpTrainer: Training and Evaluation\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval():**\n",
    "- Same forward procedure as training\n",
    "- Without backward pass\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ba719",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi √® verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CoCoOpTrainer fornisce:\n",
    "1. train_epoch(): esegue un epoca di training su base classes\n",
    "2. eval(): valuta su base o novel classes\n",
    "\n",
    "Importante: CLIP rimane congelato, alleniamo solo i prompt learner!\n",
    "\"\"\"\n",
    "\n",
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, base_classnames, base_classes, \n",
    "                 novel_classes, device, lr=0.002):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip_model: modello CLIP caricato\n",
    "            base_classnames: lista di nomi classi base\n",
    "            base_classes: lista di indici base classes\n",
    "            novel_classes: lista di indici novel classes\n",
    "            device: \"cuda\" o \"cpu\"\n",
    "            lr: learning rate\n",
    "        \"\"\"\n",
    "        self.clip_model = clip_model\n",
    "        self.base_classnames = base_classnames\n",
    "        self.base_classes = base_classes\n",
    "        self.novel_classes = novel_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Precompute mapping from original class id -> contiguous index for base classes\n",
    "        # This will be reused during training to remap dataset labels to [0, num_base_classes)\n",
    "        self.contig_cat2idx = {cat: idx for idx, cat in enumerate(self.base_classes)}\n",
    "        \n",
    "        # Freeze CLIP parameters\n",
    "        for p in clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # Crea il prompt learner\n",
    "        self.prompt_learner = CoCoOpPromptLearner(\n",
    "            clip_model, \n",
    "            base_classnames\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimizer - allena solo il prompt learner\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.prompt_learner.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4\n",
    "        )\n",
    "        \n",
    "    def train_epoch(self, train_dataset, batch_size=32):\n",
    "        \"\"\"\n",
    "        Esegue una epoca di training.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.train()\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"CoCoOp training\")):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # ===== FORWARD =====\n",
    "            \n",
    "            # Encode immagini (frozen)\n",
    "            with torch.no_grad():\n",
    "                img_feat = self.clip_model.encode_image(images)\n",
    "            \n",
    "            # Normalizza image features\n",
    "            img_feat = img_feat.to(self.prompt_learner.meta_net.linear1.weight.dtype)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Genera prompts condizionali per ogni immagine\n",
    "            prompts = self.prompt_learner(img_feat)  # (B, num_classes, seq_len, ctx_dim)\n",
    "            B, N, L, D = prompts.shape\n",
    "            \n",
    "            # Reshape per poter passare al text encoder\n",
    "            prompts_flat = prompts.view(B * N, L, D)\n",
    "            \n",
    "            # ===== TEXT ENCODING (Passaggio critico) =====\n",
    "            # Passiamo gli embeddings direttamente attraverso il text encoder transformer\n",
    "            x = prompts_flat  # (B*N, seq_len, 512)\n",
    "            \n",
    "            # Aggiungi positional embeddings\n",
    "            x = x + self.clip_model.positional_embedding\n",
    "            \n",
    "            # Permuta per il transformer\n",
    "            x = x.permute(1, 0, 2)  # (seq_len, B*N, 512)\n",
    "            \n",
    "            # Passa attraverso il transformer\n",
    "            x = self.clip_model.transformer(x)\n",
    "            \n",
    "            # Ritorna al formato batch\n",
    "            x = x.permute(1, 0, 2)  # (B*N, seq_len, 512)\n",
    "            \n",
    "            # Prendi il token EOT per ogni sequenza (come in CLIP.encode_text)\n",
    "            # Costruiamo gli indici delle classi ripetuti per ogni batch in modo che\n",
    "            # l'i-esimo elemento flat corrisponda alla classe (i % N). Usiamo i token ids\n",
    "            # salvati in self.prompt_learner.class_token_ids per trovare la posizione EOT.\n",
    "            class_indices = torch.arange(N, device=self.prompt_learner.class_token_ids.device).unsqueeze(0).repeat(B, 1).view(-1)\n",
    "            token_ids = self.prompt_learner.class_token_ids[class_indices]  # (B*N, L)\n",
    "            # CLIP's tokenizer places the EOT token with the highest id in the sequence,\n",
    "            # so argmax returns its position (same trick used in encode_text)\n",
    "            eot_positions = token_ids.argmax(dim=-1)  # (B*N,)\n",
    "            # Adjust for prepended tokens: n_ctx (context tokens) + 1 (conditional token)\n",
    "            offset = self.prompt_learner.n_ctx + 1\n",
    "            eot_positions = eot_positions + offset\n",
    "            # Gather the transformer output at the adjusted EOT positions\n",
    "            x = x[torch.arange(x.shape[0], device=x.device), eot_positions.to(x.device)]  # (B*N, 512)\n",
    "            \n",
    "            # Layer norm finale\n",
    "            x = self.clip_model.ln_final(x)\n",
    "            \n",
    "            # Projection\n",
    "            text_feat = self.clip_model.text_projection @ x.T  # (512, B*N)\n",
    "            text_feat = text_feat.T  # (B*N, 512)\n",
    "            \n",
    "            # Reshape back to (B, N, 512)\n",
    "            text_feat = text_feat.view(B, N, -1)\n",
    "            \n",
    "            # Normalizza\n",
    "            text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # ===== LOGITS =====\n",
    "            logit_scale = self.clip_model.logit_scale.exp()\n",
    "            logits = logit_scale * (img_feat.unsqueeze(1) * text_feat).sum(-1)\n",
    "            \n",
    "            # ===== LOSS =====\n",
    "            # Use precomputed mapping from original class id -> contiguous index\n",
    "            labels_mapped = torch.tensor(\n",
    "                [self.contig_cat2idx[l.item()] for l in labels]\n",
    "            ).to(self.device)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, labels_mapped)\n",
    "            \n",
    "            # ===== BACKWARD =====\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / max(1, n_batches)\n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataset, categories, batch_size=64):\n",
    "        \"\"\"\n",
    "        Valuta il modello su un dataset.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.eval()\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader, desc=\"CoCoOp eval\"):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Encode immagini\n",
    "            img_feat = self.clip_model.encode_image(images)\n",
    "            img_feat = img_feat.to(self.prompt_learner.meta_net.linear1.weight.dtype)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Genera prompts\n",
    "            prompts = self.prompt_learner(img_feat)\n",
    "            B, N, L, D = prompts.shape\n",
    "            prompts_flat = prompts.view(B * N, L, D)\n",
    "            \n",
    "            # Encode testi (stessa procedura del training)\n",
    "            x = prompts_flat\n",
    "            x = x + self.clip_model.positional_embedding\n",
    "            x = x.permute(1, 0, 2)\n",
    "            x = self.clip_model.transformer(x)\n",
    "            x = x.permute(1, 0, 2)\n",
    "            # Use EOT positions to extract final token (consistent with encode_text)\n",
    "            class_indices = torch.arange(N, device=self.prompt_learner.class_token_ids.device).unsqueeze(0).repeat(B, 1).view(-1)\n",
    "            token_ids = self.prompt_learner.class_token_ids[class_indices]  # (B*N, L)\n",
    "            eot_positions = token_ids.argmax(dim=-1)\n",
    "            # Adjust for prepended tokens: n_ctx (context tokens) + 1 (conditional token)\n",
    "            offset = self.prompt_learner.n_ctx + 1\n",
    "            eot_positions = eot_positions + offset\n",
    "            x = x[torch.arange(x.shape[0], device=x.device), eot_positions.to(x.device)]\n",
    "            x = self.clip_model.ln_final(x)\n",
    "            text_feat = (self.clip_model.text_projection @ x.T).T\n",
    "            text_feat = text_feat.view(B, N, -1)\n",
    "            text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Logits\n",
    "            logit_scale = self.clip_model.logit_scale.exp()\n",
    "            logits = logit_scale * (img_feat.unsqueeze(1) * text_feat).sum(-1)\n",
    "            \n",
    "            # Predizioni\n",
    "            pred = logits.argmax(dim=1)\n",
    "            labels_mapped = torch.tensor(\n",
    "                [contig_cat2idx[l.item()] for l in labels]\n",
    "            ).to(self.device)\n",
    "            \n",
    "            correct += (pred == labels_mapped).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838deabc",
   "metadata": {},
   "source": [
    "## Training CoCoOp\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate: 0.002 (SGD)\n",
    "- Momentum: 0.9\n",
    "- Weight decay: 5e-4\n",
    "- Batch size: 32\n",
    "- Epochs: 10\n",
    "\n",
    "**What happens:**\n",
    "- Context vectors V1...VM adapt to the Flowers102 dataset\n",
    "- MetaNetwork learns to generate useful conditional tokens\n",
    "- CLIP remains frozen (unchanged)\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~3.0\n",
    "- Final loss: ~1.3-1.5\n",
    "- Training time: ~5-10 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2feda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classnames (51): ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold']...\n",
      "\n",
      "[CoCoOp] ctx_dim=512, max_len=77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CoCoOp training:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparing base class names for CoCoOp\n",
    "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
    "print(f\"Base classnames ({len(base_classnames)}): {base_classnames[:5]}...\\n\")\n",
    "\n",
    "# Creating trainer for CoCoOp\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,\n",
    "    base_classnames=base_classnames,\n",
    "    base_classes=base_classes,\n",
    "    novel_classes=novel_classes,\n",
    "    device=device,\n",
    "    lr=0.002\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = trainer.train_epoch(train_base, batch_size=32)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c077f",
   "metadata": {},
   "source": [
    "## Final Evaluation and Comparison\n",
    "\n",
    "We evaluate the trained model on:\n",
    "1. **Test Base:** Classes seen during training (51 classes)\n",
    "2. **Test Novel:** Never-seen classes (51 new classes)\n",
    "\n",
    "Then compare against the zero-shot baseline.\n",
    "\n",
    "**Interpretation:**\n",
    "- Base improves significantly (specialization on dataset)\n",
    "- Novel decreases slightly (learning bias toward base)\n",
    "- HM remains positive (good compromise)\n",
    "\n",
    "If HM < 75%, we will add Knowledge Distillation in the next iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49984fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION AND COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Valutazione su base e novel\n",
    "base_acc_cocoop = trainer.eval(test_base, base_classes, batch_size=64)\n",
    "novel_acc_cocoop = trainer.eval(test_novel, novel_classes, batch_size=64)\n",
    "hm_cocoop = harmonic_mean(base_acc_cocoop, novel_acc_cocoop)\n",
    "\n",
    "# Stampa risultati\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîµ Zero-Shot CLIP (Baseline)\")\n",
    "print(f\"   Base Accuracy:  {base_accuracy*100:6.2f}%\")\n",
    "print(f\"   Novel Accuracy: {novel_accuracy*100:6.2f}%\")\n",
    "print(f\"   Harmonic Mean:  {hm_zeroshot*100:6.2f}%\")\n",
    "\n",
    "print(\"\\nüü¢ CoCoOp (Prompt Learning + MetaNetwork)\")\n",
    "print(f\"   Base Accuracy:  {base_acc_cocoop*100:6.2f}%  (Œî {(base_acc_cocoop-base_accuracy)*100:+6.2f}%)\")\n",
    "print(f\"   Novel Accuracy: {novel_acc_cocoop*100:6.2f}%  (Œî {(novel_acc_cocoop-novel_accuracy)*100:+6.2f}%)\")\n",
    "print(f\"   Harmonic Mean:  {hm_cocoop*100:6.2f}%  (Œî {(hm_cocoop-hm_zeroshot)*100:+6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
