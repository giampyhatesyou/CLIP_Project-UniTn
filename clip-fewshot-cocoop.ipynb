{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff453b1a",
      "metadata": {
        "id": "ff453b1a"
      },
      "source": [
        "# CLIP with Flowers!?!?!??!?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73175206",
      "metadata": {
        "id": "73175206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hi6qq6w8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-hi6qq6w8\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cpu)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=311d8475af11f48b8839f67da3c0fc933775f22bde09b287b29ca4fc9a46ad7b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4f12zb8/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Prefer expandable segments to reduce fragmentation (restart kernel after changing)\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Ensure CLIP is installed in the current kernel; install if missing.\n",
        "# Using subprocess with sys.executable to target the same Python interpreter.\n",
        "try:\n",
        "    import clip\n",
        "except Exception:\n",
        "    import subprocess, importlib\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"git+https://github.com/openai/CLIP.git\"], stdout=subprocess.DEVNULL)\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "df3e678e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3e678e",
        "outputId": "6724513f-f760-416e-d31a-c94408490e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIP: gi√† installato\n",
            "python: /usr/bin/python3\n",
            "torch: 2.9.0+cpu cuda_available: False\n",
            "mps_available: False\n",
            "Riavvia il kernel se necessario, poi esegui questa cella e procedi con l'allenamento CoCoOp.\n"
          ]
        }
      ],
      "source": [
        "# Check environment and CLIP installation\n",
        "import sys, importlib, torch\n",
        "\n",
        "try:\n",
        "    import clip\n",
        "    print(\"CLIP: gi√† installato\")\n",
        "except Exception:\n",
        "    print(\"CLIP non trovato: eseguo installazione nel kernel corrente...\")\n",
        "    import importlib\n",
        "    # Preferisci %pip per installare nel kernel Jupyter corrente; fallback a subprocess\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"git+https://github.com/openai/CLIP.git\"])\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "    print(\"CLIP installato correttamente nel kernel corrente\")\n",
        "\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
        "# su mac con Apple Silicon, controlla MPS\n",
        "try:\n",
        "    print(\"mps_available:\", torch.backends.mps.is_available())\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('Riavvia il kernel se necessario, poi esegui questa cella e procedi con l\\'allenamento CoCoOp.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9e45cb",
      "metadata": {
        "id": "cf9e45cb"
      },
      "source": [
        "## Dataset Functions\n",
        "\n",
        "We define utility functions for:\n",
        "- **`get_data()`**: Load Flowers102 from torchvision\n",
        "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
        "- **`split_data()`**: Filter images for base/novel in each split\n",
        "\n",
        "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9adfe795",
      "metadata": {
        "id": "9adfe795"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    all_classes = set(dataset._labels)\n",
        "    num_classes = len(all_classes)\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bda8d2",
      "metadata": {
        "id": "74bda8d2"
      },
      "source": [
        "## Class Names and Dataset Loading\n",
        "\n",
        "We load the names of 102 flower classes from Flowers102.\n",
        "\n",
        "This is **critical** for CLIP:\n",
        "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
        "- Each prompt is encoded by CLIP's text encoder\n",
        "- Image features are compared against these text templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f443bb94",
      "metadata": {
        "id": "f443bb94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345M/345M [00:09<00:00, 36.8MB/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502/502 [00:00<00:00, 1.11MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.0k/15.0k [00:00<00:00, 22.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "# Uncomment to see class names\n",
        "# print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "# print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e8a6d3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a6d3c9",
        "outputId": "f6aea588-1031-461c-d6cb-2ac6fb769a25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:04<00:00, 81.8MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Model: ViT-B/16\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ViT-B/16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e05089",
      "metadata": {
        "id": "e0e05089"
      },
      "source": [
        "## Load Flowers102 and Split Base/Novel\n",
        "\n",
        "We load the 3 splits (train, val, test) and divide into base/novel.\n",
        "\n",
        "**Statistics:**\n",
        "- Train Base: 10 images √ó 51 classes = 510 images\n",
        "- Val Base: 10 images √ó 51 classes = 510 images\n",
        "- Test Base: ~10 images √ó 51 classes (from test split)\n",
        "- Test Novel: Remaining (~10 per class)\n",
        "\n",
        "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9fdd5516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdd5516",
        "outputId": "b01660a9-9525-454e-d05e-dbd722d50063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Base: 510 samples\n",
            "Val Base: 510 samples\n",
            "Test Base: 2473 samples\n",
            "Test Novel: 3676 samples\n"
          ]
        }
      ],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# split the three datasets\n",
        "train_base, _ = split_data(train_set, base_classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Train Base: {len(train_base)} samples\")\n",
        "print(f\"Val Base: {len(val_base)} samples\")\n",
        "print(f\"Test Base: {len(test_base)} samples\")\n",
        "print(f\"Test Novel: {len(test_novel)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0fa3eba",
      "metadata": {
        "id": "b0fa3eba"
      },
      "source": [
        "## (Sezione rimossa) Zero-Shot CLIP Evaluation\n",
        "\n",
        "Questa sezione √® stata rimossa: il notebook esegue ora esclusivamente il workflow CoCoOp (prompt learning + MetaNetwork).\n",
        "\n",
        "Se vuoi riattivare la valutazione zero-shot pi√π tardi, esegui una cella separata che costruisca i prompt fissi e chiami `model.encode_text()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d28aa1de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d28aa1de",
        "outputId": "b6b5b237-febd-45d4-91e4-dba7babe121f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot evaluation removed. Proceeding with CoCoOp-only workflow.\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot evaluation removed ‚Äî notebook runs CoCoOp only now.\n",
        "# Se serve, riattivare la valutazione zero-shot separatamente.\n",
        "print(\"Zero-shot evaluation removed. Proceeding with CoCoOp-only workflow.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8071f907",
      "metadata": {
        "id": "8071f907"
      },
      "source": [
        "## Harmonic Mean (HM)\n",
        "\n",
        "Standard metric for few-shot adaptation papers.\n",
        "\n",
        "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
        "\n",
        "**Why HM instead of arithmetic mean?**\n",
        "- HM heavily penalizes outliers\n",
        "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
        "- Forces the model to balance both accuracies\n",
        "\n",
        "**Obiettivo:** massimizzare l'HM tra `base_acc_cocoop` e `novel_acc_cocoop`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3eec2ec0",
      "metadata": {
        "id": "3eec2ec0"
      },
      "outputs": [],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    numerator = 2\n",
        "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d4a384",
      "metadata": {
        "id": "84d4a384"
      },
      "source": [
        "## MetaNetwork: Conditional Token Generator\n",
        "\n",
        "**Problem:** Fixed prompts don't adapt to each image.\n",
        "\n",
        "**Solution:** A small neural network that transforms image features into a conditional token.\n",
        "\n",
        "**Parameters:** ~256K (negligible vs. fine-tuning)\n",
        "\n",
        "**Effect:** Each image gets a different prompt ‚Üí instance-level adaptation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd4e54f3",
      "metadata": {
        "id": "cd4e54f3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MetaNetwork √® una piccola rete neurale (MLP con 2 layer)\n",
        "che trasforma le image_features (512-dim) in un token\n",
        "condizionale (512-dim) usato in CoCoOp.\n",
        "\n",
        "Questo token varia per ogni immagine, permettendo prompt\n",
        "personalizzati per ogni input.\n",
        "\"\"\"\n",
        "\n",
        "class MetaNetwork(nn.Module):\n",
        "    def __init__(self, ctx_dim=512, hidden_dim=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ctx_dim: dimensione degli embeddings (512 per ViT-B/16)\n",
        "            hidden_dim: dimensione dello strato nascosto\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(ctx_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear2 = nn.Linear(hidden_dim, ctx_dim)\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: tensor (B, ctx_dim) dalle immagini encodate\n",
        "\n",
        "        Returns:\n",
        "            conditional_token: tensor (B, ctx_dim)\n",
        "        \"\"\"\n",
        "        # Assicura il tipo corretto (importante per mixed precision)\n",
        "        image_features = image_features.to(self.linear1.weight.dtype)\n",
        "\n",
        "        out = self.linear1(image_features)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916487b1",
      "metadata": {
        "id": "916487b1"
      },
      "source": [
        "## CoCoOpPromptLearner: Dynamic Prompts\n",
        "\n",
        "\n",
        "**Components:**\n",
        "1. **V1...VM:** 16 context vectors (learned via SGD)\n",
        "   - Shape: (16, 512) tensors\n",
        "   - Initialized randomly from N(0, 0.02¬≤)\n",
        "   - Optimized during training\n",
        "\n",
        "2. **œÄ(x):** Conditional token (generated per image)\n",
        "   - Shape: (B, 512) from MetaNetwork output\n",
        "   - Different for each image\n",
        "\n",
        "3. **[CLASS]:** Class name embedding\n",
        "   - Shape: (seq_len, 512) from CLIP's token embedding\n",
        "   - Same for all images of the same class\n",
        "\n",
        "**Forward Pass:**\n",
        "- Input: image_features (B, 512)\n",
        "- Output: prompts (B, num_classes, seq_len_total, 512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4cb9b5b5",
      "metadata": {
        "id": "4cb9b5b5"
      },
      "outputs": [],
      "source": [
        "class CoCoOpPromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=4):\n",
        "        super().__init__()\n",
        "        self.n_ctx = n_ctx\n",
        "        self.classnames = classnames\n",
        "        dtype = torch.float32\n",
        "        ctx_dim = int(clip_model.ln_final.weight.shape[0])\n",
        "        self.clip_context_length = clip_model.context_length  # üîß AGGIUNGI QUESTA RIGA\n",
        "        print(f\"[CoCoOp] ctx_dim={ctx_dim}, max_len={self.clip_context_length}\")\n",
        "\n",
        "        device = next(clip_model.parameters()).device\n",
        "        # keep a reference to the clip model and dtype for later updates\n",
        "        self.clip_model = clip_model\n",
        "        self.dtype = dtype\n",
        "\n",
        "        ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype, device=device)\n",
        "        nn.init.normal_(ctx_vectors, std=0.02)\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        self.meta_net = MetaNetwork(ctx_dim).to(device=device, dtype=dtype)\n",
        "\n",
        "        classnames_tokens = clip.tokenize(classnames).to(device)\n",
        "        with torch.no_grad():\n",
        "            self.register_buffer(\"class_token_embeddings\",\n",
        "                              clip_model.token_embedding(classnames_tokens).to(dtype=dtype))\n",
        "        self.register_buffer(\"class_token_ids\", classnames_tokens)\n",
        "\n",
        "    def set_classnames(self, classnames):\n",
        "        \"\"\"Replace the internal class token embeddings with a new set.\n",
        "\n",
        "        This is used at evaluation time to temporarily switch prompts\n",
        "        to a different class set (e.g., novel classes).\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        tokens = clip.tokenize(classnames).to(device)\n",
        "        with torch.no_grad():\n",
        "            emb = self.clip_model.token_embedding(tokens).to(dtype=self.dtype)\n",
        "        # register/replace buffers so state_dict remains consistent\n",
        "        self.register_buffer(\"class_token_embeddings\", emb)\n",
        "        self.register_buffer(\"class_token_ids\", tokens)\n",
        "\n",
        "    #was it causing problems?   #trying to use ai to solve\n",
        "    def forward(self, image_features):\n",
        "      \"\"\"\n",
        "      Genera prompts per ogni immagine e classe.\n",
        "\n",
        "      Args:\n",
        "          image_features: tensor (B, ctx_dim) dalle immagini encodate\n",
        "\n",
        "      Returns:\n",
        "          prompts: tensor (B, num_classes, seq_len_total, ctx_dim) in FLOAT32\n",
        "      \"\"\"\n",
        "      batch_size = image_features.shape[0]\n",
        "      num_classes, seq_len, ctx_dim = self.class_token_embeddings.shape\n",
        "\n",
        "      # FIX: converting EVERYTHING to float32 right away\n",
        "      image_features = image_features.to(dtype=torch.float32)\n",
        "\n",
        "      # Step 1: genrating conditional token\n",
        "      cond_token = self.meta_net(image_features)  # (B, ctx_dim)\n",
        "      cond_token = cond_token.unsqueeze(1).to(dtype=torch.float32)  # (B, 1, ctx_dim)\n",
        " \n",
        "      # Step 2: Context vectors\n",
        "      ctx = self.ctx.unsqueeze(0).unsqueeze(0).to(dtype=torch.float32)  # (1, 1, n_ctx, ctx_dim)\n",
        "      ctx = ctx.repeat(batch_size, num_classes, 1, 1)  # (B, num_classes, n_ctx, ctx_dim)\n",
        "\n",
        "      # Step 3: Conditional token expansion\n",
        "      cond_expand = cond_token.unsqueeze(1).to(dtype=torch.float32)  # (B, 1, 1, ctx_dim)\n",
        "      cond_expand = cond_expand.repeat(1, num_classes, 1, 1)  # (B, num_classes, 1, ctx_dim)\n",
        "\n",
        "      # Step 4: Class embeddings expansion\n",
        "      class_embed = self.class_token_embeddings.unsqueeze(0).to(dtype=torch.float32)  # (1, num_classes, seq_len, ctx_dim)\n",
        "      class_embed = class_embed.repeat(batch_size, 1, 1, 1)  # (B, num_classes, seq_len, ctx_dim)\n",
        "\n",
        "      # Step 5: Concatenate\n",
        "      prompts = torch.cat([ctx, cond_expand, class_embed], dim=2)  # (B, num_classes, n_ctx + 1 + seq_len, ctx_dim)\n",
        "\n",
        "      # Trim to CLIP max length: 77 tokens\n",
        "      prompts = prompts[:, :, :self.clip_context_length, :]\n",
        "\n",
        "      # Ensure return type is float32 -> was causing problems\n",
        "      return prompts.to(dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7b162a",
      "metadata": {
        "id": "6c7b162a"
      },
      "source": [
        "## CoCoOpTrainer: Training and Evaluation\n",
        "\n",
        "Class that manages:\n",
        "\n",
        "**1. Initialization:**\n",
        "- Create PromptLearner\n",
        "- Freeze CLIP (`requires_grad=False`)\n",
        "- Configure SGD optimizer for prompt learner only\n",
        "\n",
        "**2. train_epoch():**\n",
        "- Forward: Image encoder + PromptLearner + Text encoder\n",
        "- **Critical step:** Encode soft prompts through text transformer\n",
        "  - Add positional embeddings\n",
        "  - Pass through CLIP's transformer\n",
        "  - Extract first token\n",
        "  - Apply final layer norm + projection\n",
        "- Compute loss: Cross-entropy on base classes\n",
        "- Backward: Backprop only in PromptLearner\n",
        "- Return: Average loss of the epoch\n",
        "\n",
        "**3. eval():**\n",
        "- Same forward procedure as training\n",
        "- Without backward pass\n",
        "- Compute accuracy on any dataset (base or novel)\n",
        "\n",
        "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
        "because that method expects integer tokens, not embeddings.\n",
        "We manually forward through the text transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "072ba719",
      "metadata": {
        "id": "072ba719"
      },
      "outputs": [],
      "source": [
        "class CoCoOpTrainer:\n",
        "    def __init__(self, clip_model, base_classnames, base_classes,\n",
        "                 novel_classes, device, lr=0.002):\n",
        "        \"\"\"Trainer ultra-light per memoria limitata\"\"\"\n",
        "        self.clip_model = clip_model\n",
        "        self.base_classnames = base_classnames\n",
        "        self.base_classes = base_classes\n",
        "        self.novel_classes = novel_classes\n",
        "        self.device = device\n",
        "\n",
        "        # Contig mapping\n",
        "        self.contig_cat2idx = {cat: idx for idx, cat in enumerate(self.base_classes)}\n",
        "\n",
        "        # Freeze CLIP\n",
        "        for p in clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Prompt learner\n",
        "        self.prompt_learner = CoCoOpPromptLearner(\n",
        "            clip_model,\n",
        "            base_classnames\n",
        "        ).to(device=device, dtype=clip_model.dtype)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.prompt_learner.parameters(),\n",
        "            lr=lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=5e-4\n",
        "        )\n",
        "\n",
        "    def train_epoch(self, train_dataset, batch_size=1):\n",
        "        \"\"\"Training minimale - NO inplace operations\"\"\"\n",
        "        self.prompt_learner.train()\n",
        "        self.clip_model.eval()\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"CoCoOp\")):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Image features\n",
        "            with torch.no_grad():\n",
        "                img_feat = self.clip_model.encode_image(images)\n",
        "            img_feat = img_feat.float()\n",
        "            img_feat = img_feat / (img_feat.norm(dim=-1, keepdim=True) + 1e-8)  # NO inplace\n",
        "\n",
        "            # Prompts (already float32 from prompt_learner)\n",
        "            prompts = self.prompt_learner(img_feat)\n",
        "            B, N, L, D = prompts.shape\n",
        "            prompts_flat = prompts.view(B * N, L, D)\n",
        "\n",
        "            # Text encoding - FORCE float32\n",
        "            pos_emb = self.clip_model.positional_embedding[:L].float()\n",
        "            x = prompts_flat.float()\n",
        "            x = x + pos_emb\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = x.float()\n",
        "\n",
        "            # Transformer\n",
        "            x = self.clip_model.transformer(x)\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = x[:, 0, :].contiguous()\n",
        "\n",
        "            # Final layers\n",
        "            x = self.clip_model.ln_final(x.float())\n",
        "            text_feat = x.float() @ self.clip_model.text_projection  # (B*N, 512)\n",
        "            text_feat = text_feat.view(B, N, -1)\n",
        "            text_feat = text_feat / (text_feat.norm(dim=-1, keepdim=True) + 1e-8)  # NO inplace\n",
        "\n",
        "            # Loss\n",
        "            logit_scale = self.clip_model.logit_scale.exp()\n",
        "            logits = logit_scale * (img_feat.unsqueeze(1) * text_feat).sum(-1)\n",
        "\n",
        "            # Map labels safely to contiguous indices\n",
        "            mapped = []\n",
        "            missing = []\n",
        "            for l in labels:\n",
        "                key = l.item()\n",
        "                idx = self.contig_cat2idx.get(key)\n",
        "                if idx is None:\n",
        "                    missing.append(key)\n",
        "                else:\n",
        "                    mapped.append(idx)\n",
        "            if missing:\n",
        "                raise ValueError(f\"Found labels not in trainer's categories: {missing}.\\nAvailable categories: {list(self.contig_cat2idx.keys())}\")\n",
        "\n",
        "            labels_mapped = torch.tensor(mapped, dtype=torch.long, device=self.device)\n",
        "\n",
        "            loss = F.cross_entropy(logits, labels_mapped)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "            del prompts, prompts_flat, x, logits, text_feat, img_feat\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return total_loss / max(1, n_batches)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval(self, dataset, categories, batch_size=1, classnames=None):\n",
        "        \"\"\"Evaluation - NO inplace operations\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset to evaluate\n",
        "            categories: list of category indices (contiguous ids from original dataset)\n",
        "            batch_size: dataloader batch size\n",
        "            classnames: optional list of class name strings corresponding to `categories`.\n",
        "                        If provided, the prompt learner will be temporarily switched\n",
        "                        to use these classnames when building prompts.\n",
        "        \"\"\"\n",
        "        self.prompt_learner.eval()\n",
        "        self.clip_model.eval()\n",
        "\n",
        "        contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "        # If requested, swap prompt learner class embeddings to match `classnames`.\n",
        "        old_emb = None\n",
        "        old_ids = None\n",
        "        if classnames is not None:\n",
        "            old_emb = self.prompt_learner.class_token_embeddings.clone()\n",
        "            old_ids = self.prompt_learner.class_token_ids.clone()\n",
        "            self.prompt_learner.set_classnames(classnames)\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(dataloader, desc=\"Eval\"):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            img_feat = self.clip_model.encode_image(images).float()\n",
        "            img_feat = img_feat / (img_feat.norm(dim=-1, keepdim=True) + 1e-8)  # NO inplace\n",
        "\n",
        "            prompts = self.prompt_learner(img_feat)\n",
        "            B, N, L, D = prompts.shape\n",
        "            prompts_flat = prompts.view(B * N, L, D)\n",
        "\n",
        "            # Text encoding\n",
        "            pos_emb = self.clip_model.positional_embedding[:L].float()\n",
        "            x = prompts_flat.float()\n",
        "            x = x + pos_emb\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = x.float()\n",
        "\n",
        "            x = self.clip_model.transformer(x)\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = x[:, 0, :].contiguous()\n",
        "\n",
        "            x = self.clip_model.ln_final(x.float())\n",
        "            text_feat = x.float() @ self.clip_model.text_projection\n",
        "            text_feat = text_feat.view(B, N, -1)\n",
        "            text_feat = text_feat / (text_feat.norm(dim=-1, keepdim=True) + 1e-8)  #NO inplace -> was making runtime crash since it worked directly on memory\n",
        "\n",
        "            logit_scale = self.clip_model.logit_scale.exp()\n",
        "            logits = logit_scale * (img_feat.unsqueeze(1) * text_feat).sum(-1)\n",
        "\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            # Map labels safely to contiguous indices for evaluation\n",
        "            mapped = []\n",
        "            missing = []\n",
        "            for l in labels:\n",
        "                key = l.item()\n",
        "                idx = contig_cat2idx.get(key)\n",
        "                if idx is None:\n",
        "                    missing.append(key)\n",
        "                else:\n",
        "                    mapped.append(idx)\n",
        "            if missing:\n",
        "                raise ValueError(f\"Found labels not in evaluation categories: {missing}.\\nProvided categories: {categories}\")\n",
        "\n",
        "            labels_mapped = torch.tensor(mapped, dtype=torch.long, device=self.device)\n",
        "\n",
        "            correct += (pred == labels_mapped).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            del prompts, prompts_flat, x, logits, text_feat, img_feat\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # restore old class buffers if we swapped them\n",
        "        if classnames is not None and old_emb is not None and old_ids is not None:\n",
        "            self.prompt_learner.register_buffer(\"class_token_embeddings\", old_emb)\n",
        "            self.prompt_learner.register_buffer(\"class_token_ids\", old_ids)\n",
        "\n",
        "        return correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838deabc",
      "metadata": {
        "id": "838deabc"
      },
      "source": [
        "## Training CoCoOp\n",
        "\n",
        "We will train the PromptLearner for **5 epochs** on **base classes only**.\n",
        "\n",
        "**Hyperparameters:**\n",
        "- Learning rate: 0.002 (SGD)\n",
        "- Momentum: 0.9\n",
        "- Weight decay: 5e-4\n",
        "- Batch size: 1\n",
        "- Epochs: 5\n",
        "\n",
        "**What happens:**\n",
        "- Context vectors V1...VM adapt to the Flowers102 dataset\n",
        "- MetaNetwork learns to generate useful conditional tokens\n",
        "- CLIP remains frozen (unchanged)\n",
        "\n",
        "**Expected output:**\n",
        "- Initial loss: ~3.0\n",
        "- Final loss: ~1.3-1.5\n",
        "- Training time: ~5-10 minutes on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2d6800",
      "metadata": {},
      "source": [
        "## How I fixed\n",
        "\n",
        "| Problem           | Fix                                                  |\n",
        "| ----------------- | ---------------------------------------------------- |\n",
        "| Inplace /=        | text_feat = text_feat / (text_feat.norm(...) + 1e-8) |\n",
        "| Dtype mismatch    | .float() everywhere prima del transformer               |\n",
        "| Memory leak       | del + torch.cuda.empty_cache() each batch            |\n",
        "| Gradient tracking | NO .detach() in forward pass                        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c2feda02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "c2feda02",
        "outputId": "2ba903f3-feb4-42a2-82fd-ccbabb9b5ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base classnames (51): ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold']...\n",
            "\n",
            "[CoCoOp] ctx_dim=512, max_len=77\n",
            "\n",
            "============================================================\n",
            "Training CoCoOp\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CoCoOp:   0%|          | 1/510 [00:18<2:35:56, 18.38s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1031236525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2251595082.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_dataset, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
        "print(f\"Base classnames ({len(base_classnames)}): {base_classnames[:5]}...\\n\")\n",
        "\n",
        "trainer = CoCoOpTrainer(\n",
        "    clip_model=model,\n",
        "    base_classnames=base_classnames,\n",
        "    base_classes=base_classes,\n",
        "    novel_classes=novel_classes,\n",
        "    device=device,\n",
        "    lr=0.002\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training CoCoOp\") #memory optimised (could it be better?)\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = trainer.train_epoch(train_base, batch_size=1)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69c077f",
      "metadata": {
        "id": "b69c077f"
      },
      "source": [
        "## Final Evaluation (CoCoOp only)\n",
        "\n",
        "We'll evaluate the model with:\n",
        "1. Test Base\n",
        "2. Test Novel\n",
        "\n",
        "Computing Harmonic Mean between them to evaluate the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49984fae",
      "metadata": {
        "id": "49984fae"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION AND COMPARISON (CoCoOp Only)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare class name lists for evaluation\n",
        "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
        "novel_classnames = [CLASS_NAMES[i] for i in novel_classes]\n",
        "\n",
        "# Evaluating CoCoOp on base and novel\n",
        "base_acc_cocoop = trainer.eval(test_base, base_classes, batch_size=64, classnames=base_classnames)\n",
        "novel_acc_cocoop = trainer.eval(test_novel, novel_classes, batch_size=64, classnames=novel_classnames)\n",
        "hm_cocoop = harmonic_mean(base_acc_cocoop, novel_acc_cocoop)\n",
        "\n",
        "# Printing results for CoCoOp\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CoCoOp RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"   Base Accuracy:  {base_acc_cocoop*100:6.2f}%\")\n",
        "print(f\"   Novel Accuracy: {novel_acc_cocoop*100:6.2f}%\")\n",
        "print(f\"   Harmonic Mean:  {hm_cocoop*100:6.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
