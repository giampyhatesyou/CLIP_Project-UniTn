{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff453b1a",
      "metadata": {
        "id": "ff453b1a"
      },
      "source": [
        "# CLIP with Flowers!?!?!??!?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73175206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73175206",
        "outputId": "5ec8d849-42b0-4539-b0a8-772659c5d5db"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Prefer expandable segments to reduce fragmentation (restart kernel after changing)\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Ensure CLIP is installed in the current kernel; install if missing.\n",
        "# Using subprocess with sys.executable to target the same Python interpreter.\n",
        "try:\n",
        "    import clip\n",
        "except Exception:\n",
        "    import subprocess, importlib\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"git+https://github.com/openai/CLIP.git\"], stdout=subprocess.DEVNULL)\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from collections import OrderedDict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "df3e678e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3e678e",
        "outputId": "2a8f176f-5873-482d-d3fb-cde406a5e386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIP: già installato\n",
            "python: /usr/bin/python3\n",
            "torch: 2.9.0+cu126 cuda_available: True\n",
            "mps_available: False\n",
            "Riavvia il kernel se necessario, poi esegui questa cella e procedi con l'allenamento CoCoOp.\n"
          ]
        }
      ],
      "source": [
        "# Check environment and CLIP installation\n",
        "import sys, importlib, torch\n",
        "\n",
        "try:\n",
        "    import clip\n",
        "    print(\"CLIP: già installato\")\n",
        "except Exception:\n",
        "    print(\"CLIP non trovato: eseguo installazione nel kernel corrente...\")\n",
        "    import importlib\n",
        "    # Preferisci %pip per installare nel kernel Jupyter corrente; fallback a subprocess\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"git+https://github.com/openai/CLIP.git\"])\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "    print(\"CLIP installato correttamente nel kernel corrente\")\n",
        "\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
        "# su mac con Apple Silicon, controlla MPS\n",
        "try:\n",
        "    print(\"mps_available:\", torch.backends.mps.is_available())\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('Riavvia il kernel se necessario, poi esegui questa cella e procedi con l\\'allenamento CoCoOp.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9e45cb",
      "metadata": {
        "id": "cf9e45cb"
      },
      "source": [
        "## Dataset Functions\n",
        "\n",
        "We define utility functions for:\n",
        "- **`get_data()`**: Load Flowers102 from torchvision\n",
        "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
        "- **`split_data()`**: Filter images for base/novel in each split\n",
        "\n",
        "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9adfe795",
      "metadata": {
        "id": "9adfe795"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    \"\"\"Return base and novel class id lists using the actual labels present\n",
        "    in the dataset. Prefer public attributes (`targets` then `labels`) and\n",
        "    only fall back to the dataset private attribute `_labels` if neither is\n",
        "    available.\n",
        "    \"\"\"\n",
        "    labels = getattr(dataset, \"targets\", None)\n",
        "    if labels is None:\n",
        "        labels = getattr(dataset, \"labels\", None)\n",
        "\n",
        "    if labels is None and hasattr(dataset, \"_labels\"):\n",
        "        # FALLBACK: using private dataset internals. Flowers102 exposes\n",
        "        # `_labels` but this is a private attribute; prefer public attributes\n",
        "        # above so future datasets remain compatible.\n",
        "        labels = dataset._labels\n",
        "\n",
        "    if labels is None:\n",
        "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    num_classes = len(unique_labels)\n",
        "    mid = num_classes // 2\n",
        "    base_classes = unique_labels[:mid]\n",
        "    novel_classes = unique_labels[mid:]\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bda8d2",
      "metadata": {
        "id": "74bda8d2"
      },
      "source": [
        "## Class Names and Dataset Loading\n",
        "\n",
        "We load the names of 102 flower classes from Flowers102.\n",
        "\n",
        "This is **critical** for CLIP:\n",
        "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
        "- Each prompt is encoded by CLIP's text encoder\n",
        "- Image features are compared against these text templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f443bb94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f443bb94",
        "outputId": "ed41ef4f-067e-487e-928a-e8a3063d9bc6"
      },
      "outputs": [],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "# Uncomment to see class names\n",
        "# print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "# print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e8a6d3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a6d3c9",
        "outputId": "aaf9b04a-ce42-40b1-cc0a-b9bbea0c5b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: ViT-B/16\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ViT-B/16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e05089",
      "metadata": {
        "id": "e0e05089"
      },
      "source": [
        "## Load Flowers102 and Split Base/Novel\n",
        "\n",
        "We load the 3 splits (train, val, test) and divide into base/novel.\n",
        "\n",
        "**Statistics:**\n",
        "- Train Base: 10 images × 51 classes = 510 images\n",
        "- Val Base: 10 images × 51 classes = 510 images\n",
        "- Test Base: ~10 images × 51 classes (from test split)\n",
        "- Test Novel: Remaining (~10 per class)\n",
        "\n",
        "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9fdd5516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdd5516",
        "outputId": "005897b3-5a96-4f5f-d8d5-02fce24f478f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Base (few-shot): 510 samples (16 shots per class)\n",
            "Val Base: 510 samples\n",
            "Test Base: 2473 samples\n",
            "Test Novel: 3676 samples\n"
          ]
        }
      ],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# Few-shot: sample `shots_per_class` images per base class from the train split\n",
        "shots_per_class = 16\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Collect indices per class in the original train_set\n",
        "indices_per_class = {c: [] for c in base_classes}\n",
        "for idx, label in enumerate(train_set._labels):\n",
        "    if label in indices_per_class:\n",
        "        indices_per_class[label].append(idx)\n",
        "\n",
        "selected = []\n",
        "for c in base_classes:\n",
        "    inds = indices_per_class.get(c, [])\n",
        "    random.shuffle(inds)\n",
        "    # take up to shots_per_class (if fewer available, take all)\n",
        "    selected.extend(inds[:shots_per_class])\n",
        "\n",
        "# Create the few-shot training subset\n",
        "train_base = torch.utils.data.Subset(train_set, selected)\n",
        "\n",
        "# validation and test splits remain full (or filtered by base classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Train Base (few-shot): {len(train_base)} samples ({shots_per_class} shots per class)\")\n",
        "print(f\"Val Base: {len(val_base)} samples\")\n",
        "print(f\"Test Base: {len(test_base)} samples\")\n",
        "print(f\"Test Novel: {len(test_novel)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8071f907",
      "metadata": {
        "id": "8071f907"
      },
      "source": [
        "## Harmonic Mean (HM)\n",
        "\n",
        "Standard metric for few-shot adaptation papers.\n",
        "\n",
        "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
        "\n",
        "**Why HM instead of arithmetic mean?**\n",
        "- HM heavily penalizes outliers\n",
        "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
        "- Forces the model to balance both accuracies\n",
        "\n",
        "**Obiettivo:** massimizzare l'HM tra `base_acc_cocoop` e `novel_acc_cocoop`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eec2ec0",
      "metadata": {
        "id": "3eec2ec0"
      },
      "outputs": [],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    # Guard against zero to avoid division-by-zero errors\n",
        "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
        "        return 0.0\n",
        "    numerator = 2.0\n",
        "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12012951",
      "metadata": {},
      "source": [
        "## Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f29fc20f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"Encodes soft prompts through CLIP's text transformer.\"\"\"\n",
        "    \n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            prompts: (batch_size, n_tokens, 512) soft prompt embeddings\n",
        "            tokenized_prompts: (n_cls, n_tokens) token indices for EOT detection\n",
        "        \n",
        "        Returns:\n",
        "            text_features: (batch_size, 512) per-class text features\n",
        "        \"\"\"\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        \n",
        "        # Extract EOT token\n",
        "        x = x[torch.arange(x.shape), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d4a384",
      "metadata": {
        "id": "84d4a384"
      },
      "source": [
        "## MetaNetwork: Conditional Token Generator\n",
        "\n",
        "**Problem:** Fixed prompts don't adapt to each image.\n",
        "\n",
        "**Solution:** A small neural network that transforms image features into a conditional token.\n",
        "\n",
        "**Parameters:** ~256K (negligible vs. fine-tuning)\n",
        "\n",
        "**Effect:** Each image gets a different prompt → instance-level adaptation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cd4e54f3",
      "metadata": {
        "id": "cd4e54f3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MetaNetwork è una piccola rete neurale (MLP con 2 layer)\n",
        "che trasforma le image_features (512-dim) in un token\n",
        "condizionale (512-dim) usato in CoCoOp.\n",
        "\n",
        "Questo token varia per ogni immagine, permettendo prompt\n",
        "personalizzati per ogni input.\n",
        "\"\"\"\n",
        "\\\n",
        "class MetaNetwork(nn.Module):\n",
        "    def __init__(self, ctx_dim=512, hidden_dim=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ctx_dim: dimensione degli embeddings (512 per ViT-B/16)\n",
        "            hidden_dim: dimensione dello strato nascosto\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(ctx_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.linear2 = nn.Linear(hidden_dim, ctx_dim)\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: tensor (B, ctx_dim) dalle immagini encodate\n",
        "\n",
        "        Returns:\n",
        "            conditional_token: tensor (B, ctx_dim)\n",
        "        \"\"\"\n",
        "        # Assicura il tipo corretto (importante per mixed precision)\n",
        "        image_features = image_features.to(self.linear1.weight.dtype)\n",
        "\n",
        "        out = self.linear1(image_features)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916487b1",
      "metadata": {
        "id": "916487b1"
      },
      "source": [
        "## CoCoOpPromptLearner: Dynamic Prompts\n",
        "\n",
        "\n",
        "**Components:**\n",
        "1. **V1...VM:** 16 context vectors (learned via SGD)\n",
        "   - Shape: (16, 512) tensors\n",
        "   - Initialized randomly from N(0, 0.02²)\n",
        "   - Optimized during training\n",
        "\n",
        "2. **π(x):** Conditional token (generated per image)\n",
        "   - Shape: (B, 512) from MetaNetwork output\n",
        "   - Different for each image\n",
        "\n",
        "3. **[CLASS]:** Class name embedding\n",
        "   - Shape: (seq_len, 512) from CLIP's token embedding\n",
        "   - Same for all images of the same class\n",
        "\n",
        "**Forward Pass:**\n",
        "- Input: image_features (B, 512)\n",
        "- Output: prompts (B, num_classes, seq_len_total, 512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb9b5b5",
      "metadata": {
        "id": "4cb9b5b5"
      },
      "outputs": [],
      "source": [
        "class CoCoOpPromptLearner(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable prompt module with meta-net for instance-conditioned adaptation.\n",
        "    \n",
        "    Based on CoCoOp: https://arxiv.org/abs/2203.05557\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        \n",
        "        n_cls = len(classnames)\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape  # 512 per ViT-B/16\n",
        "        vis_dim = clip_model.visual.output_dim  # 512 per ViT-B/16\n",
        "        \n",
        "        # ✅ CONTEXT VECTORS (learnable)\n",
        "        if ctx_init:\n",
        "            # Initialize from provided text\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            # Random initialization\n",
        "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        \n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words: {n_ctx}\")\n",
        "        \n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "        \n",
        "        # ✅ META NETWORK (corrected architecture da GitHub)\n",
        "        self.meta_net = nn.Sequential(OrderedDict([\n",
        "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
        "            (\"relu\", nn.ReLU(inplace=True)),\n",
        "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
        "        ]))\n",
        "        \n",
        "        # ✅ CLASS EMBEDDINGS\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "        _tokenizer = _Tokenizer()\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        \n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "        \n",
        "        # Register buffers for prefix and suffix tokens\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])  # CLS, EOS\n",
        "        \n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.dtype = dtype\n",
        "    \n",
        "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "        \"\"\"Construct prompt embeddings from context and class-specific tokens.\"\"\"\n",
        "        if label is not None:\n",
        "            prefix = prefix[label]\n",
        "            suffix = suffix[label]\n",
        "        \n",
        "        prompts = torch.cat([\n",
        "            prefix,  # (dim0, 1, dim)\n",
        "            ctx,     # (dim0, n_ctx, dim)\n",
        "            suffix,  # (dim0, *, dim)\n",
        "        ], dim=1)\n",
        "        \n",
        "        return prompts\n",
        "    \n",
        "    def forward(self, im_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            im_features: (batch_size, 512) image features from CLIP's ViT\n",
        "        \n",
        "        Returns:\n",
        "            prompts: (batch_size, n_cls, n_tokens, 512) soft prompts for all classes\n",
        "        \"\"\"\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        \n",
        "        # ✅ INSTANCE-CONDITIONED CONTEXT\n",
        "        ctx = self.ctx.unsqueeze(0)  # (1, n_ctx, ctx_dim)\n",
        "        bias = self.meta_net(im_features)  # (batch_size, ctx_dim)\n",
        "        bias = bias.unsqueeze(1)  # (batch_size, 1, ctx_dim)\n",
        "        \n",
        "        # ✅ SHIFT CONTEXT per image\n",
        "        ctx_shifted = ctx + bias  # (batch_size, n_ctx, ctx_dim)\n",
        "        \n",
        "        # Build prompts for each image and each class\n",
        "        prompts = []\n",
        "        for ctx_shifted_i in ctx_shifted:\n",
        "            # Replicate across classes\n",
        "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tokens, ctx_dim)\n",
        "            prompts.append(pts_i)\n",
        "        \n",
        "        prompts = torch.stack(prompts)  # (batch_size, n_cls, n_tokens, ctx_dim)\n",
        "        \n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7b162a",
      "metadata": {
        "id": "6c7b162a"
      },
      "source": [
        "## CoCoOpTrainer: Training and Evaluation\n",
        "\n",
        "Class that manages:\n",
        "\n",
        "**1. Initialization:**\n",
        "- Create PromptLearner\n",
        "- Freeze CLIP (`requires_grad=False`)\n",
        "- Configure SGD optimizer for prompt learner only\n",
        "\n",
        "**2. train_epoch():**\n",
        "- Forward: Image encoder + PromptLearner + Text encoder\n",
        "- **Critical step:** Encode soft prompts through text transformer\n",
        "  - Add positional embeddings\n",
        "  - Pass through CLIP's transformer\n",
        "  - Extract first token\n",
        "  - Apply final layer norm + projection\n",
        "- Compute loss: Cross-entropy on base classes\n",
        "- Backward: Backprop only in PromptLearner\n",
        "- Return: Average loss of the epoch\n",
        "\n",
        "**3. eval():**\n",
        "- Same forward procedure as training\n",
        "- Without backward pass\n",
        "- Compute accuracy on any dataset (base or novel)\n",
        "\n",
        "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
        "because that method expects integer tokens, not embeddings.\n",
        "We manually forward through the text transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "072ba719",
      "metadata": {
        "id": "072ba719"
      },
      "outputs": [],
      "source": [
        "class CoCoOpPromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_dim=512, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.n_ctx = n_ctx\n",
        "        self.ctx_dim = ctx_dim\n",
        "        self.device = device\n",
        "\n",
        "        # ✅ LEARNABLE CONTEXT\n",
        "        self.ctx = nn.Parameter(\n",
        "            torch.randn(n_ctx, ctx_dim).float() * 0.02\n",
        "        )\n",
        "\n",
        "        # ✅ META NETWORK\n",
        "        self.meta_net = nn.Sequential(\n",
        "            nn.Linear(ctx_dim, ctx_dim // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(ctx_dim // 2, ctx_dim)\n",
        "        )\n",
        "\n",
        "        # ✅ CLASS EMBEDDINGS - SEMPLICE E DIRETTO\n",
        "        # Costruisci gli embeddings SENZA tokenizer esterno\n",
        "        class_embs = []\n",
        "        for classname in classnames:\n",
        "            # Crea il testo\n",
        "            text = f\"a photo of {classname}\"\n",
        "\n",
        "            # ✅ TOKENIZZA MANUALMENTE usando il tokenizer di CLIP interno\n",
        "            # Importa il tokenizer\n",
        "            from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "            _tokenizer = _Tokenizer()\n",
        "\n",
        "            # Tokenizza il testo\n",
        "            tokens = _tokenizer.encode(text)\n",
        "            # Padding a 77 token\n",
        "            tokens = tokens + [0] * (77 - len(tokens))\n",
        "            tokens = torch.tensor([tokens[:77]]).to(device)\n",
        "\n",
        "            # Estrai l'embedding\n",
        "            with torch.no_grad():\n",
        "                emb = clip_model.token_embedding(tokens).type(torch.float32)  # (1, 77, 512)\n",
        "            class_embs.append(emb)\n",
        "\n",
        "        # Stack: (num_classes, 77, 512)\n",
        "        class_token_embeddings = torch.cat(class_embs, dim=0)\n",
        "\n",
        "        self.register_buffer(\"class_token_embeddings\", class_token_embeddings)\n",
        "\n",
        "        self.clip_context_length = 77\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        \"\"\"\n",
        "        image_features: (B, 512)\n",
        "        Output: (B, num_classes, 77, 512)\n",
        "        \"\"\"\n",
        "        batch_size = image_features.shape[0]\n",
        "        num_classes = self.class_token_embeddings.shape[0]\n",
        "\n",
        "        # Meta network - genera condizionamenti\n",
        "        cond = self.meta_net(image_features)  # (B, 512)\n",
        "\n",
        "        # Context base\n",
        "        ctx = self.ctx.unsqueeze(0)  # (1, n_ctx, 512)\n",
        "        ctx = ctx.repeat(batch_size, 1, 1)  # (B, n_ctx, 512)\n",
        "\n",
        "        # Modula context con meta_net\n",
        "        ctx = ctx + cond.unsqueeze(1) * 0.1  # (B, n_ctx, 512)\n",
        "\n",
        "        # Class embeddings\n",
        "        class_embed = self.class_token_embeddings.unsqueeze(0)  # (1, num_classes, 77, 512)\n",
        "        class_embed = class_embed.repeat(batch_size, 1, 1, 1)  # (B, num_classes, 77, 512)\n",
        "\n",
        "        # Build prompts per ogni classe\n",
        "        prompts_list = []\n",
        "        for i in range(num_classes):\n",
        "            # Prendi i token significativi dalla class embedding\n",
        "            prompt_i = torch.cat([\n",
        "                ctx,  # (B, n_ctx, 512)\n",
        "                class_embed[:, i, 1:self.clip_context_length-self.n_ctx, :]  # (B, 77-n_ctx-1, 512)\n",
        "            ], dim=1)  # (B, 77, 512)\n",
        "\n",
        "            # Garantisci che sia esattamente 77 token\n",
        "            if prompt_i.shape[1] < self.clip_context_length:\n",
        "                pad_len = self.clip_context_length - prompt_i.shape[1]\n",
        "                prompt_i = torch.cat([\n",
        "                    prompt_i,\n",
        "                    torch.zeros(batch_size, pad_len, self.ctx_dim, device=image_features.device, dtype=torch.float32)\n",
        "                ], dim=1)\n",
        "            elif prompt_i.shape[1] > self.clip_context_length:\n",
        "                prompt_i = prompt_i[:, :self.clip_context_length, :]\n",
        "\n",
        "            prompts_list.append(prompt_i)\n",
        "\n",
        "        prompts = torch.stack(prompts_list, dim=1)  # (B, num_classes, 77, 512)\n",
        "\n",
        "        return prompts.to(dtype=torch.float32)\n",
        "\n",
        "\n",
        "class CoCoOpTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for CoCoOp with corrected hyperparameters and scheduler.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, clip_model, classnames, base_classes, novel_classes, \n",
        "                 device='cuda', lr=0.002, n_ctx=4, num_epochs=10):\n",
        "        \"\"\"\n",
        "        ✅ CORRECTED hyperparameters\n",
        "        \n",
        "        Args:\n",
        "            lr: 0.002 (NOT 0.02)\n",
        "            num_epochs: for scheduler initialization\n",
        "        \"\"\"\n",
        "        self.clip_model = clip_model.float()\n",
        "        self.classnames = classnames\n",
        "        self.base_classes = base_classes\n",
        "        self.novel_classes = novel_classes\n",
        "        self.device = device\n",
        "        self.num_epochs = num_epochs\n",
        "        \n",
        "        # Mapping from class ID to index\n",
        "        self.contig_cat2idx = {cat: idx for idx, cat in enumerate(self.base_classes)}\n",
        "        \n",
        "        # Freeze CLIP\n",
        "        for p in self.clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Create model\n",
        "        self.model = CustomCLIP(self.clip_model, classnames, n_ctx=n_ctx, \n",
        "                               device=device).to(device)\n",
        "        \n",
        "        # Only optimize prompt learner\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.model.prompt_learner.parameters(),\n",
        "            lr=lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=5e-4\n",
        "        )\n",
        "        \n",
        "        # ✅ SCHEDULER INITIALIZED HERE\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs)\n",
        "        \n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"CoCoOpTrainer initialized:\")\n",
        "        print(f\"  LR: {lr}\")\n",
        "        print(f\"  Momentum: 0.9\")\n",
        "        print(f\"  Weight decay: 5e-4\")\n",
        "        print(f\"  Scheduler: CosineAnnealing (T_max={num_epochs})\")\n",
        "        print(f\"  Trainable params: {sum(p.numel() for p in self.model.prompt_learner.parameters() if p.requires_grad):,}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    def train_epoch(self, train_dataset, batch_size=4):\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "        \n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "            images = images.to(self.device).float()\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            # ✅ Map class labels to contiguous indices\n",
        "            labels_mapped = torch.tensor(\n",
        "                [self.contig_cat2idx[l.item()] for l in labels],\n",
        "                dtype=torch.long,\n",
        "                device=self.device\n",
        "            )\n",
        "            \n",
        "            # Forward pass\n",
        "            loss = self.model(images, labels_mapped)\n",
        "            \n",
        "            # Backward\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "            \n",
        "            # Debug: print gradients on first batch\n",
        "            if batch_idx == 0:\n",
        "                print(f\"\\n{'='*70}\")\n",
        "                print(\"GRADIENT CHECK (first batch):\")\n",
        "                for name, param in self.model.prompt_learner.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        grad_norm = param.grad.norm().item()\n",
        "                        print(f\"  {name:40s} | grad_norm: {grad_norm:.8f}\")\n",
        "                print(f\"{'='*70}\\n\")\n",
        "        \n",
        "        # Step scheduler\n",
        "        self.scheduler.step()\n",
        "        \n",
        "        return total_loss / max(1, n_batches)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def eval(self, dataset, categories, batch_size=64):\n",
        "        \"\"\"\n",
        "        ✅ CORRECTED SIGNATURE: removed 'classnames' parameter\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(self.device).float()\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            # Get logits\n",
        "            logits = self.model(images)  # (batch_size, n_cls)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            \n",
        "            # Map labels\n",
        "            labels_mapped = torch.tensor(\n",
        "                [contig_cat2idx[l.item()] for l in labels],\n",
        "                dtype=torch.long,\n",
        "                device=self.device\n",
        "            )\n",
        "            \n",
        "            correct += (pred == labels_mapped).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        accuracy = correct / total if total > 0 else 0.0\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "959af3c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    \"\"\"CLIP model with learnable prompts.\"\"\"\n",
        "    \n",
        "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.prompt_learner = CoCoOpPromptLearner(clip_model, classnames, \n",
        "                                   n_ctx=n_ctx,\n",
        "                                   device=device)\n",
        "\n",
        "        \n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "    \n",
        "    def forward(self, image, label=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image: (batch_size, 3, 224, 224)\n",
        "            label: (batch_size,) class labels during training\n",
        "        \n",
        "        Returns:\n",
        "            logits: (batch_size, n_cls) or loss if label provided\n",
        "        \"\"\"\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        \n",
        "        # ✅ Encode images\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        # ✅ Generate instance-conditioned prompts\n",
        "        prompts = self.prompt_learner(image_features)  # (batch, n_cls, n_tokens, dim)\n",
        "        \n",
        "        # ✅ Encode prompts and compute logits\n",
        "        batch_size = prompts.shape\n",
        "        logits = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            # Prompts for this image: (n_cls, n_tokens, dim)\n",
        "            pts_i = prompts[i]\n",
        "            # Image feature for this image: (dim,)\n",
        "            imf_i = image_features[i]\n",
        "            \n",
        "            # Encode prompts through text transformer\n",
        "            text_features = self.text_encoder(pts_i, self.tokenized_prompts)\n",
        "            # text_features: (n_cls, dim)\n",
        "            \n",
        "            # ✅ Normalize and compute similarity\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            l_i = logit_scale * (imf_i @ text_features.t())  # (n_cls,)\n",
        "            logits.append(l_i)\n",
        "        \n",
        "        logits = torch.stack(logits)  # (batch_size, n_cls)\n",
        "        \n",
        "        # Return loss during training, logits during evaluation\n",
        "        if label is not None:\n",
        "            return F.cross_entropy(logits, label)\n",
        "        \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838deabc",
      "metadata": {
        "id": "838deabc"
      },
      "source": [
        "## Training CoCoOp\n",
        "\n",
        "We will train the PromptLearner for **5 epochs** on **base classes only**.\n",
        "\n",
        "**Hyperparameters:**\n",
        "- Learning rate: 0.002 (SGD)\n",
        "- Momentum: 0.9\n",
        "- Weight decay: 5e-4\n",
        "- Batch size: 1\n",
        "- Epochs: 5\n",
        "\n",
        "**What happens:**\n",
        "- Context vectors V1...VM adapt to the Flowers102 dataset\n",
        "- MetaNetwork learns to generate useful conditional tokens\n",
        "- CLIP remains frozen (unchanged)\n",
        "\n",
        "**Expected output:**\n",
        "- Initial loss: ~3.0\n",
        "- Final loss: ~1.3-1.5\n",
        "- Training time: ~5-10 minutes on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c2feda02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "c2feda02",
        "outputId": "8dc9c0b8-c51f-40a3-d123-aa34b4ec821e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got torch.Size\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-450841222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Initialize trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m trainer = CoCoOpTrainer(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mclip_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mclassnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1612640578.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clip_model, classnames, base_classes, novel_classes, device, lr, n_ctx, num_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         self.model = CustomCLIP(self.clip_model, classnames, n_ctx=n_ctx, \n\u001b[0m\u001b[1;32m    128\u001b[0m                                device=device).to(device)\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1434315162.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clip_model, classnames, n_ctx, ctx_init, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         self.prompt_learner = CoCoOpPromptLearner(clip_model, classnames, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                    \u001b[0mn_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                    device=device)\n",
            "\u001b[0;32m/tmp/ipython-input-3345771469.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clip_model, classnames, n_ctx, ctx_init, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Random initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mctx_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprompt_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got torch.Size\""
          ]
        }
      ],
      "source": [
        "def harmonic_mean(base_acc, novel_acc):\n",
        "    \"\"\"Compute harmonic mean of accuracies.\"\"\"\n",
        "    if base_acc <= 0 or novel_acc <= 0:\n",
        "        return 0.0\n",
        "    return 2.0 / (1.0 / base_acc + 1.0 / novel_acc)\n",
        "\n",
        "\n",
        "# Setup\n",
        "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
        "novel_classnames = [CLASS_NAMES[i] for i in novel_classes]\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = CoCoOpTrainer(\n",
        "    clip_model=model,\n",
        "    classnames=base_classnames,\n",
        "    base_classes=base_classes,\n",
        "    novel_classes=novel_classes,\n",
        "    device=device,\n",
        "    lr=0.002,  # ✅ CORRECTED (was 0.02)\n",
        "    n_ctx=4,\n",
        "    num_epochs=10\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING CoCoOp\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = trainer.train_epoch(train_base, batch_size=4)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69c077f",
      "metadata": {
        "id": "b69c077f"
      },
      "source": [
        "## Final Evaluation (CoCoOp only)\n",
        "\n",
        "We'll evaluate the model with:\n",
        "1. Test Base\n",
        "2. Test Novel\n",
        "\n",
        "Computing Harmonic Mean between them to evaluate the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49984fae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49984fae",
        "outputId": "44efcb1e-48b1-4aca-9c78-176053deefc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base classnames (51): ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells']...\n",
            "Novel classnames (51): ['wild pansy', 'primula', 'sunflower']...\n",
            "\n",
            "============================================================\n",
            "EVALUATION\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval: 100%|██████████| 39/39 [05:06<00:00,  7.87s/it]\n",
            "Eval: 100%|██████████| 58/58 [07:39<00:00,  7.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CoCoOp RESULTS\n",
            "============================================================\n",
            "\n",
            " Base Accuracy:    0.81%\n",
            " Novel Accuracy:   1.77%\n",
            " Harmonic Mean:    1.11%\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ✅ CORRECTED eval() call (no 'classnames' parameter)\n",
        "base_acc = trainer.eval(test_base, base_classes, batch_size=64)\n",
        "novel_acc = trainer.eval(test_novel, novel_classes, batch_size=64)\n",
        "hm = harmonic_mean(base_acc, novel_acc)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
        "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
        "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
