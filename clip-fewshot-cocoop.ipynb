{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff453b1a",
      "metadata": {
        "id": "ff453b1a"
      },
      "source": [
        "# CLIP with Flowers!?!?!??!?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "73175206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73175206",
        "outputId": "fb56fbf4-f574-42f8-f09f-c0e12dbf36cc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "df3e678e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3e678e",
        "outputId": "4a82e2f7-20d3-47b9-b8c6-5eddbe9efb82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ CLIP already installed\n",
            "Device: cuda\n",
            "PyTorch: 2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import clip\n",
        "    print(\"✓ CLIP already installed\")\n",
        "except Exception:\n",
        "    print(\"Installing CLIP...\")\n",
        "    import subprocess, importlib\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
        "                              \"git+https://github.com/openai/CLIP.git\"])\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9e45cb",
      "metadata": {
        "id": "cf9e45cb"
      },
      "source": [
        "## Dataset Functions\n",
        "\n",
        "We define utility functions for:\n",
        "- **`get_data()`**: Load Flowers102 from torchvision\n",
        "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
        "- **`split_data()`**: Filter images for base/novel in each split\n",
        "\n",
        "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9adfe795",
      "metadata": {
        "id": "9adfe795"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    \"\"\"Return base and novel class id lists using the actual labels present\n",
        "    in the dataset. Prefer public attributes (`targets` then `labels`) and\n",
        "    only fall back to the dataset private attribute `_labels` if neither is\n",
        "    available.\n",
        "    \"\"\"\n",
        "    labels = getattr(dataset, \"targets\", None)\n",
        "    if labels is None:\n",
        "        labels = getattr(dataset, \"labels\", None)\n",
        "\n",
        "    if labels is None and hasattr(dataset, \"_labels\"):\n",
        "        # FALLBACK: using private dataset internals. Flowers102 exposes\n",
        "        # `_labels` but this is a private attribute; prefer public attributes\n",
        "        # above so future datasets remain compatible.\n",
        "        labels = dataset._labels\n",
        "\n",
        "    if labels is None:\n",
        "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    num_classes = len(unique_labels)\n",
        "    mid = num_classes // 2\n",
        "    base_classes = unique_labels[:mid]\n",
        "    novel_classes = unique_labels[mid:]\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bda8d2",
      "metadata": {
        "id": "74bda8d2"
      },
      "source": [
        "## Class Names and Dataset Loading\n",
        "\n",
        "We load the names of 102 flower classes from Flowers102.\n",
        "\n",
        "This is **critical** for CLIP:\n",
        "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
        "- Each prompt is encoded by CLIP's text encoder\n",
        "- Image features are compared against these text templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f443bb94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f443bb94",
        "outputId": "b8c777b9-3e20-4a8f-a748-bece7ade5051"
      },
      "outputs": [],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "# Uncomment to see class names\n",
        "# print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "# print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e8a6d3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a6d3c9",
        "outputId": "0a979e91-25fe-4780-c396-883fa9772d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: ViT-B/16\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ViT-B/16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e05089",
      "metadata": {
        "id": "e0e05089"
      },
      "source": [
        "## Load Flowers102 and Split Base/Novel\n",
        "\n",
        "We load the 3 splits (train, val, test) and divide into base/novel.\n",
        "\n",
        "**Statistics:**\n",
        "- Train Base: 10 images × 51 classes = 510 images\n",
        "- Val Base: 10 images × 51 classes = 510 images\n",
        "- Test Base: ~10 images × 51 classes (from test split)\n",
        "- Test Novel: Remaining (~10 per class)\n",
        "\n",
        "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9fdd5516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdd5516",
        "outputId": "55367b6e-6008-4f7c-98fa-c9c153f988c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Base (few-shot): 510 samples (16 shots per class)\n",
            "Val Base: 510 samples\n",
            "Test Base: 2473 samples\n",
            "Test Novel: 3676 samples\n"
          ]
        }
      ],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# Few-shot: sample `shots_per_class` images per base class from the train split\n",
        "shots_per_class = 16\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Collect indices per class in the original train_set\n",
        "indices_per_class = {c: [] for c in base_classes}\n",
        "for idx, label in enumerate(train_set._labels):\n",
        "    if label in indices_per_class:\n",
        "        indices_per_class[label].append(idx)\n",
        "\n",
        "selected = []\n",
        "for c in base_classes:\n",
        "    inds = indices_per_class.get(c, [])\n",
        "    random.shuffle(inds)\n",
        "    # take up to shots_per_class (if fewer available, take all)\n",
        "    selected.extend(inds[:shots_per_class])\n",
        "\n",
        "# Create the few-shot training subset\n",
        "train_base = torch.utils.data.Subset(train_set, selected)\n",
        "\n",
        "# validation and test splits remain full (or filtered by base classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Train Base (few-shot): {len(train_base)} samples ({shots_per_class} shots per class)\")\n",
        "print(f\"Val Base: {len(val_base)} samples\")\n",
        "print(f\"Test Base: {len(test_base)} samples\")\n",
        "print(f\"Test Novel: {len(test_novel)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8071f907",
      "metadata": {
        "id": "8071f907"
      },
      "source": [
        "## Harmonic Mean (HM)\n",
        "\n",
        "Standard metric for few-shot adaptation papers.\n",
        "\n",
        "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
        "\n",
        "**Why HM instead of arithmetic mean?**\n",
        "- HM heavily penalizes outliers\n",
        "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
        "- Forces the model to balance both accuracies\n",
        "\n",
        "**Obiettivo:** massimizzare l'HM tra `base_acc_cocoop` e `novel_acc_cocoop`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3eec2ec0",
      "metadata": {
        "id": "3eec2ec0"
      },
      "outputs": [],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    # Guard against zero to avoid division-by-zero errors\n",
        "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
        "        return 0.0\n",
        "    numerator = 2.0\n",
        "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12012951",
      "metadata": {
        "id": "12012951"
      },
      "source": [
        "## Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f29fc20f",
      "metadata": {
        "id": "f29fc20f"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        x = x[torch.arange(int(x.shape[0])), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916487b1",
      "metadata": {
        "id": "916487b1"
      },
      "source": [
        "## CoCoOpPromptLearner: Dynamic Prompts\n",
        "\n",
        "\n",
        "**Components:**\n",
        "1. **V1...VM:** 16 context vectors (learned via SGD)\n",
        "   - Shape: (16, 512) tensors\n",
        "   - Initialized randomly from N(0, 0.02²)\n",
        "   - Optimized during training\n",
        "\n",
        "2. **π(x):** Conditional token (generated per image)\n",
        "   - Shape: (B, 512) from MetaNetwork output\n",
        "   - Different for each image\n",
        "\n",
        "3. **[CLASS]:** Class name embedding\n",
        "   - Shape: (seq_len, 512) from CLIP's token embedding\n",
        "   - Same for all images of the same class\n",
        "\n",
        "**Forward Pass:**\n",
        "- Input: image_features (B, 512)\n",
        "- Output: prompts (B, num_classes, seq_len_total, 512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "072ba719",
      "metadata": {
        "id": "072ba719"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        vis_dim = clip_model.visual.output_dim\n",
        "        \n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(torch.float32)\n",
        "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        \n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words: {n_ctx}\")\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "        \n",
        "        self.meta_net = nn.Sequential(OrderedDict([\n",
        "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
        "            (\"relu\", nn.ReLU(inplace=True)),\n",
        "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
        "        ]))\n",
        "        \n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
        "        tokenized_prompts = tokenized_prompts.to(device)  # FIX: Move to device\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(torch.float32)\n",
        "        \n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "    \n",
        "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "        if label is not None:\n",
        "            prefix = prefix[label]\n",
        "            suffix = suffix[label]\n",
        "        prompts = torch.cat([prefix, ctx, suffix], dim=1)\n",
        "        return prompts\n",
        "    \n",
        "    def forward(self, im_features):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx.unsqueeze(0)\n",
        "        bias = self.meta_net(im_features)\n",
        "        bias = bias.unsqueeze(1)\n",
        "        ctx_shifted = ctx + bias\n",
        "        prompts = []\n",
        "        for ctx_shifted_i in ctx_shifted:\n",
        "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)\n",
        "            prompts.append(pts_i)\n",
        "        prompts = torch.stack(prompts)\n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7b162a",
      "metadata": {
        "id": "6c7b162a"
      },
      "source": [
        "## CoCoOpTrainer: Training and Evaluation\n",
        "\n",
        "Class that manages:\n",
        "\n",
        "**1. Initialization:**\n",
        "- Create PromptLearner\n",
        "- Freeze CLIP (`requires_grad=False`)\n",
        "- Configure SGD optimizer for prompt learner only\n",
        "\n",
        "**2. train_epoch():**\n",
        "- Forward: Image encoder + PromptLearner + Text encoder\n",
        "- **Critical step:** Encode soft prompts through text transformer\n",
        "  - Add positional embeddings\n",
        "  - Pass through CLIP's transformer\n",
        "  - Extract first token\n",
        "  - Apply final layer norm + projection\n",
        "- Compute loss: Cross-entropy on base classes\n",
        "- Backward: Backprop only in PromptLearner\n",
        "- Return: Average loss of the epoch\n",
        "\n",
        "**3. eval():**\n",
        "- Same forward procedure as training\n",
        "- Without backward pass\n",
        "- Compute accuracy on any dataset (base or novel)\n",
        "\n",
        "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
        "because that method expects integer tokens, not embeddings.\n",
        "We manually forward through the text transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "959af3c5",
      "metadata": {
        "id": "959af3c5"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx=n_ctx, ctx_init=ctx_init, device=device)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "    \n",
        "    def forward(self, image, label=None): #now it's parallel -> more efficient\n",
        "        # encode images\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # generate instance-conditioned prompts: (batch, n_cls, n_tokens, dim)\n",
        "        prompts = self.prompt_learner(image_features)\n",
        "        batch_size = int(prompts.shape[0])\n",
        "        n_cls = int(self.prompt_learner.n_cls)\n",
        "\n",
        "        # Flatten prompts to (batch*n_cls, n_tokens, dim) for parallel encoding\n",
        "        n_tokens = prompts.shape[2]\n",
        "        dim = prompts.shape[3]\n",
        "        prompts_flat = prompts.reshape(batch_size * n_cls, n_tokens, dim).type(self.dtype)\n",
        "\n",
        "        # Repeat tokenized prompts for each image in the batch\n",
        "        tokenized = self.tokenized_prompts.to(prompts_flat.device)\n",
        "        tokenized_expanded = tokenized.repeat(batch_size, 1)\n",
        "\n",
        "        # Encode all prompts in parallel and reshape back: (batch, n_cls, dim)\n",
        "        text_features = self.text_encoder(prompts_flat, tokenized_expanded)\n",
        "        text_features = text_features.reshape(batch_size, n_cls, -1)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute logits: (batch, n_cls)\n",
        "        image_features_expanded = image_features.unsqueeze(1)\n",
        "        logits = logit_scale * (image_features_expanded @ text_features.transpose(1, 2)).squeeze(1)\n",
        "\n",
        "        if label is not None:\n",
        "            return F.cross_entropy(logits, label)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838deabc",
      "metadata": {
        "id": "838deabc"
      },
      "source": [
        "## Training CoCoOp\n",
        "\n",
        "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
        "\n",
        "**Hyperparameters:**\n",
        "- Learning rate: 0.002 (SGD)\n",
        "- Momentum: 0.9\n",
        "- Weight decay: 5e-4\n",
        "- Batch size: 1\n",
        "- Epochs: 10\n",
        "\n",
        "**What happens:**\n",
        "- Context vectors V1...VM adapt to the Flowers102 dataset\n",
        "- MetaNetwork learns to generate useful conditional tokens\n",
        "- CLIP remains frozen (unchanged)\n",
        "\n",
        "**Expected output:**\n",
        "- Initial loss: ~3.0\n",
        "- Final loss: ~1.3-1.5\n",
        "- Training time: ~5-10 minutes on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9d315494",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CoCoOpTrainer:\n",
        "    def __init__(self, clip_model, classnames, base_classes, novel_classes, \n",
        "                 device='cuda', lr=0.002, n_ctx=4, num_epochs=10):\n",
        "        \n",
        "        self.clip_model = clip_model.float()\n",
        "        self.classnames = classnames\n",
        "        self.base_classes = base_classes\n",
        "        self.device = device\n",
        "        self.num_epochs = num_epochs\n",
        "        self.contig_cat2idx = {cat: idx for idx, cat in enumerate(self.base_classes)}\n",
        "        \n",
        "        for p in self.clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        self.model = CustomCLIP(self.clip_model, classnames, n_ctx=n_ctx, device=device).to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.prompt_learner.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs)\n",
        "        \n",
        "        trainable = sum(p.numel() for p in self.model.prompt_learner.parameters())\n",
        "        print(f\"\\nCoCoOpTrainer initialized: {trainable:,} trainable params\\n\")\n",
        "    \n",
        "    def train_epoch(self, train_dataset, batch_size=4):\n",
        "        self.model.train()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False\n",
        "        )\n",
        "        \n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "            images = images.to(self.device).float()\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            labels_mapped = torch.tensor(\n",
        "                [self.contig_cat2idx[l.item()] for l in labels],\n",
        "                dtype=torch.long,\n",
        "                device=self.device\n",
        "            )\n",
        "            \n",
        "            loss = self.model(images, labels_mapped)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        self.scheduler.step()\n",
        "        return total_loss / max(1, n_batches)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def eval(self, dataset, categories, batch_size=64):\n",
        "        self.model.eval()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False\n",
        "        )\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(self.device).float()\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            logits = self.model(images)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            \n",
        "            labels_mapped = torch.tensor(\n",
        "                [contig_cat2idx[l.item()] for l in labels],\n",
        "                dtype=torch.long,\n",
        "                device=self.device\n",
        "            )\n",
        "            \n",
        "            correct += (pred == labels_mapped).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        return correct / total if total > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4324e6e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Base: 510 | Test Base: 2473 | Test Novel: 3676\n"
          ]
        }
      ],
      "source": [
        "#Loading data and model\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "shots_per_class = 16\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "indices_per_class = {c: [] for c in base_classes}\n",
        "for idx, label in enumerate(train_set._labels):\n",
        "    if label in indices_per_class:\n",
        "        indices_per_class[label].append(idx)\n",
        "\n",
        "selected = []\n",
        "for c in base_classes:\n",
        "    inds = indices_per_class.get(c, [])\n",
        "    random.shuffle(inds)\n",
        "    selected.extend(inds[:shots_per_class])\n",
        "\n",
        "train_base = torch.utils.data.Subset(train_set, selected)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Train Base: {len(train_base)} | Test Base: {len(test_base)} | Test Novel: {len(test_novel)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c2feda02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2feda02",
        "outputId": "d1e3de97-7eef-45f2-fe33-9293ee28910c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial context: \"X X X X\"\n",
            "Number of context words: 4\n",
            "\n",
            "CoCoOpTrainer initialized: 35,360 trainable params\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TRAINING CoCoOp\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:11<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Loss: 1.0291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Loss: 0.5585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Loss: 0.4377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Loss: 0.3555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:09<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Loss: 0.3048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Loss: 0.2562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:09<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Loss: 0.2340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Loss: 0.2191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Loss: 0.2101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 128/128 [02:10<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Loss: 0.2029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
        "novel_classnames = [CLASS_NAMES[i] for i in novel_classes]\n",
        "\n",
        "trainer = CoCoOpTrainer(\n",
        "    clip_model=model,\n",
        "    classnames=base_classnames,\n",
        "    base_classes=base_classes,\n",
        "    novel_classes=novel_classes,\n",
        "    device=device,\n",
        "    lr=0.002,\n",
        "    n_ctx=4,\n",
        "    num_epochs=10\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING CoCoOp\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(10):\n",
        "    avg_loss = trainer.train_epoch(train_base, batch_size=4)\n",
        "    print(f\"Epoch {epoch+1}/10 - Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69c077f",
      "metadata": {
        "id": "b69c077f"
      },
      "source": [
        "## Final Evaluation (CoCoOp only)\n",
        "\n",
        "We'll evaluate the model with:\n",
        "1. Test Base\n",
        "2. Test Novel\n",
        "\n",
        "Computing Harmonic Mean between them to evaluate the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6d054b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "49984fae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49984fae",
        "outputId": "093533f6-9dfa-4ea9-e134-ca0072e250a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 39/39 [05:02<00:00,  7.75s/it]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "base_acc = trainer.eval(test_base, base_classes, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a44f90bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swapping class definitions to 51 novel classes (In-Place)...\n",
            "Class definitions swapped. Starting evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Novel: 100%|██████████| 230/230 [07:17<00:00,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Corrected Novel Accuracy: 73.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Evaluation for novel classes with in-place class swapping\n",
        "@torch.no_grad()\n",
        "def evaluate_novel_inplace(trainer, test_dataset, novel_classnames, novel_classes_ids, device='cuda'):\n",
        "    print(f\"Swapping class definitions to {len(novel_classnames)} novel classes (In-Place)...\")\n",
        "    \n",
        "    model = trainer.model\n",
        "    prompt_learner = model.prompt_learner\n",
        "    \n",
        "    # 1. SALVIAMO LO STATO ORIGINALE (Base Classes)\n",
        "    # Per poterlo ripristinare se servisse (opzionale, ma buona pratica)\n",
        "    old_n_cls = prompt_learner.n_cls\n",
        "    old_token_prefix = prompt_learner.token_prefix\n",
        "    old_token_suffix = prompt_learner.token_suffix\n",
        "    old_tokenized_prompts = model.tokenized_prompts\n",
        "    \n",
        "    # 2. GENERIAMO I NUOVI EMBEDDING DI TESTO (Novel Classes)\n",
        "    # Tokenizziamo i nuovi nomi\n",
        "    # Nota: la logica di replace(\"_\", \" \") è già nei nomi passati o la facciamo qui\n",
        "    clean_names = [name.replace(\"_\", \" \") for name in novel_classnames]\n",
        "    prompts = [prompt_learner.ctx_init + \" \" + name + \".\" if hasattr(prompt_learner, 'ctx_init') and prompt_learner.ctx_init else \"X \" * prompt_learner.n_ctx + name + \".\" for name in clean_names]\n",
        "    \n",
        "    # Ricostruiamo il prompt template standard usato in PromptLearner\n",
        "    # PromptLearner usa: \"X X X X classname.\"\n",
        "    # Dobbiamo replicare la logica esatta per ottenere prefix e suffix corretti\n",
        "    # Recuperiamo il prefisso \"X X X X\" (dummy) usato per l'inizializzazione\n",
        "    n_ctx = prompt_learner.n_ctx\n",
        "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
        "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
        "    \n",
        "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "    \n",
        "    # Otteniamo gli embedding dal Text Encoder di CLIP\n",
        "    with torch.no_grad():\n",
        "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
        "    \n",
        "    # 3. SOVRASCRIVIAMO I BUFFER DEL MODELLO\n",
        "    # PromptLearner ha bisogno di prefix e suffix per \"incastrare\" i vettori appresi nel mezzo\n",
        "    new_token_prefix = embedding[:, :1, :]           # [n_cls, 1, dim]\n",
        "    new_token_suffix = embedding[:, 1+n_ctx:, :]     # [n_cls, len-1-n_ctx, dim]\n",
        "    \n",
        "    prompt_learner.register_buffer(\"token_prefix\", new_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", new_token_suffix)\n",
        "    prompt_learner.n_cls = len(novel_classnames)\n",
        "    prompt_learner.tokenized_prompts = new_tokenized\n",
        "    model.tokenized_prompts = new_tokenized # Aggiorniamo anche il riferimento nel modello padre\n",
        "    \n",
        "    print(\"Class definitions swapped. Starting evaluation...\")\n",
        "    \n",
        "    # 4. VALUTAZIONE\n",
        "    model.eval()\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=16, shuffle=False, num_workers=0 # Batch basso per sicurezza\n",
        "    )\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Mappa: ID Originale (51) -> Indice Locale (0)\n",
        "    target_map = {original_id: idx for idx, original_id in enumerate(novel_classes_ids)}\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Evaluating Novel\"):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        logits = model(images) # Ora genera logits per le classi Novel!\n",
        "        pred = logits.argmax(dim=1)\n",
        "        \n",
        "        # Mapping labels\n",
        "        labels_cpu = labels.tolist()\n",
        "        try:\n",
        "            mapped_labels = torch.tensor([target_map[l] for l in labels_cpu], device=device)\n",
        "            correct += (pred == mapped_labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        except KeyError:\n",
        "            continue\n",
        "            \n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    \n",
        "    # 5. RIPRISTINO (Opzionale, se vuoi riusare il trainer per le base classes dopo)\n",
        "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
        "    prompt_learner.n_cls = old_n_cls\n",
        "    prompt_learner.tokenized_prompts = old_tokenized_prompts\n",
        "    model.tokenized_prompts = old_tokenized_prompts\n",
        "    \n",
        "    return acc\n",
        "\n",
        "# === ESECUZIONE ===\n",
        "# Assicuriamoci di liberare memoria spazzatura prima di iniziare\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "novel_acc = evaluate_novel_inplace(\n",
        "    trainer, \n",
        "    test_novel, \n",
        "    novel_classnames, # Lista nomi ['rose', 'tulip'...]\n",
        "    novel_classes,    # Lista ID [51, 52...] definita nel tuo notebook\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nCorrected Novel Accuracy: {novel_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c3e9db2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "  Base Accuracy:   92.03%\n",
            "  Novel Accuracy:  73.50%\n",
            "  Harmonic Mean:   81.73%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "hm = harmonic_mean(base_acc, novel_acc)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
        "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
        "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a4ed1e",
      "metadata": {},
      "source": [
        "## Test applicazione reale -> tutte le classi insieme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b3a3422b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on ALL 102 classes simultaneously (Generalized Setting)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval Generalized: 100%|██████████| 193/193 [23:14<00:00,  7.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generalized Accuracy (Base + Novel mixed): 72.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_generalized(trainer, test_dataset, all_classnames, all_class_ids, device='cuda'):\n",
        "    print(f\"Evaluating on ALL {len(all_classnames)} classes simultaneously (Generalized Setting)...\")\n",
        "    \n",
        "    # 1. Setup del modello con TUTTE le classi (Base + Novel)\n",
        "    model = trainer.model\n",
        "    prompt_learner = model.prompt_learner\n",
        "    \n",
        "    # Salviamo stato vecchio\n",
        "    old_n_cls = prompt_learner.n_cls\n",
        "    old_token_prefix = prompt_learner.token_prefix\n",
        "    old_token_suffix = prompt_learner.token_suffix\n",
        "    old_tokenized = model.tokenized_prompts\n",
        "\n",
        "    # 2. Creiamo i prompt per TUTTE le classi (0..101)\n",
        "    clean_names = [name.replace(\"_\", \" \") for name in all_classnames]\n",
        "    \n",
        "    # Ricostruiamo i prompt dummy\n",
        "    n_ctx = prompt_learner.n_ctx\n",
        "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
        "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
        "    \n",
        "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
        "    \n",
        "    # Aggiorniamo i buffer\n",
        "    prompt_learner.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "    prompt_learner.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
        "    prompt_learner.n_cls = len(all_classnames)\n",
        "    prompt_learner.tokenized_prompts = new_tokenized\n",
        "    model.tokenized_prompts = new_tokenized\n",
        "\n",
        "    # 3. Valutazione\n",
        "    model.eval()\n",
        "    # Usiamo il dataset di test COMPLETO (Base + Novel) se possibile, o un subset\n",
        "    dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Mappa: ID Originale (0..101) -> Indice nel modello (0..101)\n",
        "    # Essendo \"tutte\" le classi ordinate, la mappa è solitamente identica (0->0, 101->101)\n",
        "    # Ma per sicurezza usiamo gli ID passati\n",
        "    target_map = {original_id: idx for idx, original_id in enumerate(all_class_ids)}\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Eval Generalized\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        logits = model(images) # Output shape: [Batch, 102]\n",
        "        pred = logits.argmax(dim=1)\n",
        "        \n",
        "        # Le label qui arrivano come ID originali (es. 0, 55, 101)\n",
        "        # Dobbiamo assicurarci che corrispondano agli indici del modello\n",
        "        mapped_labels = torch.tensor([target_map[l.item()] for l in labels], device=device)\n",
        "        \n",
        "        correct += (pred == mapped_labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "    # Ripristino\n",
        "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
        "    prompt_learner.n_cls = old_n_cls\n",
        "    prompt_learner.tokenized_prompts = old_tokenized\n",
        "    model.tokenized_prompts = old_tokenized\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "# === UTILIZZO ===\n",
        "# Uniamo le liste\n",
        "all_names = base_classnames + novel_classnames\n",
        "all_ids = list(base_classes) + list(novel_classes)\n",
        "\n",
        "# Uniamo i dataset di test (Base + Novel) per fare un test unico \"Reale\"\n",
        "full_test_set = torch.utils.data.ConcatDataset([test_base, test_novel])\n",
        "\n",
        "acc_generalized = evaluate_generalized(trainer, full_test_set, all_names, all_ids)\n",
        "print(f\"Generalized Accuracy (Base + Novel mixed): {acc_generalized*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
