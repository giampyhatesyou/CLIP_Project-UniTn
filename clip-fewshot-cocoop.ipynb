{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff453b1a",
      "metadata": {
        "id": "ff453b1a"
      },
      "source": [
        "# CLIP with Flowers!?!?!??!?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73175206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73175206",
        "outputId": "fb56fbf4-f574-42f8-f09f-c0e12dbf36cc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "df3e678e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3e678e",
        "outputId": "4a82e2f7-20d3-47b9-b8c6-5eddbe9efb82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing CLIP...\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2e17y8e2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2e17y8e2\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=9dc450b31ab75fc0c1919302ec222db5fcbd3d524c0bde1c5adae21787e7ae25\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k2woroam/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n",
            "Device: cuda\n",
            "PyTorch: 2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import clip\n",
        "    print(\"✓ CLIP already installed\")\n",
        "except Exception:\n",
        "    print(\"Installing CLIP...\")\n",
        "    import subprocess, importlib\n",
        "    try:\n",
        "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
        "    except:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
        "                              \"git+https://github.com/openai/CLIP.git\"])\n",
        "    importlib.invalidate_caches()\n",
        "    import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9e45cb",
      "metadata": {
        "id": "cf9e45cb"
      },
      "source": [
        "## Dataset Functions\n",
        "\n",
        "We define utility functions for:\n",
        "- **`get_data()`**: Load Flowers102 from torchvision\n",
        "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
        "- **`split_data()`**: Filter images for base/novel in each split\n",
        "\n",
        "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adfe795",
      "metadata": {
        "id": "9adfe795"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    \"\"\"Return base and novel class id lists using the actual labels present\n",
        "    in the dataset. Prefer public attributes (`targets` then `labels`) and\n",
        "    only fall back to the dataset private attribute `_labels` if neither is\n",
        "    available.\n",
        "    \"\"\"\n",
        "    labels = getattr(dataset, \"targets\", None)\n",
        "    if labels is None:\n",
        "        labels = getattr(dataset, \"labels\", None)\n",
        "\n",
        "    if labels is None and hasattr(dataset, \"_labels\"):\n",
        "        labels = dataset._labels\n",
        "\n",
        "    if labels is None:\n",
        "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    num_classes = len(unique_labels)\n",
        "    mid = num_classes // 2\n",
        "    base_classes = unique_labels[:mid]\n",
        "    novel_classes = unique_labels[mid:]\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bda8d2",
      "metadata": {
        "id": "74bda8d2"
      },
      "source": [
        "## Class Names and Dataset Loading\n",
        "\n",
        "We load the names of 102 flower classes from Flowers102.\n",
        "\n",
        "This is **critical** for CLIP:\n",
        "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
        "- Each prompt is encoded by CLIP's text encoder\n",
        "- Image features are compared against these text templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f443bb94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f443bb94",
        "outputId": "b8c777b9-3e20-4a8f-a748-bece7ade5051"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 345M/345M [00:13<00:00, 26.0MB/s] \n",
            "100%|██████████| 502/502 [00:00<00:00, 1.96MB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 30.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "# Uncomment to see class names\n",
        "# print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "# print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a6d3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a6d3c9",
        "outputId": "0a979e91-25fe-4780-c396-883fa9772d2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:06<00:00, 54.2MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Model: ViT-B/16\n",
            "Augmentation pipeline defined.\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# --- Data transformation for Augmentation ---\n",
        "#keeping CLIP normalization values\n",
        "aug_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
        "    torchvision.transforms.RandomCrop(224),           # Random Crop\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5), \n",
        "    torchvision.transforms.RandomRotation(15),        # smooth rotation\n",
        "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Color Jitter\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.481, 0.457, 0.408), (0.268, 0.261, 0.275)) # Mean/Std of CLIP\n",
        "])\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model: ViT-B/16\")\n",
        "print(\"Augmentation pipeline defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e05089",
      "metadata": {
        "id": "e0e05089"
      },
      "source": [
        "## Load Flowers102 and Split Base/Novel\n",
        "\n",
        "We load the 3 splits (train, val, test) and divide into base/novel.\n",
        "\n",
        "**Statistics:**\n",
        "- Train Base: 10 images × 51 classes = 510 images\n",
        "- Val Base: 10 images × 51 classes = 510 images\n",
        "- Test Base: ~10 images × 51 classes (from test split)\n",
        "- Test Novel: Remaining (~10 per class)\n",
        "\n",
        "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdd5516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdd5516",
        "outputId": "55367b6e-6008-4f7c-98fa-c9c153f988c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset Creato con Successo!\n",
            "Base Classes: 51 | Shots: 16\n",
            "Dimensioni Subset Normale: 816\n",
            "Dimensioni Subset Augmentato: 816\n",
            "-> TOTALE Train Base: 1632 samples (Dovrebbe essere 1632)\n"
          ]
        }
      ],
      "source": [
        "# --- STEP 1: Raw Data Retrieval ---\n",
        "# Download Train and Val separately but without fixed transformations for now\n",
        "# (we apply them later via wrapper or by reloading)\n",
        "# Note: Flowers102 has 10 imgs in train and 10 in val. We use both to reach 16 shots.\n",
        "#SHALL WE ONLY USE THE 10 IMAGES IN TRAIN SET RATHER THAN BOTH? AND THEN AUGMENT TO GET 16?\n",
        "\n",
        "# Helper to load with a specific transformation\n",
        "def load_split(split, transform):\n",
        "    return torchvision.datasets.Flowers102(root=\"./data\", split=split, download=True, transform=transform)\n",
        "\n",
        "# Load \"Normal\" sets (Standard CLIP preprocess)\n",
        "train_set_norm = load_split(\"train\", preprocess)\n",
        "val_set_norm = load_split(\"val\", preprocess)\n",
        "test_set = load_split(\"test\", preprocess)\n",
        "\n",
        "# Load \"Augmented\" sets (Your custom transformation)\n",
        "train_set_aug = load_split(\"train\", aug_transform)\n",
        "val_set_aug = load_split(\"val\", aug_transform)\n",
        "\n",
        "# Create a unified \"Pool\" to select indices (we use the normal version to read labels)\n",
        "# ConcatDataset does not expose ._labels, so we work on logical indices.\n",
        "# Indices 0-1019 are train, 1020-2039 are val.\n",
        "full_labels = train_set_norm._labels + val_set_norm._labels \n",
        "# (Warning: check that _labels is a list, in Flowers102 it is)\n",
        "\n",
        "# --- STEP 2: Base/Novel Class Split ---\n",
        "# We use only the original train set to define classes, as per standard protocol\n",
        "base_classes, novel_classes = base_novel_categories(train_set_norm)\n",
        "\n",
        "# --- STEP 3: Few-Shot Selection (16 Real Shots) ---\n",
        "shots_per_class = 16\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Collect all available indices for each class in the unified pool (Train + Val)\n",
        "indices_per_class = {c: [] for c in base_classes}\n",
        "for global_idx, label in enumerate(full_labels):\n",
        "    if label in base_classes:\n",
        "        indices_per_class[label].append(global_idx)\n",
        "\n",
        "selected_indices = []\n",
        "for c in base_classes:\n",
        "    inds = indices_per_class.get(c, [])\n",
        "    random.shuffle(inds)\n",
        "    # We now have 20 images available (10 train + 10 val). We take 16.\n",
        "    selected_indices.extend(inds[:shots_per_class])\n",
        "\n",
        "# --- STEP 4: Creation of Hybrid Dataset (Normal + Augmented) ---\n",
        "# We need to fetch images from the correct datasets based on the global index\n",
        "\n",
        "# Helper function to map global index -> (Dataset, local index)\n",
        "def get_sample_from_pool(global_idx, is_augmented=False):\n",
        "    # If global_idx < 1020 we are in 'train', otherwise in 'val'\n",
        "    threshold = len(train_set_norm) # 1020\n",
        "    \n",
        "    if global_idx < threshold:\n",
        "        ds = train_set_aug if is_augmented else train_set_norm\n",
        "        local_idx = global_idx\n",
        "    else:\n",
        "        ds = val_set_aug if is_augmented else val_set_norm\n",
        "        local_idx = global_idx - threshold\n",
        "        \n",
        "    return ds, local_idx\n",
        "\n",
        "# Manually build the two Subsets\n",
        "# Unfortunately, torch.utils.data.Subset wants a single parent dataset. \n",
        "# Here we need a trick: Merge the datasets FIRST and then subset.\n",
        "\n",
        "# Merge physical datasets\n",
        "combined_norm = torch.utils.data.ConcatDataset([train_set_norm, val_set_norm])\n",
        "combined_aug = torch.utils.data.ConcatDataset([train_set_aug, val_set_aug])\n",
        "\n",
        "# Create subsets using selected indices (16 per class)\n",
        "subset_normal = torch.utils.data.Subset(combined_norm, selected_indices)\n",
        "subset_augmented = torch.utils.data.Subset(combined_aug, selected_indices)\n",
        "\n",
        "# --- FINAL RESULT: Concatenate Normal + Augmented ---\n",
        "train_base = torch.utils.data.ConcatDataset([subset_normal, subset_augmented])\n",
        "\n",
        "# Validation and Test remain standard (only Base classes filtered from original Test set)\n",
        "# Note: val_base here is slightly redundant if we used val for training, \n",
        "# but for consistency with original code we leave it from test split or use what remains.\n",
        "# For simplicity, we use part of the test set as validation or ignore val_base.\n",
        "# Maintain original logic:\n",
        "val_base, _ = split_data(test_set, base_classes) # Use test set as val for quick monitoring\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Dataset Created Successfully!\")\n",
        "print(f\"Base Classes: {len(base_classes)} | Shots: {shots_per_class}\")\n",
        "print(f\"Normal Subset Size: {len(subset_normal)}\")\n",
        "print(f\"Augmented Subset Size: {len(subset_augmented)}\")\n",
        "print(f\"-> TOTAL Train Base: {len(train_base)} samples (Should be {len(base_classes)*shots_per_class*2})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8071f907",
      "metadata": {
        "id": "8071f907"
      },
      "source": [
        "## Harmonic Mean (HM)\n",
        "\n",
        "Standard metric for few-shot adaptation papers.\n",
        "\n",
        "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
        "\n",
        "**Why HM instead of arithmetic mean?**\n",
        "- HM heavily penalizes outliers\n",
        "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
        "- Forces the model to balance both accuracies\n",
        "\n",
        "**Obiettivo:** massimizzare l'HM tra `base_acc_cocoop` e `novel_acc_cocoop`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3eec2ec0",
      "metadata": {
        "id": "3eec2ec0"
      },
      "outputs": [],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    # Guard against zero to avoid division-by-zero errors\n",
        "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
        "        return 0.0\n",
        "    numerator = 2.0\n",
        "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12012951",
      "metadata": {
        "id": "12012951"
      },
      "source": [
        "## Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f29fc20f",
      "metadata": {
        "id": "f29fc20f"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        x = x[torch.arange(int(x.shape[0])), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916487b1",
      "metadata": {
        "id": "916487b1"
      },
      "source": [
        "## CoCoOpPromptLearner: Dynamic Prompts (Optimized)\n",
        "\n",
        "**Components:**\n",
        "1. **Context Vectors (V):** 16 vectors (learnable).\n",
        "   - Shape: `(16, 512)`\n",
        "   - Initialized: Gaussian noise N(0, 0.02)\n",
        "   - Function: Provide the base context for the prompt.\n",
        "\n",
        "2. **Meta-Network (Bias Generator):**\n",
        "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
        "   - Input: Image Features `(Batch, 512)`\n",
        "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
        "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
        "\n",
        "3. **Class Embeddings:**\n",
        "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
        "   - Fixed during training.\n",
        "\n",
        "**Forward Pass (Vectorized):**\n",
        "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
        "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
        "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
        "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072ba719",
      "metadata": {
        "id": "072ba719"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        # Get embedding dimension from CLIP's final layer\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        vis_dim = clip_model.visual.output_dim\n",
        "        \n",
        "        # 1. Context Vectors Initialization\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            \n",
        "            # --- FIX: Move tokenized prompt to correct device ---\n",
        "            prompt = clip.tokenize(ctx_init).to(device) \n",
        "            \n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(torch.float32)\n",
        "            \n",
        "            # embedding[0] because tokenize adds a batch dim\n",
        "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            # Random initialization standard (Sigma=0.02)\n",
        "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        \n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words: {n_ctx}\")\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "        \n",
        "        # 2. Meta-Network (Less Aggressive Bottleneck for better Generalization)\n",
        "        # Trying to increase hidden dim from 32 (//16) to 128 (//4) to prevent underfitting novel classes\n",
        "        hidden_dim = vis_dim // 4\n",
        "        self.meta_net = nn.Sequential(OrderedDict([\n",
        "            (\"linear1\", nn.Linear(vis_dim, hidden_dim)),\n",
        "            (\"relu\", nn.ReLU(inplace=True)),\n",
        "            (\"linear2\", nn.Linear(hidden_dim, ctx_dim))\n",
        "        ]))\n",
        "        \n",
        "        # 3. Pre-computing Class Names (Prefix/Suffix)\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        \n",
        "        # Tokenize and get embeddings\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
        "        tokenized_prompts = tokenized_prompts.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(torch.float32)\n",
        "        \n",
        "        # Save Prefix (SOS) and Suffix (Class Name + EOS) as fixed buffers\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])      # (n_cls, 1, dim)\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :]) # (n_cls, len, dim)\n",
        "        \n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "    \n",
        "    def forward(self, im_features):\n",
        "        batch_size = im_features.shape[0]\n",
        "        \n",
        "        # 1. Calculate bias from image\n",
        "        bias = self.meta_net(im_features)\n",
        "        bias = bias.unsqueeze(1) # (Batch, 1, 512)\n",
        "        \n",
        "        # 2. Generate shifted context\n",
        "        ctx = self.ctx.unsqueeze(0) # (1, n_ctx, dim)\n",
        "        ctx_shifted = ctx + bias    # (Batch, n_ctx, dim)\n",
        "        \n",
        "        # 3. Parallel prompt construction\n",
        "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
        "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)\n",
        "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
        "        \n",
        "        prompts = torch.cat(\n",
        "            [prefix, ctx_expanded, suffix],\n",
        "            dim=2\n",
        "        )\n",
        "        \n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7b162a",
      "metadata": {
        "id": "6c7b162a"
      },
      "source": [
        "## CoCoOpTrainer: Training and Evaluation\n",
        "\n",
        "Class that manages:\n",
        "\n",
        "**1. Initialization:**\n",
        "- Create PromptLearner\n",
        "- Freeze CLIP (`requires_grad=False`)\n",
        "- Configure SGD optimizer for prompt learner only\n",
        "\n",
        "**2. train_epoch():**\n",
        "- Forward: Image encoder + PromptLearner + Text encoder\n",
        "- **Critical step:** Encode soft prompts through text transformer\n",
        "  - Add positional embeddings\n",
        "  - Pass through CLIP's transformer\n",
        "  - Extract first token\n",
        "  - Apply final layer norm + projection\n",
        "- Compute loss: Cross-entropy on base classes\n",
        "- Backward: Backprop only in PromptLearner\n",
        "- Return: Average loss of the epoch\n",
        "\n",
        "**3. eval():**\n",
        "- Same forward procedure as training\n",
        "- Without backward pass\n",
        "- Compute accuracy on any dataset (base or novel)\n",
        "\n",
        "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
        "because that method expects integer tokens, not embeddings.\n",
        "We manually forward through the text transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "959af3c5",
      "metadata": {
        "id": "959af3c5"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx=n_ctx, ctx_init=ctx_init, device=device)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "    \n",
        "    def forward(self, image, label=None): #now it's parallel -> more efficient\n",
        "        # encode images\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # generate instance-conditioned prompts: (batch, n_cls, n_tokens, dim)\n",
        "        prompts = self.prompt_learner(image_features)\n",
        "        batch_size = int(prompts.shape[0])\n",
        "        n_cls = int(self.prompt_learner.n_cls)\n",
        "\n",
        "        # Flatten prompts to (batch*n_cls, n_tokens, dim) for parallel encoding\n",
        "        n_tokens = prompts.shape[2]\n",
        "        dim = prompts.shape[3]\n",
        "        prompts_flat = prompts.reshape(batch_size * n_cls, n_tokens, dim).type(self.dtype)\n",
        "\n",
        "        # Repeat tokenized prompts for each image in the batch\n",
        "        tokenized = self.tokenized_prompts.to(prompts_flat.device)\n",
        "        tokenized_expanded = tokenized.repeat(batch_size, 1)\n",
        "\n",
        "        # Encode all prompts in parallel and reshape back: (batch, n_cls, dim)\n",
        "        text_features = self.text_encoder(prompts_flat, tokenized_expanded)\n",
        "        text_features = text_features.reshape(batch_size, n_cls, -1)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute logits: (batch, n_cls)\n",
        "        image_features_expanded = image_features.unsqueeze(1)\n",
        "        logits = logit_scale * (image_features_expanded @ text_features.transpose(1, 2)).squeeze(1)\n",
        "\n",
        "        if label is not None:\n",
        "            return F.cross_entropy(logits, label)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838deabc",
      "metadata": {
        "id": "838deabc"
      },
      "source": [
        "## Training CoCoOp (Optimized)\n",
        "\n",
        "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
        "\n",
        "**Hyperparameters (Optimized):**\n",
        "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
        "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
        "- **Learning rate:** 0.002 (SGD)\n",
        "- **Momentum:** 0.9\n",
        "- **Weight decay:** 5e-4\n",
        "- **Epochs:** 10\n",
        "\n",
        "**What happens:**\n",
        "- The `PromptLearner` adapts its 16 context vectors to the Flowers102 dataset.\n",
        "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
        "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
        "\n",
        "**Expected output:**\n",
        "- Initial loss: ~2.5 - 3.5\n",
        "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
        "- Training time: ~2-4 minutes on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d315494",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CoCoOpTrainer:\n",
        "    def __init__(self, clip_model, classnames, base_classes, novel_classes, \n",
        "                 device='cuda', lr=0.002, n_ctx=16, num_epochs=10, ctx_init=None): # <--- NEW PARAMETER\n",
        "        \n",
        "        self.clip_model = clip_model.float()\n",
        "        self.classnames = classnames\n",
        "        self.base_classes = base_classes\n",
        "        self.device = device\n",
        "        self.num_epochs = num_epochs\n",
        "        \n",
        "        # --- LABEL MAPPING OPTIMIZATION (GPU Lookup) ---\n",
        "        # We need to map original dataset labels (e.g., 0, 55, 101) to local indices (0..N)\n",
        "        max_label_id = max(max(base_classes), max(novel_classes)) + 1\n",
        "        self.label_map = torch.full((max_label_id,), -1, dtype=torch.long, device=device)\n",
        "        \n",
        "        base_ids_tensor = torch.tensor(base_classes, device=device, dtype=torch.long)\n",
        "        target_indices = torch.arange(len(base_classes), device=device, dtype=torch.long)\n",
        "        self.label_map[base_ids_tensor] = target_indices\n",
        "        \n",
        "        # Freeze CLIP\n",
        "        for p in self.clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Initialize Custom Model (Passing ctx_init)\n",
        "        print(f\"Initializing CustomCLIP with ctx_init='{ctx_init}'...\")\n",
        "        self.model = CustomCLIP(self.clip_model, classnames, n_ctx=n_ctx, ctx_init=ctx_init, device=device).to(device)\n",
        "        \n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.model.prompt_learner.parameters(), \n",
        "            lr=lr, \n",
        "            momentum=0.9, \n",
        "            weight_decay=5e-4\n",
        "        )\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs)\n",
        "        \n",
        "        trainable = sum(p.numel() for p in self.model.prompt_learner.parameters())\n",
        "        print(f\"\\nCoCoOpTrainer initialized: {trainable:,} trainable params\")\n",
        "        print(f\"Context: {n_ctx} | Gradient Accumulation Enabled | Init: {ctx_init if ctx_init else 'Random'}\")\n",
        "    \n",
        "    def train_epoch(self, train_dataset, batch_size=4, accumulation_steps=4):\n",
        "        \"\"\"\n",
        "        Runs a training epoch with Gradient Accumulation.\n",
        "        -> Effective Batch Size = batch_size * accumulation_steps (e.g., 4 * 4 = 16)\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False\n",
        "        )\n",
        "        \n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training (Grad Accum)\")):\n",
        "            images = images.to(self.device).float()\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            # Fast mapping using pre-computed GPU tensor\n",
        "            labels_mapped = self.label_map[labels]\n",
        "            \n",
        "            # Forward pass\n",
        "            loss = self.model(images, labels_mapped)\n",
        "            \n",
        "            # --- GRADIENT ACCUMULATION LOGIC ---\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "            \n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "            n_batches += 1\n",
        "        \n",
        "        # Process remaining gradients if dataloader size is not divisible by accumulation_steps\n",
        "        if len(dataloader) % accumulation_steps != 0:\n",
        "             self.optimizer.step()\n",
        "             self.optimizer.zero_grad()\n",
        "        \n",
        "        self.scheduler.step()\n",
        "        return total_loss / max(1, n_batches)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def eval(self, dataset, categories, batch_size=64):\n",
        "        self.model.eval()\n",
        "        self.clip_model.eval()\n",
        "        \n",
        "        local_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        "        )\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(self.device).float()\n",
        "            logits = self.model(images)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            \n",
        "            labels_cpu = labels.tolist()\n",
        "            try:\n",
        "                mapped_targets = torch.tensor(\n",
        "                    [local_cat2idx[l] for l in labels_cpu], \n",
        "                    device=self.device\n",
        "                )\n",
        "                correct += (pred == mapped_targets).sum().item()\n",
        "                total += labels.size(0)\n",
        "            except KeyError:\n",
        "                continue\n",
        "        \n",
        "        return correct / total if total > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4324e6e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Base: 510 | Test Base: 2473 | Test Novel: 3676\n"
          ]
        }
      ],
      "source": [
        "#Loading data and model\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "shots_per_class = 10\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "indices_per_class = {c: [] for c in base_classes}\n",
        "for idx, label in enumerate(train_set._labels):\n",
        "    if label in indices_per_class:\n",
        "        indices_per_class[label].append(idx)\n",
        "\n",
        "selected = []\n",
        "for c in base_classes:\n",
        "    inds = indices_per_class.get(c, [])\n",
        "    random.shuffle(inds)\n",
        "    selected.extend(inds[:shots_per_class])\n",
        "\n",
        "train_base = torch.utils.data.Subset(train_set, selected)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "print(f\"Train Base: {len(train_base)} | Test Base: {len(test_base)} | Test Novel: {len(test_novel)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2feda02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2feda02",
        "outputId": "d1e3de97-7eef-45f2-fe33-9293ee28910c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing CustomCLIP with ctx_init='a photo of a flower'...\n",
            "Initial context: \"a photo of a flower\"\n",
            "Number of context words: 5\n",
            "\n",
            "CoCoOpTrainer initialized: 35,872 trainable params\n",
            "Context: 16 | Gradient Accumulation Enabled | Init: a photo of a flower\n",
            "\n",
            "======================================================================\n",
            "TRAINING CoCoOp (Smart Init + Checkpointing)\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 - Loss: 1.6401 | Val Acc: 70.78%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/50 - Loss: 1.1262 | Val Acc: 71.96%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/50 - Loss: 1.0363 | Val Acc: 74.31%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/50 - Loss: 0.9550 | Val Acc: 76.08%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50 - Loss: 0.8779 | Val Acc: 77.84%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/50 - Loss: 0.7951 | Val Acc: 77.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/50 - Loss: 0.7018 | Val Acc: 80.59%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/50 - Loss: 0.6113 | Val Acc: 82.55%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50 - Loss: 0.5548 | Val Acc: 81.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 - Loss: 0.5200 | Val Acc: 82.75%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 - Loss: 0.4643 | Val Acc: 84.31%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50 - Loss: 0.4294 | Val Acc: 85.29%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50 - Loss: 0.4190 | Val Acc: 87.65%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/50 - Loss: 0.3721 | Val Acc: 87.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50 - Loss: 0.3482 | Val Acc: 88.24%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50 - Loss: 0.3379 | Val Acc: 87.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50 - Loss: 0.3333 | Val Acc: 90.20%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/50 - Loss: 0.3070 | Val Acc: 90.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50 - Loss: 0.2919 | Val Acc: 90.78%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50 - Loss: 0.2822 | Val Acc: 90.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/50 - Loss: 0.2843 | Val Acc: 91.18%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50 - Loss: 0.2496 | Val Acc: 90.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50 - Loss: 0.2503 | Val Acc: 90.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/50 - Loss: 0.2340 | Val Acc: 91.76%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50 - Loss: 0.2228 | Val Acc: 91.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/50 - Loss: 0.2202 | Val Acc: 92.94%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50 - Loss: 0.2114 | Val Acc: 92.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50 - Loss: 0.1925 | Val Acc: 93.14%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/50 - Loss: 0.1931 | Val Acc: 93.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50 - Loss: 0.1820 | Val Acc: 92.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/50 - Loss: 0.1779 | Val Acc: 93.33%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/50 - Loss: 0.1730 | Val Acc: 93.53%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/50 - Loss: 0.1679 | Val Acc: 93.92%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/50 - Loss: 0.1594 | Val Acc: 93.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/50 - Loss: 0.1620 | Val Acc: 93.53%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/50 - Loss: 0.1527 | Val Acc: 93.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/50 - Loss: 0.1448 | Val Acc: 93.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/50 - Loss: 0.1456 | Val Acc: 93.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/50 - Loss: 0.1437 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/50 - Loss: 0.1379 | Val Acc: 94.12%  [★ BEST SAVED]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/50 - Loss: 0.1374 | Val Acc: 93.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/50 - Loss: 0.1351 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/50 - Loss: 0.1325 | Val Acc: 94.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/50 - Loss: 0.1313 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/50 - Loss: 0.1302 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/50 - Loss: 0.1294 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/50 - Loss: 0.1292 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:03<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/50 - Loss: 0.1287 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/50 - Loss: 0.1281 | Val Acc: 93.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (Grad Accum): 100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
            "Evaluating: 100%|██████████| 8/8 [01:02<00:00,  7.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/50 - Loss: 0.1280 | Val Acc: 93.92%\n",
            "======================================================================\n",
            "Training completato. Best Val Acc: 94.12% at epoch 40\n",
            "\n",
            "Reloading best model weights for final evaluation...\n",
            "✅ Best model loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Configurazione\n",
        "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
        "novel_classnames = [CLASS_NAMES[i] for i in novel_classes]\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "trainer = CoCoOpTrainer(\n",
        "    clip_model=model,\n",
        "    classnames=base_classnames,\n",
        "    base_classes=base_classes,\n",
        "    novel_classes=novel_classes,\n",
        "    device=device,\n",
        "    lr=0.002,          \n",
        "    n_ctx=16,           #if ctx_init is used, this will be ignored and set to 5\n",
        "    num_epochs=50,      \n",
        "    ctx_init=\"a photo of a flower\" \n",
        ")\n",
        "\n",
        "# INTRODUCING EARLY STOPPING\n",
        "patience = 5         \n",
        "counter = 0        \n",
        "best_acc = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"TRAINING CoCoOp (Smart Init + Early Stopping @ Patience {patience})\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(trainer.num_epochs):\n",
        "    # Training\n",
        "    avg_loss = trainer.train_epoch(train_base, batch_size=4, accumulation_steps=4)\n",
        "    \n",
        "    # Validation\n",
        "    val_acc = trainer.eval(val_base, base_classes, batch_size=64)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{trainer.num_epochs} - Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}%\", end=\"\")\n",
        "    \n",
        "    # EARLY STOPPING and CHECKPOINTING\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_epoch = epoch + 1\n",
        "        counter = 0\n",
        "        \n",
        "        # saving best model\n",
        "        state_dict = trainer.model.prompt_learner.state_dict()\n",
        "        torch.save(state_dict, \"checkpoints/best_model.pth\")\n",
        "        print(f\"  [★ BEST SAVED] - Counter reset\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"  [No Improv. {counter}/{patience}]\")\n",
        "        \n",
        "        if counter >= patience:\n",
        "            print(f\"\\n⏹ EARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
        "            print(f\"La validation accuracy non migliora da {patience} epoche.\")\n",
        "            break\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Training terminato. Best Val Acc: {best_acc*100:.2f}% at epoch {best_epoch}\")\n",
        "\n",
        "# Reload Best Model\n",
        "print(\"\\nReloading best model weights for final evaluation...\")\n",
        "best_checkpoint = torch.load(\"checkpoints/best_model.pth\")\n",
        "trainer.model.prompt_learner.load_state_dict(best_checkpoint)\n",
        "print(\"Best model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69c077f",
      "metadata": {
        "id": "b69c077f"
      },
      "source": [
        "## Final Evaluation (CoCoOp only)\n",
        "\n",
        "We'll evaluate the model with:\n",
        "1. Test Base\n",
        "2. Test Novel\n",
        "\n",
        "Computing Harmonic Mean between them to evaluate the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "49984fae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49984fae",
        "outputId": "093533f6-9dfa-4ea9-e134-ca0072e250a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 39/39 [05:03<00:00,  7.79s/it]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "base_acc = trainer.eval(test_base, base_classes, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a44f90bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swapping class definitions to 51 novel classes (In-Place)...\n",
            "Class definitions swapped. Starting evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Novel: 100%|██████████| 230/230 [07:16<00:00,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Corrected Novel Accuracy: 68.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation for novel classes with in-place class swapping\n",
        "@torch.no_grad()\n",
        "def evaluate_novel_inplace(trainer, test_dataset, novel_classnames, novel_classes_ids, device='cuda'):\n",
        "    print(f\"Swapping class definitions to {len(novel_classnames)} novel classes (In-Place)...\")\n",
        "    \n",
        "    model = trainer.model\n",
        "    prompt_learner = model.prompt_learner\n",
        "    \n",
        "    # 1. SAVE ORIGINAL STATE (Base Classes)\n",
        "    old_n_cls = prompt_learner.n_cls\n",
        "    old_token_prefix = prompt_learner.token_prefix\n",
        "    old_token_suffix = prompt_learner.token_suffix\n",
        "    old_tokenized_prompts = model.tokenized_prompts\n",
        "    \n",
        "    # 2. GENERATE NEW TEXT EMBEDDINGS (Novel Classes)\n",
        "    # Tokenize new names\n",
        "    clean_names = [name.replace(\"_\", \" \") for name in novel_classnames]\n",
        "    \n",
        "    # Reconstruct the standard prompt template used in PromptLearner\n",
        "    # PromptLearner uses: \"X X X X classname.\"\n",
        "    # We must replicate the exact logic to get correct prefix and suffix\n",
        "    n_ctx = prompt_learner.n_ctx\n",
        "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
        "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
        "    \n",
        "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "    \n",
        "    # Get embeddings from CLIP's Text Encoder\n",
        "    with torch.no_grad():\n",
        "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
        "    \n",
        "    # 3. OVERWRITE MODEL BUFFERS\n",
        "    # PromptLearner needs prefix and suffix to \"sandwich\" the learned vectors in between\n",
        "    new_token_prefix = embedding[:, :1, :]           # [n_cls, 1, dim]\n",
        "    new_token_suffix = embedding[:, 1+n_ctx:, :]     # [n_cls, len-1-n_ctx, dim]\n",
        "    \n",
        "    prompt_learner.register_buffer(\"token_prefix\", new_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", new_token_suffix)\n",
        "    prompt_learner.n_cls = len(novel_classnames)\n",
        "    prompt_learner.tokenized_prompts = new_tokenized\n",
        "    model.tokenized_prompts = new_tokenized \n",
        "    \n",
        "    print(\"Class definitions swapped. Starting evaluation...\")\n",
        "    \n",
        "    # 4. EVALUATION\n",
        "    model.eval()\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=16, shuffle=False, num_workers=0 # Low batch size for safety\n",
        "    )\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Mapping from original novel class IDs to local indices (0..N-1)\n",
        "    target_map = {original_id: idx for idx, original_id in enumerate(novel_classes_ids)}\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Evaluating Novel\"):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        logits = model(images) # generating logits for novel classes\n",
        "        pred = logits.argmax(dim=1)\n",
        "        \n",
        "        # Mapping labels\n",
        "        labels_cpu = labels.tolist()\n",
        "        try:\n",
        "            mapped_labels = torch.tensor([target_map[l] for l in labels_cpu], device=device)\n",
        "            correct += (pred == mapped_labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        except KeyError:\n",
        "            continue\n",
        "            \n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    \n",
        "    # 5. RESTORE ORIGINAL STATE\n",
        "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
        "    prompt_learner.n_cls = old_n_cls\n",
        "    prompt_learner.tokenized_prompts = old_tokenized_prompts\n",
        "    model.tokenized_prompts = old_tokenized_prompts\n",
        "    \n",
        "    return acc\n",
        "\n",
        "# EXECUTION\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "novel_acc = evaluate_novel_inplace(\n",
        "    trainer, \n",
        "    test_novel, \n",
        "    novel_classnames, # Name list ['rose', 'tulip'...]\n",
        "    novel_classes,    # ID list [51, 52...]\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nCorrected Novel Accuracy: {novel_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c3e9db2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "  Base Accuracy:   94.34%\n",
            "  Novel Accuracy:  68.58%\n",
            "  Harmonic Mean:   79.42%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "hm = harmonic_mean(base_acc, novel_acc)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
        "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
        "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a4ed1e",
      "metadata": {},
      "source": [
        "## Real-World-Scenario Testing -> base + novel classes at the same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a3422b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on ALL 102 classes simultaneously (Generalized Setting)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval Generalized: 100%|██████████| 193/193 [23:17<00:00,  7.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generalized Accuracy (Base + Novel mixed): 72.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_generalized(trainer, test_dataset, all_classnames, all_class_ids, device='cuda'):\n",
        "    print(f\"Evaluating on ALL {len(all_classnames)} classes simultaneously (Generalized Setting)...\")\n",
        "    \n",
        "    # 1. Model Setup with ALL classes (Base + Novel)\n",
        "    model = trainer.model\n",
        "    prompt_learner = model.prompt_learner\n",
        "    \n",
        "    # Save old state to restore later\n",
        "    old_n_cls = prompt_learner.n_cls\n",
        "    old_token_prefix = prompt_learner.token_prefix\n",
        "    old_token_suffix = prompt_learner.token_suffix\n",
        "    old_tokenized = model.tokenized_prompts\n",
        "\n",
        "    # 2. Create prompts for ALL classes (0..101)\n",
        "    clean_names = [name.replace(\"_\", \" \") for name in all_classnames]\n",
        "    \n",
        "    # Reconstruct dummy prompts\n",
        "    n_ctx = prompt_learner.n_ctx\n",
        "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
        "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
        "    \n",
        "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
        "    \n",
        "    # Update buffers\n",
        "    prompt_learner.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
        "    prompt_learner.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
        "    prompt_learner.n_cls = len(all_classnames)\n",
        "    prompt_learner.tokenized_prompts = new_tokenized\n",
        "    model.tokenized_prompts = new_tokenized\n",
        "\n",
        "    # 3. Evaluation\n",
        "    model.eval()\n",
        "    # Use the COMPLETE test dataset (Base + Novel) if possible, or a subset\n",
        "    dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Map: Original ID (0..101) -> Model Index (0..101)\n",
        "    # Since \"all\" classes are ordered, the map is usually identical (0->0, 101->101)\n",
        "    # But for safety, we use the passed IDs\n",
        "    target_map = {original_id: idx for idx, original_id in enumerate(all_class_ids)}\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Eval Generalized\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        logits = model(images) # Output shape: [Batch, 102]\n",
        "        pred = logits.argmax(dim=1)\n",
        "        \n",
        "        # Labels here arrive as Original IDs (e.g. 0, 55, 101)\n",
        "        # We must ensure they correspond to the model indices\n",
        "        mapped_labels = torch.tensor([target_map[l.item()] for l in labels], device=device)\n",
        "        \n",
        "        correct += (pred == mapped_labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "    # Restore state\n",
        "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
        "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
        "    prompt_learner.n_cls = old_n_cls\n",
        "    prompt_learner.tokenized_prompts = old_tokenized\n",
        "    model.tokenized_prompts = old_tokenized\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "# Merge lists\n",
        "all_names = base_classnames + novel_classnames\n",
        "all_ids = list(base_classes) + list(novel_classes)\n",
        "\n",
        "# Merge test datasets (Base + Novel) to perform a unique \"Real\" test\n",
        "full_test_set = torch.utils.data.ConcatDataset([test_base, test_novel])\n",
        "\n",
        "acc_generalized = evaluate_generalized(trainer, full_test_set, all_names, all_ids)\n",
        "print(f\"Generalized Accuracy (Base + Novel mixed): {acc_generalized*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
