{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff453b1a",
   "metadata": {
    "id": "ff453b1a"
   },
   "source": [
    "# CLIP with Flowers!?!?!??!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73175206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73175206",
    "outputId": "fb56fbf4-f574-42f8-f09f-c0e12dbf36cc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3e678e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3e678e",
    "outputId": "4a82e2f7-20d3-47b9-b8c6-5eddbe9efb82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP already installed\n",
      "Device: cpu\n",
      "PyTorch: 2.9.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\clip\\clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import clip\n",
    "    print(\"✓ CLIP already installed\")\n",
    "except Exception:\n",
    "    print(\"Installing CLIP...\")\n",
    "    import subprocess, importlib\n",
    "    try:\n",
    "        get_ipython().run_line_magic('pip', 'install --upgrade git+https://github.com/openai/CLIP.git')\n",
    "    except:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \n",
    "                              \"git+https://github.com/openai/CLIP.git\"])\n",
    "    importlib.invalidate_caches()\n",
    "    import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e45cb",
   "metadata": {
    "id": "cf9e45cb"
   },
   "source": [
    "## Dataset Functions\n",
    "\n",
    "We define utility functions for:\n",
    "- **`get_data()`**: Load Flowers102 from torchvision\n",
    "- **`base_novel_categories()`**: Split 102 classes into base (0-50) and novel (51-101)\n",
    "- **`split_data()`**: Filter images for base/novel in each split\n",
    "\n",
    "This simulates the real scenario: we have 51 seen classes during training (base) and 51 new ones (novel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adfe795",
   "metadata": {
    "id": "9adfe795"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    \"\"\"Return base and novel class id lists using the actual labels present\n",
    "    in the dataset. Prefer public attributes (`targets` then `labels`) and\n",
    "    only fall back to the dataset private attribute `_labels` if neither is\n",
    "    available.\n",
    "    \"\"\"\n",
    "    labels = getattr(dataset, \"targets\", None)\n",
    "    if labels is None:\n",
    "        labels = getattr(dataset, \"labels\", None)\n",
    "\n",
    "    if labels is None and hasattr(dataset, \"_labels\"):\n",
    "        labels = dataset._labels\n",
    "\n",
    "    if labels is None:\n",
    "        raise ValueError(\"Could not find labels on dataset (checked 'targets','labels','_labels').\")\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    num_classes = len(unique_labels)\n",
    "    mid = num_classes // 2\n",
    "    base_classes = unique_labels[:mid]\n",
    "    novel_classes = unique_labels[mid:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bda8d2",
   "metadata": {
    "id": "74bda8d2"
   },
   "source": [
    "## Class Names and Dataset Loading\n",
    "\n",
    "We load the names of 102 flower classes from Flowers102.\n",
    "\n",
    "This is **critical** for CLIP:\n",
    "- Creates prompts like \"a photo of a **rose**, a type of flower\"\n",
    "- Each prompt is encoded by CLIP's text encoder\n",
    "- Image features are compared against these text templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f443bb94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f443bb94",
    "outputId": "b8c777b9-3e20-4a8f-a748-bece7ade5051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# Uncomment to see class names\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a6d3c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8a6d3c9",
    "outputId": "0a979e91-25fe-4780-c396-883fa9772d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model: ViT-B/16\n",
      "Augmentation pipeline defined.\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# --- Data transformation for Augmentation ---\n",
    "#keeping CLIP normalization values\n",
    "aug_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.RandomCrop(224),           # Random Crop\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5), \n",
    "    torchvision.transforms.RandomRotation(15),        # smooth rotation\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Color Jitter\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.481, 0.457, 0.408), (0.268, 0.261, 0.275)) # Mean/Std of CLIP\n",
    "])\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: ViT-B/16\")\n",
    "print(\"Augmentation pipeline defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4b330",
   "metadata": {},
   "source": [
    "## Prototype-Based Representations\n",
    "\n",
    "We construct **class prototypes** from CLIP image embeddings of training samples.\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Use **frozen CLIP** (not the adapted model) to preserve zero-shot knowledge\n",
    "- Compute prototypes from **both normal and augmented samples** for better coverage\n",
    "- **L2-normalize** embeddings before averaging and after\n",
    "\n",
    "**At Inference:**\n",
    "- Compute prototype similarity: $\\text{sim}_{\\text{proto}}(x, c) = \\frac{f(x) \\cdot p_c}{\\|f(x)\\| \\|p_c\\|}$\n",
    "- Fuse with CoCoOp logits: $\\text{logits}_{\\text{final}} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{proto}}$\n",
    "- The fusion weight $\\alpha$ controls the trade-off between prompt-based and prototype-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271e54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_prototypes(clip_model, train_dataset, base_classes, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract CLIP image embeddings and compute mean prototype per class.\n",
    "    Uses the ORIGINAL frozen CLIP to preserve zero-shot knowledge.\n",
    "    \n",
    "    Args:\n",
    "        clip_model: Frozen CLIP model\n",
    "        train_dataset: Training dataset (can include augmented samples)\n",
    "        base_classes: List of base class IDs\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        prototypes: Dict mapping class_id -> normalized prototype tensor (shape: [dim])\n",
    "        prototype_matrix: Tensor of shape [num_classes, dim] for efficient inference\n",
    "    \"\"\"\n",
    "    clip_model.eval()\n",
    "    \n",
    "    # Collect embeddings per class\n",
    "    embeddings_per_class = {c: [] for c in base_classes}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(train_dataset)} samples...\")\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Building Prototypes\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get CLIP image features\n",
    "        features = clip_model.encode_image(images)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "        \n",
    "        for feat, label in zip(features, labels):\n",
    "            label_id = label.item()\n",
    "            if label_id in embeddings_per_class:\n",
    "                embeddings_per_class[label_id].append(feat.cpu())\n",
    "    \n",
    "    # Compute mean prototype per class\n",
    "    prototypes = {}\n",
    "    for cls_id in base_classes:\n",
    "        if embeddings_per_class[cls_id]:\n",
    "            class_embeddings = torch.stack(embeddings_per_class[cls_id])\n",
    "            prototype = class_embeddings.mean(dim=0).to(device)\n",
    "            prototype = prototype / prototype.norm()  # Re-normalize after averaging\n",
    "            prototypes[cls_id] = prototype\n",
    "    \n",
    "    # Create matrix for efficient inference (ordered by base_classes)\n",
    "    prototype_matrix = torch.stack([prototypes[c] for c in base_classes]).to(device)\n",
    "    \n",
    "    print(f\"✓ Built {len(prototypes)} prototypes | Matrix shape: {prototype_matrix.shape}\")\n",
    "    \n",
    "    return prototypes, prototype_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e05089",
   "metadata": {
    "id": "e0e05089"
   },
   "source": [
    "## Load Flowers102 and Split Base/Novel\n",
    "\n",
    "We load the 3 splits (train, val, test) and divide into base/novel.\n",
    "\n",
    "**Statistics:**\n",
    "- Train Base: 10 images × 51 classes = 510 images\n",
    "- Val Base: 10 images × 51 classes = 510 images\n",
    "- Test Base: ~10 images × 51 classes (from test split)\n",
    "- Test Novel: Remaining (~10 per class)\n",
    "\n",
    "**Note:** Train and val have ~10 images per class (few-shot setting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd5516",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fdd5516",
    "outputId": "55367b6e-6008-4f7c-98fa-c9c153f988c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created Successfully!\n",
      "Base Classes: 51 | Shots: 16\n",
      "Normal Subset Size: 816\n",
      "Augmented Subset Size: 816\n",
      "-> TOTAL Train Base: 1632 samples (Should be 1632)\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Raw Data Retrieval ---\n",
    "# Note: Flowers102 has 10 imgs per class in train set. We use only train for 10 shots.\n",
    "# Augmented samples are used ONLY for building prototypes, not for training.\n",
    "\n",
    "# Helper to load with a specific transformation\n",
    "def load_split(split, transform):\n",
    "    return torchvision.datasets.Flowers102(root=\"./data\", split=split, download=True, transform=transform)\n",
    "\n",
    "# Load \"Normal\" sets (Standard CLIP preprocess)\n",
    "train_set_norm = load_split(\"train\", preprocess)\n",
    "val_set_norm = load_split(\"val\", preprocess)\n",
    "test_set = load_split(\"test\", preprocess)\n",
    "\n",
    "# Load \"Augmented\" set (only for prototype building)\n",
    "train_set_aug = load_split(\"train\", aug_transform)\n",
    "\n",
    "# --- STEP 2: Base/Novel Class Split ---\n",
    "base_classes, novel_classes = base_novel_categories(train_set_norm)\n",
    "\n",
    "# --- STEP 3: Few-Shot Selection (10 Real Shots from Train Set) ---\n",
    "shots_per_class = 10\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Collect all available indices for each base class in the train set\n",
    "indices_per_class = {c: [] for c in base_classes}\n",
    "for idx, label in enumerate(train_set_norm._labels):\n",
    "    if label in base_classes:\n",
    "        indices_per_class[label].append(idx)\n",
    "\n",
    "selected_indices = []\n",
    "for c in base_classes:\n",
    "    inds = indices_per_class.get(c, [])\n",
    "    random.shuffle(inds)\n",
    "    # Take 10 shots from train set\n",
    "    selected_indices.extend(inds[:shots_per_class])\n",
    "\n",
    "# --- STEP 4: Create Datasets ---\n",
    "# Training dataset: ONLY normal samples (no augmentation)\n",
    "train_base = torch.utils.data.Subset(train_set_norm, selected_indices)\n",
    "\n",
    "# Prototype dataset: Normal + Augmented samples (for richer prototype representations)\n",
    "subset_normal = torch.utils.data.Subset(train_set_norm, selected_indices)\n",
    "subset_augmented = torch.utils.data.Subset(train_set_aug, selected_indices)\n",
    "prototype_dataset = torch.utils.data.ConcatDataset([subset_normal, subset_augmented])\n",
    "\n",
    "# Validation and Test sets\n",
    "val_base, _ = split_data(test_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "print(f\"Dataset Created Successfully!\")\n",
    "print(f\"Base Classes: {len(base_classes)} | Shots: {shots_per_class}\")\n",
    "print(f\"-> Training Set: {len(train_base)} samples (normal only)\")\n",
    "print(f\"-> Prototype Set: {len(prototype_dataset)} samples (normal + augmented)\")\n",
    "print(f\"-> Val Base: {len(val_base)} | Test Base: {len(test_base)} | Test Novel: {len(test_novel)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071f907",
   "metadata": {
    "id": "8071f907"
   },
   "source": [
    "## Harmonic Mean (HM)\n",
    "\n",
    "Standard metric for few-shot adaptation papers.\n",
    "\n",
    "Formula: HM = 2 / (1/base_acc + 1/novel_acc)\n",
    "\n",
    "**Why HM instead of arithmetic mean?**\n",
    "- HM heavily penalizes outliers\n",
    "- If base=90% and novel=50%: arithmetic mean=70%, HM=64.3%\n",
    "- Forces the model to balance both accuracies\n",
    "\n",
    "**Obiettivo:** massimizzare l'HM tra `base_acc_cocoop` e `novel_acc_cocoop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eec2ec0",
   "metadata": {
    "id": "3eec2ec0"
   },
   "outputs": [],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    # Guard against zero to avoid division-by-zero errors\n",
    "    if base_accuracy <= 0 or novel_accuracy <= 0:\n",
    "        return 0.0\n",
    "    numerator = 2.0\n",
    "    denominator = 1.0 / base_accuracy + 1.0 / novel_accuracy\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12012951",
   "metadata": {
    "id": "12012951"
   },
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29fc20f",
   "metadata": {
    "id": "f29fc20f"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(int(x.shape[0])), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916487b1",
   "metadata": {
    "id": "916487b1"
   },
   "source": [
    "## CoCoOpPromptLearner: Dynamic Prompts (Optimized)\n",
    "\n",
    "**Components:**\n",
    "1. **Context Vectors (V):** 16 vectors (learnable).\n",
    "   - Shape: `(16, 512)`\n",
    "   - Initialized: Gaussian noise N(0, 0.02)\n",
    "   - Function: Provide the base context for the prompt.\n",
    "\n",
    "2. **Meta-Network (Bias Generator):**\n",
    "   - Architecture: Linear(512->32) -> ReLU -> Linear(32->512)\n",
    "   - Input: Image Features `(Batch, 512)`\n",
    "   - Output: Bias `(Batch, 512)` added to Context Vectors.\n",
    "   - **Note:** Unlike the paper's simplified notation \"$\\pi$\", we implement this as an **additive bias** to the context vectors.\n",
    "\n",
    "3. **Class Embeddings:**\n",
    "   - Pre-computed embeddings for \"[CLASS] + EOS\".\n",
    "   - Fixed during training.\n",
    "\n",
    "**Forward Pass (Vectorized):**\n",
    "Instead of looping through images, we broadcast tensors to shape `(Batch, Num_Classes, Sequence_Length, Dim)`:\n",
    "1. **Compute Bias:** $Bias = MetaNet(Image)$\n",
    "2. **Shift Context:** $Ctx_{new} = Ctx_{base} + Bias$ (Broadcasting over classes)\n",
    "3. **Concatenate:** $[Prefix] + [Ctx_{new}] + [Suffix]$ (All in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072ba719",
   "metadata": {
    "id": "072ba719"
   },
   "outputs": [],
   "source": [
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        # Get embedding dimension from CLIP's final layer\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        \n",
    "        # 1. Context Vectors Initialization\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            \n",
    "            # --- FIX: Move tokenized prompt to correct device ---\n",
    "            prompt = clip.tokenize(ctx_init).to(device) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(torch.float16)\n",
    "            \n",
    "            # embedding[0] because tokenize adds a batch dim\n",
    "            ctx_vectors = embedding[0, 1:1+n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # Random initialization standard (Sigma=0.02)\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float16)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words: {n_ctx}\")\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # 2. Meta-Network (Less Aggressive Bottleneck for better Generalization)\n",
    "        # Trying to increase hidden dim from 32 (//16) to 128 (//4) to prevent underfitting novel classes\n",
    "        hidden_dim = vis_dim // 4\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, hidden_dim)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(hidden_dim, ctx_dim))\n",
    "        ]))\n",
    "        \n",
    "        # 3. Pre-computing Class Names (Prefix/Suffix)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "        \n",
    "        # Tokenize and get embeddings\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        tokenized_prompts = tokenized_prompts.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(torch.float16)\n",
    "        \n",
    "        # Save Prefix (SOS) and Suffix (Class Name + EOS) as fixed buffers\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])      # (n_cls, 1, dim)\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :]) # (n_cls, len, dim)\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "    \n",
    "    def forward(self, im_features):\n",
    "        batch_size = im_features.shape[0]\n",
    "        \n",
    "        # 1. Calculate bias from image\n",
    "        bias = self.meta_net(im_features)\n",
    "        bias = bias.unsqueeze(1) # (Batch, 1, 512)\n",
    "        \n",
    "        # 2. Generate shifted context\n",
    "        ctx = self.ctx.unsqueeze(0) # (1, n_ctx, dim)\n",
    "        ctx_shifted = ctx + bias    # (Batch, n_ctx, dim)\n",
    "        \n",
    "        # 3. Parallel prompt construction\n",
    "        prefix = self.token_prefix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        ctx_expanded = ctx_shifted.unsqueeze(1).expand(-1, self.n_cls, -1, -1)\n",
    "        suffix = self.token_suffix.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        \n",
    "        prompts = torch.cat(\n",
    "            [prefix, ctx_expanded, suffix],\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b162a",
   "metadata": {
    "id": "6c7b162a"
   },
   "source": [
    "## CoCoOpTrainer: Training and Evaluation (with Prototype Fusion)\n",
    "\n",
    "Class that manages:\n",
    "\n",
    "**1. Initialization:**\n",
    "- Create PromptLearner\n",
    "- Freeze CLIP (`requires_grad=False`)\n",
    "- Configure SGD optimizer for prompt learner only\n",
    "\n",
    "**2. train_epoch():**\n",
    "- Forward: Image encoder + PromptLearner + Text encoder\n",
    "- **Critical step:** Encode soft prompts through text transformer\n",
    "  - Add positional embeddings\n",
    "  - Pass through CLIP's transformer\n",
    "  - Extract first token\n",
    "  - Apply final layer norm + projection\n",
    "- Compute loss: Cross-entropy on base classes\n",
    "- Backward: Backprop only in PromptLearner\n",
    "- Return: Average loss of the epoch\n",
    "\n",
    "**3. eval() with Prototype Fusion:**\n",
    "- Same forward procedure as training\n",
    "- **NEW:** Optionally fuse CoCoOp logits with prototype similarity scores\n",
    "- Fusion formula: $\\text{logits} = \\alpha \\cdot \\text{logits}_{\\text{CoCoOp}} + (1-\\alpha) \\cdot \\text{logits}_{\\text{prototype}}$\n",
    "- Compute accuracy on any dataset (base or novel)\n",
    "\n",
    "**Important note:** We don't use `model.encode_text()` on soft prompts\n",
    "because that method expects integer tokens, not embeddings.\n",
    "We manually forward through the text transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959af3c5",
   "metadata": {
    "id": "959af3c5"
   },
   "outputs": [],
   "source": [
    "class CustomCLIP(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=4, ctx_init=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx=n_ctx, ctx_init=ctx_init, device=device)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "        \n",
    "        # Prototype fusion parameters\n",
    "        self.prototype_matrix = None  # Will be set externally\n",
    "        self.alpha = 0.5  # Fusion weight: alpha * CoCoOp + (1-alpha) * Prototype\n",
    "    \n",
    "    def set_prototypes(self, prototype_matrix, alpha=0.5):\n",
    "        \"\"\"Set prototype matrix for fusion at inference time.\"\"\"\n",
    "        self.prototype_matrix = prototype_matrix.type(self.dtype)\n",
    "        self.alpha = alpha\n",
    "        print(f\"✓ Prototypes set | Alpha (CoCoOp weight): {alpha}\")\n",
    "    \n",
    "    def forward(self, image, label=None, use_prototypes=False):\n",
    "        \"\"\"\n",
    "        Forward pass with optional prototype fusion.\n",
    "        \n",
    "        Args:\n",
    "            image: Input images\n",
    "            label: Labels for training (returns loss if provided)\n",
    "            use_prototypes: Whether to fuse prototype logits at inference\n",
    "        \"\"\"\n",
    "        # encode images\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # generate instance-conditioned prompts: (batch, n_cls, n_tokens, dim)\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        batch_size = int(prompts.shape[0])\n",
    "        n_cls = int(self.prompt_learner.n_cls)\n",
    "\n",
    "        # Flatten prompts to (batch*n_cls, n_tokens, dim) for parallel encoding\n",
    "        n_tokens = prompts.shape[2]\n",
    "        dim = prompts.shape[3]\n",
    "        prompts_flat = prompts.reshape(batch_size * n_cls, n_tokens, dim).type(self.dtype)\n",
    "\n",
    "        # Repeat tokenized prompts for each image in the batch\n",
    "        tokenized = self.tokenized_prompts.to(prompts_flat.device)\n",
    "        tokenized_expanded = tokenized.repeat(batch_size, 1)\n",
    "\n",
    "        # Encode all prompts in parallel and reshape back: (batch, n_cls, dim)\n",
    "        text_features = self.text_encoder(prompts_flat, tokenized_expanded)\n",
    "        text_features = text_features.reshape(batch_size, n_cls, -1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute CoCoOp logits: (batch, n_cls)\n",
    "        image_features_expanded = image_features.unsqueeze(1)\n",
    "        cocoop_logits = logit_scale * (image_features_expanded @ text_features.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # Prototype fusion\n",
    "        if use_prototypes and self.prototype_matrix is not None:\n",
    "            # Compute prototype logits: (batch, n_cls)\n",
    "            prototype_logits = logit_scale * (image_features @ self.prototype_matrix.T)\n",
    "            \n",
    "            # Fused logits\n",
    "            logits = self.alpha * cocoop_logits + (1 - self.alpha) * prototype_logits\n",
    "        else:\n",
    "            logits = cocoop_logits\n",
    "\n",
    "        if label is not None:\n",
    "            return F.cross_entropy(logits, label)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838deabc",
   "metadata": {
    "id": "838deabc"
   },
   "source": [
    "## Training CoCoOp (Optimized)\n",
    "\n",
    "We will train the PromptLearner for **10 epochs** on **base classes only**.\n",
    "\n",
    "**Hyperparameters (Optimized):**\n",
    "- **Context Length (`n_ctx`):** 16 (Increased capacity for fine-grained details)\n",
    "- **Batch size:** 4 (Increased from 1 thanks to parallelization)\n",
    "- **Learning rate:** 0.002 (SGD)\n",
    "- **Momentum:** 0.9\n",
    "- **Weight decay:** 5e-4\n",
    "- **Epochs:** 10\n",
    "\n",
    "**What happens:**\n",
    "- The `PromptLearner` adapts its 16 context vectors to the Flowers102 dataset.\n",
    "- The `MetaNetwork` learns to inject image-specific bias efficiently.\n",
    "- **Optimization:** We use a GPU-based label lookup table to speed up target mapping.\n",
    "\n",
    "**Expected output:**\n",
    "- Initial loss: ~2.5 - 3.5\n",
    "- Final loss: ~0.5 - 1.0 (Lower than before due to better context capacity)\n",
    "- Training time: ~2-4 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d315494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpTrainer:\n",
    "    def __init__(self, clip_model, classnames, base_classes, novel_classes, \n",
    "                 device='cuda', lr=0.002, n_ctx=16, num_epochs=10, ctx_init=None): # <--- NEW PARAMETER\n",
    "        \n",
    "        self.clip_model = clip_model.float()\n",
    "        self.classnames = classnames\n",
    "        self.base_classes = base_classes\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # --- LABEL MAPPING OPTIMIZATION (GPU Lookup) ---\n",
    "        # We need to map original dataset labels (e.g., 0, 55, 101) to local indices (0..N)\n",
    "        max_label_id = max(max(base_classes), max(novel_classes)) + 1\n",
    "        self.label_map = torch.full((max_label_id,), -1, dtype=torch.long, device=device)\n",
    "        \n",
    "        base_ids_tensor = torch.tensor(base_classes, device=device, dtype=torch.long)\n",
    "        target_indices = torch.arange(len(base_classes), device=device, dtype=torch.long)\n",
    "        self.label_map[base_ids_tensor] = target_indices\n",
    "        \n",
    "        # Freeze CLIP\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # Initialize Custom Model (Passing ctx_init)\n",
    "        print(f\"Initializing CustomCLIP with ctx_init='{ctx_init}'...\")\n",
    "        self.model = CustomCLIP(self.clip_model, classnames, n_ctx=n_ctx, ctx_init=ctx_init, device=device).to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.prompt_learner.parameters(), \n",
    "            lr=lr, \n",
    "            momentum=0.9, \n",
    "            weight_decay=5e-4\n",
    "        )\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs)\n",
    "        \n",
    "        trainable = sum(p.numel() for p in self.model.prompt_learner.parameters())\n",
    "        print(f\"\\nCoCoOpTrainer initialized: {trainable:,} trainable params\")\n",
    "        print(f\"Context: {n_ctx} | Gradient Accumulation Enabled | Init: {ctx_init if ctx_init else 'Random'}\")\n",
    "    \n",
    "    def train_epoch(self, train_dataset, batch_size=4, accumulation_steps=4):\n",
    "        \"\"\"\n",
    "        Runs a training epoch with Gradient Accumulation.\n",
    "        -> Effective Batch Size = batch_size * accumulation_steps (e.g., 4 * 4 = 16)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False\n",
    "        )\n",
    "        \n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training (Grad Accum)\")):\n",
    "            images = images.to(self.device).float()\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Fast mapping using pre-computed GPU tensor\n",
    "            labels_mapped = self.label_map[labels]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = self.model(images, labels_mapped)\n",
    "            \n",
    "            # --- GRADIENT ACCUMULATION LOGIC ---\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Process remaining gradients if dataloader size is not divisible by accumulation_steps\n",
    "        if len(dataloader) % accumulation_steps != 0:\n",
    "             self.optimizer.step()\n",
    "             self.optimizer.zero_grad()\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        return total_loss / max(1, n_batches)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataset, categories, batch_size=128, use_prototypes=False):\n",
    "        \"\"\"\n",
    "        Evaluate model on dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset to evaluate on\n",
    "            categories: List of category IDs\n",
    "            batch_size: Batch size for evaluation\n",
    "            use_prototypes: Whether to use prototype fusion (requires set_prototypes called first)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        local_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    "        )\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(self.device).float()\n",
    "            logits = self.model(images, use_prototypes=use_prototypes)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            \n",
    "            labels_cpu = labels.tolist()\n",
    "            try:\n",
    "                mapped_targets = torch.tensor(\n",
    "                    [local_cat2idx[l] for l in labels_cpu], \n",
    "                    device=self.device\n",
    "                )\n",
    "                correct += (pred == mapped_targets).sum().item()\n",
    "                total += labels.size(0)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Base: 510 | Test Base: 2473 | Test Novel: 3676\n"
     ]
    }
   ],
   "source": [
    "#Loading data and model\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Note: train_base (normal only) and prototype_dataset (normal + augmented) \n",
    "# are already created in Cell 10\n",
    "\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "print(f\"Train Base: {len(train_base)} samples (normal only, for training)\")\n",
    "print(f\"Prototype Dataset: {len(prototype_dataset)} samples (normal + augmented, for prototypes)\")\n",
    "print(f\"Val Base: {len(val_base)} | Test Base: {len(test_base)} | Test Novel: {len(test_novel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING CLASS PROTOTYPES\n",
      "======================================================================\n",
      "Extracting embeddings from 510 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 8/8 [01:34<00:00, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built 51 prototypes | Matrix shape: torch.Size([51, 512])\n",
      "Prototype matrix dtype: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build class prototypes from frozen CLIP using normal + augmented samples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING CLASS PROTOTYPES (using normal + augmented samples)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prototypes, prototype_matrix = build_prototypes(\n",
    "    clip_model=model,  # Frozen original CLIP\n",
    "    train_dataset=prototype_dataset,  # Uses both normal and augmented samples\n",
    "    base_classes=base_classes,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Prototype matrix dtype: {prototype_matrix.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2feda02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2feda02",
    "outputId": "d1e3de97-7eef-45f2-fe33-9293ee28910c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CustomCLIP with ctx_init='a photo of a flower'...\n",
      "Initial context: \"a photo of a flower\"\n",
      "Number of context words: 5\n",
      "\n",
      "CoCoOpTrainer initialized: 134,272 trainable params\n",
      "Context: 16 | Gradient Accumulation Enabled | Init: a photo of a flower\n",
      "\n",
      "======================================================================\n",
      "TRAINING CoCoOp (Smart Init + Early Stopping @ Patience 5)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Grad Accum): 100%|██████████| 128/128 [33:28<00:00, 15.69s/it]\n",
      "Evaluating:  50%|█████     | 4/8 [11:54<11:54, 178.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m avg_loss = trainer.train_epoch(train_base, batch_size=\u001b[32m4\u001b[39m, accumulation_steps=\u001b[32m4\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m val_acc = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# EARLY STOPPING and CHECKPOINTING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mCoCoOpTrainer.eval\u001b[39m\u001b[34m(self, dataset, categories, batch_size, use_prototypes)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mEvaluating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m     images = images.to(\u001b[38;5;28mself\u001b[39m.device).float()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_prototypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_prototypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     pred = logits.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m    114\u001b[39m     labels_cpu = labels.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mCustomCLIP.forward\u001b[39m\u001b[34m(self, image, label, use_prototypes)\u001b[39m\n\u001b[32m     47\u001b[39m tokenized_expanded = tokenized.repeat(batch_size, \u001b[32m1\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Encode all prompts in parallel and reshape back: (batch, n_cls, dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m text_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_expanded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m text_features = text_features.reshape(batch_size, n_cls, -\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m text_features = text_features / text_features.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mTextEncoder.forward\u001b[39m\u001b[34m(self, prompts, tokenized_prompts)\u001b[39m\n\u001b[32m     11\u001b[39m x = prompts + \u001b[38;5;28mself\u001b[39m.positional_embedding.type(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m     12\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     15\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x).type(\u001b[38;5;28mself\u001b[39m.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\clip\\model.py:204\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\clip\\model.py:192\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    191\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.attention(\u001b[38;5;28mself\u001b[39m.ln_1(x))\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marco\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Configurazione\n",
    "base_classnames = [CLASS_NAMES[i] for i in base_classes]\n",
    "novel_classnames = [CLASS_NAMES[i] for i in novel_classes]\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "trainer = CoCoOpTrainer(\n",
    "    clip_model=model,\n",
    "    classnames=base_classnames,\n",
    "    base_classes=base_classes,\n",
    "    novel_classes=novel_classes,\n",
    "    device=device,\n",
    "    lr=0.002,          \n",
    "    n_ctx=16,           #if ctx_init is used, this will be ignored and set to 5\n",
    "    num_epochs=50,      \n",
    "    ctx_init=\"a photo of a flower\" \n",
    ")\n",
    "\n",
    "# INTRODUCING EARLY STOPPING\n",
    "patience = 5         \n",
    "counter = 0        \n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING CoCoOp (Smart Init + Early Stopping @ Patience {patience})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(trainer.num_epochs):\n",
    "    # Training\n",
    "    avg_loss = trainer.train_epoch(train_base, batch_size=4, accumulation_steps=4)\n",
    "    \n",
    "    # Validation\n",
    "    val_acc = trainer.eval(val_base, base_classes, batch_size=64)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{trainer.num_epochs} - Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}%\", end=\"\")\n",
    "    \n",
    "    # EARLY STOPPING and CHECKPOINTING\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        counter = 0\n",
    "        \n",
    "        # saving best model\n",
    "        state_dict = trainer.model.prompt_learner.state_dict()\n",
    "        torch.save(state_dict, \"checkpoints/best_model.pth\")\n",
    "        print(f\"  [★ BEST SAVED] - Counter reset\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"  [No Improv. {counter}/{patience}]\")\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"\\n⏹ EARLY STOPPING TRIGGERED at epoch {epoch+1}!\")\n",
    "            print(f\"La validation accuracy non migliora da {patience} epoche.\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training terminated. Best Val Acc: {best_acc*100:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "# Reload Best Model\n",
    "print(\"\\nReloading best model weights for final evaluation...\")\n",
    "best_checkpoint = torch.load(\"checkpoints/best_model.pth\")\n",
    "trainer.model.prompt_learner.load_state_dict(best_checkpoint)\n",
    "print(\"Best model loaded.\")\n",
    "\n",
    "# Set prototypes for inference-time fusion\n",
    "trainer.model.set_prototypes(prototype_matrix, alpha=0.5)  # Try different alpha values: 0.3, 0.5, 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c077f",
   "metadata": {
    "id": "b69c077f"
   },
   "source": [
    "## Final Evaluation (CoCoOp + Prototype Fusion)\n",
    "\n",
    "We'll evaluate the model with:\n",
    "1. **Test Base** - CoCoOp only vs CoCoOp + Prototypes\n",
    "2. **Test Novel** - CoCoOp only (no prototypes for novel classes)\n",
    "\n",
    "Computing Harmonic Mean between them to evaluate the trade-off.\n",
    "\n",
    "**Note:** Prototypes are only available for base classes (built from training data).\n",
    "For novel classes, we rely solely on CoCoOp's generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49984fae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49984fae",
    "outputId": "093533f6-9dfa-4ea9-e134-ca0072e250a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 39/39 [04:53<00:00,  7.52s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "base_acc = trainer.eval(test_base, base_classes, batch_size=64, use_prototypes=True)\n",
    "print(f\"Base Accuracy: {base_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a44f90bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapping class definitions to 51 novel classes (In-Place)...\n",
      "Class definitions swapped. Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Novel: 100%|██████████| 115/115 [06:49<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corrected Novel Accuracy: 73.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for novel classes with in-place class swapping\n",
    "@torch.no_grad()\n",
    "def evaluate_novel_inplace(trainer, test_dataset, novel_classnames, novel_classes_ids, device='cuda'):\n",
    "    print(f\"Swapping class definitions to {len(novel_classnames)} novel classes (In-Place)...\")\n",
    "    \n",
    "    model = trainer.model\n",
    "    prompt_learner = model.prompt_learner\n",
    "    \n",
    "    # 1. SAVE ORIGINAL STATE (Base Classes)\n",
    "    old_n_cls = prompt_learner.n_cls\n",
    "    old_token_prefix = prompt_learner.token_prefix\n",
    "    old_token_suffix = prompt_learner.token_suffix\n",
    "    old_tokenized_prompts = model.tokenized_prompts\n",
    "    \n",
    "    # 2. GENERATE NEW TEXT EMBEDDINGS (Novel Classes)\n",
    "    # Tokenize new names\n",
    "    clean_names = [name.replace(\"_\", \" \") for name in novel_classnames]\n",
    "    \n",
    "    # Reconstruct the standard prompt template used in PromptLearner\n",
    "    # PromptLearner uses: \"X X X X classname.\"\n",
    "    # We must replicate the exact logic to get correct prefix and suffix\n",
    "    n_ctx = prompt_learner.n_ctx\n",
    "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
    "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
    "    \n",
    "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "    \n",
    "    # Get embeddings from CLIP's Text Encoder\n",
    "    with torch.no_grad():\n",
    "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
    "    \n",
    "    # 3. OVERWRITE MODEL BUFFERS\n",
    "    # PromptLearner needs prefix and suffix to \"sandwich\" the learned vectors in between\n",
    "    new_token_prefix = embedding[:, :1, :]           # [n_cls, 1, dim]\n",
    "    new_token_suffix = embedding[:, 1+n_ctx:, :]     # [n_cls, len-1-n_ctx, dim]\n",
    "    \n",
    "    prompt_learner.register_buffer(\"token_prefix\", new_token_prefix)\n",
    "    prompt_learner.register_buffer(\"token_suffix\", new_token_suffix)\n",
    "    prompt_learner.n_cls = len(novel_classnames)\n",
    "    prompt_learner.tokenized_prompts = new_tokenized\n",
    "    model.tokenized_prompts = new_tokenized \n",
    "    \n",
    "    print(\"Class definitions swapped. Starting evaluation...\")\n",
    "    \n",
    "    # 4. EVALUATION\n",
    "    model.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=32, shuffle=False, num_workers=0 # trying a higher batch size\n",
    "    )\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Mapping from original novel class IDs to local indices (0..N-1)\n",
    "    target_map = {original_id: idx for idx, original_id in enumerate(novel_classes_ids)}\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluating Novel\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        logits = model(images) # generating logits for novel classes\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        # Mapping labels\n",
    "        labels_cpu = labels.tolist()\n",
    "        try:\n",
    "            mapped_labels = torch.tensor([target_map[l] for l in labels_cpu], device=device)\n",
    "            correct += (pred == mapped_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    # 5. RESTORE ORIGINAL STATE\n",
    "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
    "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
    "    prompt_learner.n_cls = old_n_cls\n",
    "    prompt_learner.tokenized_prompts = old_tokenized_prompts\n",
    "    model.tokenized_prompts = old_tokenized_prompts\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# EXECUTION\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "novel_acc = evaluate_novel_inplace(\n",
    "    trainer, \n",
    "    test_novel, \n",
    "    novel_classnames, # Name list ['rose', 'tulip'...]\n",
    "    novel_classes,    # ID list [51, 52...]\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nCorrected Novel Accuracy: {novel_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3e9db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "  Base Accuracy:   91.87%\n",
      "  Novel Accuracy:  73.86%\n",
      "  Harmonic Mean:   81.89%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Base Accuracy:  {base_acc*100:6.2f}%\")\n",
    "print(f\"  Novel Accuracy: {novel_acc*100:6.2f}%\")\n",
    "print(f\"  Harmonic Mean:  {hm*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4ed1e",
   "metadata": {},
   "source": [
    "## Real-World-Scenario Testing -> base + novel classes at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3a3422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ALL 102 classes simultaneously (Generalized Setting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Generalized:   0%|          | 0/193 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.28 GiB is free. Process 4116 has 13.46 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 106.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3114408114.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mfull_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_novel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0macc_generalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_generalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generalized Accuracy (Base + Novel mixed): {acc_generalized*100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3114408114.py\u001b[0m in \u001b[0;36mevaluate_generalized\u001b[0;34m(trainer, test_dataset, all_classnames, all_class_ids, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output shape: [Batch, 102]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1633855578.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, label)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Encode all prompts in parallel and reshape back: (batch, n_cls, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_expanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4129437004.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prompts, tokenized_prompts)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1486\u001b[0m             )\n\u001b[1;32m   1487\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1489\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6305\u001b[0m             \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6306\u001b[0m         )\n\u001b[0;32m-> 6307\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6308\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6309\u001b[0m         assert q_proj_weight is not None, (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5704\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5705\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5706\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5707\u001b[0m             )\n\u001b[1;32m   5708\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.28 GiB is free. Process 4116 has 13.46 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 106.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_generalized(trainer, test_dataset, all_classnames, all_class_ids, device='cuda'):\n",
    "    print(f\"Evaluating on ALL {len(all_classnames)} classes simultaneously (Generalized Setting)...\")\n",
    "    \n",
    "    # 1. Model Setup with ALL classes (Base + Novel)\n",
    "    model = trainer.model\n",
    "    prompt_learner = model.prompt_learner\n",
    "    \n",
    "    # Save old state to restore later\n",
    "    old_n_cls = prompt_learner.n_cls\n",
    "    old_token_prefix = prompt_learner.token_prefix\n",
    "    old_token_suffix = prompt_learner.token_suffix\n",
    "    old_tokenized = model.tokenized_prompts\n",
    "\n",
    "    # 2. Create prompts for ALL classes (0..101)\n",
    "    clean_names = [name.replace(\"_\", \" \") for name in all_classnames]\n",
    "    \n",
    "    # Reconstruct dummy prompts\n",
    "    n_ctx = prompt_learner.n_ctx\n",
    "    dummy_ctx = \" \".join([\"X\"] * n_ctx)\n",
    "    prompts = [dummy_ctx + \" \" + name + \".\" for name in clean_names]\n",
    "    \n",
    "    new_tokenized = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = trainer.clip_model.token_embedding(new_tokenized).type(trainer.clip_model.dtype)\n",
    "    \n",
    "    # Update buffers\n",
    "    prompt_learner.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "    prompt_learner.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])\n",
    "    prompt_learner.n_cls = len(all_classnames)\n",
    "    prompt_learner.tokenized_prompts = new_tokenized\n",
    "    model.tokenized_prompts = new_tokenized\n",
    "\n",
    "    # 3. Evaluation\n",
    "    model.eval()\n",
    "    # Use the COMPLETE test dataset (Base + Novel) if possible, or a subset\n",
    "    dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Map: Original ID (0..101) -> Model Index (0..101)\n",
    "    # Since \"all\" classes are ordered, the map is usually identical (0->0, 101->101)\n",
    "    # But for safety, we use the passed IDs\n",
    "    target_map = {original_id: idx for idx, original_id in enumerate(all_class_ids)}\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Eval Generalized\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(images) # Output shape: [Batch, 102]\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        # Labels here arrive as Original IDs (e.g. 0, 55, 101)\n",
    "        # We must ensure they correspond to the model indices\n",
    "        mapped_labels = torch.tensor([target_map[l.item()] for l in labels], device=device)\n",
    "        \n",
    "        correct += (pred == mapped_labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    # Restore state\n",
    "    prompt_learner.register_buffer(\"token_prefix\", old_token_prefix)\n",
    "    prompt_learner.register_buffer(\"token_suffix\", old_token_suffix)\n",
    "    prompt_learner.n_cls = old_n_cls\n",
    "    prompt_learner.tokenized_prompts = old_tokenized\n",
    "    model.tokenized_prompts = old_tokenized\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Merge lists\n",
    "all_names = base_classnames + novel_classnames\n",
    "all_ids = list(base_classes) + list(novel_classes)\n",
    "\n",
    "# Merge test datasets (Base + Novel) to perform a unique \"Real\" test\n",
    "full_test_set = torch.utils.data.ConcatDataset([test_base, test_novel])\n",
    "\n",
    "acc_generalized = evaluate_generalized(trainer, full_test_set, all_names, all_ids)\n",
    "print(f\"Generalized Accuracy (Base + Novel mixed): {acc_generalized*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
